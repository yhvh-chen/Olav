
services:
  # Fluent Bit - Network Device Log Collector
  # Replaces rsyslog for better OpenSearch 2.x compatibility
  fluent-bit:
    image: fluent/fluent-bit:latest
    container_name: olav-fluent-bit
    ports:
      - "${FLUENT_SYSLOG_PORT_EXTERNAL:-514}:514/udp"
      - "${FLUENT_SYSLOG_PORT_EXTERNAL:-514}:514/tcp"
      - "${FLUENT_HTTP_PORT_EXTERNAL:-2020}:2020"
    volumes:
      - ./config/fluent-bit/fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:ro
      - ./config/fluent-bit/parsers.conf:/fluent-bit/etc/parsers.conf:ro
    depends_on:
      opensearch:
        condition: service_healthy
    networks:
      - olav-network
    restart: unless-stopped

  # OpenSearch - Vector Database & Schema Index
  opensearch:
    image: opensearchproject/opensearch:2.16.0
    container_name: olav-opensearch
    environment:
      - cluster.name=olav-cluster
      - node.name=olav-node
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
      # Security Configuration (set OPENSEARCH_SECURITY_DISABLED=true for Quick Test mode)
      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_INITIAL_ADMIN_PASSWORD:-OlavOS-2025!A7r9xQ}
      - plugins.security.disabled=${OPENSEARCH_SECURITY_DISABLED:-false}
      # SSL/TLS disabled for development (use nginx/traefik for production TLS)
      - plugins.security.ssl.http.enabled=false
      - plugins.security.ssl.transport.enabled=true
      - plugins.security.ssl.transport.enforce_hostname_verification=false
      # Allow all hosts for development
      - plugins.security.allow_default_init_securityindex=true
      - plugins.security.audit.type=internal_opensearch
      - plugins.security.enable_snapshot_restore_privilege=true
      - plugins.security.check_snapshot_restore_write_privileges=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "${OPENSEARCH_PORT_EXTERNAL:-9200}:9200"
      - "${OPENSEARCH_METRICS_PORT_EXTERNAL:-9600}:9600"
    volumes:
      - opensearch_data:/usr/share/opensearch/data
    restart: unless-stopped
    healthcheck:
      # Use auth when security enabled, skip auth when OPENSEARCH_SECURITY_DISABLED=true
      test: ["CMD-SHELL", "if [ \"${OPENSEARCH_SECURITY_DISABLED:-false}\" = \"true\" ]; then curl -sf http://localhost:9200/_cluster/health; else curl -sf -u admin:${OPENSEARCH_PASSWORD:-OlavOS-2025!A7r9xQ} http://localhost:9200/_cluster/health; fi || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - olav-network

  # PostgreSQL - LangGraph Checkpointer
  postgres:
    image: postgres:16-alpine
    container_name: olav-postgres
    env_file:
      - .env
    ports:
      - "${POSTGRES_PORT_EXTERNAL:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U olav -d olav"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - olav-network

  # Redis (Valkey) - Session & Cache (OPTIONAL)
  # Enable with: docker-compose --profile cache up -d
  # Without Redis, OLAV uses in-memory cache (sufficient for single-instance)
  # Using Valkey - Redis-compatible fork with better performance
  olav-redis:
    image: docker.io/valkey/valkey:8.1-alpine
    container_name: olav-redis
    command:
      - sh
      - -c
      - valkey-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - olav_redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - olav-network
    profiles:
      - cache
      - full

  # SuzieQ - Network Observability
  suzieq:
    image: netenglabs/suzieq:latest
    container_name: olav-suzieq
    env_file:
      - .env
    user: "0:0"  # Run as root to avoid Windows bind-mount permission issues
    ports:
      - "${SUZIEQ_GUI_PORT_EXTERNAL:-8501}:8501"  # GUI
    volumes:
      - ./data/suzieq-parquet:/suzieq/parquet
      - ./data/generated_configs:/suzieq/config:ro
    # Use absolute python path to avoid symlink execution issues
    # gui_main() invokes streamlit run internally with config/port args
    entrypoint: ["/usr/local/bin/python3.9"]
    command:
      - "-c"
      - "from suzieq.gui.sq_gui import gui_main; import sys; sys.argv = ['sq-gui', '--config', '/suzieq/config/suzieq_config.yml', '--port', '8501']; gui_main()"
    depends_on:
      opensearch:
        condition: service_healthy
    networks:
      - olav-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "python3.9 -c \"import urllib.request; urllib.request.urlopen('http://localhost:8501/healthz', timeout=5).read()\" >/dev/null 2>&1 || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3

  suzieq-poller:
    image: netenglabs/suzieq:latest
    container_name: olav-suzieq-poller
    env_file:
      - .env
    environment:
      # Avoid SuzieQ NetBox inventory treating a string tag as an iterable (s/u/z/...) and failing tag validation.
      # Tag filtering should come from inventory.yml where we write a proper YAML list.
      SUZIEQ_TAG_NAME: ""
    user: "0:0"  # Run as root to avoid Windows bind-mount permission issues
    volumes:
      - ./data/suzieq-parquet:/suzieq/parquet
      - ./data/generated_configs:/suzieq/config:ro
    entrypoint: ["/bin/sh", "-c"]
    command:
      - "mkdir -p /root/.ssh && chmod 700 /root/.ssh && echo 'Host *' > /root/.ssh/config && echo '    StrictHostKeyChecking no' >> /root/.ssh/config && echo '    UserKnownHostsFile /dev/null' >> /root/.ssh/config && echo '    KexAlgorithms +diffie-hellman-group14-sha1' >> /root/.ssh/config && echo '    HostKeyAlgorithms +ssh-rsa' >> /root/.ssh/config && echo '    PubkeyAcceptedKeyTypes +ssh-rsa' >> /root/.ssh/config && chmod 600 /root/.ssh/config && while true; do /usr/local/bin/python3.9 -c \"from suzieq.poller.sq_poller import controller_main; import sys; sys.argv = ['sq-poller', '--config', '/suzieq/config/suzieq_config.yml', '-I', '/suzieq/config/inventory.yml', '--ssh-config-file', '/root/.ssh/config']; controller_main()\"; rc=$$?; echo \"sq-poller exited rc=$$rc; retrying in 30s\"; sleep 30; done"
    # entrypoint: ["/usr/local/bin/python3.9"]
    # Reverted monkey patch that broke pydantic; original startup restored.
    # command:
    #   - "-c"
    #   - "from suzieq.poller.sq_poller import controller_main; import sys; sys.argv = ['sq-poller', '--config', '/suzieq/config/suzieq_config.yml', '-I', '/suzieq/config/inventory.yml']; controller_main()"
    # entrypoint: ["/bin/sh", "-c", "sleep infinity"]
    # entrypoint: ["/usr/local/bin/python3.9"]
    # Reverted monkey patch that broke pydantic; original startup restored.
    # command:
    #   - "-c"
    #   - "from suzieq.poller.sq_poller import controller_main; import sys; sys.argv = ['sq-poller', '--config', '/suzieq/config/suzieq_config.yml', '-I', '/suzieq/config/inventory.yml']; controller_main()"
    depends_on:
      netbox:
        condition: service_healthy
    networks:
      - olav-network
    restart: unless-stopped
    profiles:
      - netbox


  # NetBox - Source of Truth (optional local instance)
  netbox-postgres:
    image: postgres:15-alpine
    container_name: olav-netbox-postgres
    environment:
      POSTGRES_DB: netbox
      POSTGRES_USER: netbox
      POSTGRES_PASSWORD: netbox
    volumes:
      - netbox_postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U netbox -d netbox"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - olav-network
    profiles:
      - netbox

  # NetBox Redis (Valkey) - Primary
  netbox-redis:
    image: docker.io/valkey/valkey:8.1-alpine
    container_name: olav-netbox-redis
    command:
      - sh
      - -c
      - valkey-server --appendonly yes
    volumes:
      - netbox_redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - olav-network
    profiles:
      - netbox

  # NetBox Redis (Valkey) - Cache
  netbox-redis-cache:
    image: docker.io/valkey/valkey:8.1-alpine
    container_name: olav-netbox-redis-cache
    command:
      - sh
      - -c
      - valkey-server
    volumes:
      - netbox_redis_cache_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - olav-network
    profiles:
      - netbox

  netbox:
    image: netboxcommunity/netbox:latest
    container_name: olav-netbox
    depends_on:
      netbox-postgres:
        condition: service_healthy
      netbox-redis:
        condition: service_healthy
      netbox-redis-cache:
        condition: service_healthy
    environment:
      # Database
      DB_NAME: netbox
      DB_USER: netbox
      DB_PASSWORD: netbox
      DB_HOST: netbox-postgres
      DB_PORT: 5432
      # Redis (primary)
      REDIS_HOST: netbox-redis
      REDIS_PORT: 6379
      # Redis (cache)
      REDIS_CACHE_HOST: netbox-redis-cache
      REDIS_CACHE_PORT: 6379
      # Security
      ALLOWED_HOSTS: '*'  # Development convenience
      SECRET_KEY: ${NETBOX_SECRET_KEY}
      # Superuser auto-creation
      SUPERUSER_NAME: ${NETBOX_SUPERUSER_NAME}
      SUPERUSER_EMAIL: ${NETBOX_SUPERUSER_EMAIL}
      SUPERUSER_PASSWORD: ${NETBOX_SUPERUSER_PASSWORD}
      SUPERUSER_API_TOKEN: ${NETBOX_TOKEN}
      # Startup
      DB_WAIT_TIMEOUT: 90
      DB_WAIT_DEBUG: 1
    ports:
      - "${NETBOX_PORT_EXTERNAL:-8080}:8080"
    networks:
      - olav-network
    profiles:
      - netbox
    volumes:
      - ./data/netbox-media:/opt/netbox/netbox/media
      - ./data/netbox-reports:/opt/netbox/netbox/reports
      - ./data/netbox-scripts:/opt/netbox/netbox/scripts
      - ./config/netbox-extra:/etc/netbox/config/extra
    restart: unless-stopped
    healthcheck:
      # Accept 200 or 403 (auth required) as healthy - server is responding
      # Healthcheck runs inside the container; always use container port 8080 (not host-mapped NETBOX_PORT).
      test: ["CMD-SHELL", "curl -s -o /dev/null -w '%{http_code}' http://localhost:8080/api/ | grep -E '^(200|403)$' || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 300s
      retries: 10

  # NOTE: olav-init container removed - use CLI instead:
  #   uv run python cli.py --init           # Initialize all
  #   uv run python cli.py --init --force   # Force recreate indexes
  #   uv run python cli.py --init --status  # Check status

  # OLAV App - Main ChatOps Application (Legacy CLI)
  olav-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: olav-app
    env_file:
      - .env
    environment:
      # Container network URLs (use internal hostnames, NOT localhost)
      # These override any .env values to ensure containers use Docker network
      POSTGRES_URI: postgresql://olav:${POSTGRES_PASSWORD:-olav}@postgres:5432/olav
      OPENSEARCH_URL: http://opensearch:9200
      REDIS_URL: ${REDIS_URL:-}  # Optional: redis://olav-redis:6379 if using --profile cache
      NETBOX_URL: http://netbox:8080
      # LLM Configuration for container (use host.docker.internal to reach host Ollama)
      LLM_BASE_URL: ${LLM_BASE_URL:-http://host.docker.internal:11434}
      EMBEDDING_BASE_URL: ${EMBEDDING_BASE_URL:-http://host.docker.internal:11434}
    ports:
      - "${OLAV_APP_PORT_EXTERNAL:-8888}:8000"
    volumes:
      - ./config:/app/config  # Writable for prompt debugging
      - ./data:/app/data
    # Use uv run to ensure virtualenv is active
    command: uv run python -m olav.main serve
    depends_on:
      postgres:
        condition: service_healthy
      opensearch:
        condition: service_healthy
      # NOTE: Run `olav --init` before first startup
    networks:
      - olav-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -sf --max-time 8 http://localhost:8000/health | python -c \"import sys, json; j=json.load(sys.stdin); sys.exit(0 if j.get('orchestrator_ready') else 1)\" || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3

  # OLAV API Server - LangServe API Platform
  olav-server:
    build:
      context: .
      dockerfile: Dockerfile.server
    container_name: olav-server
    env_file:
      - .env
    ports:
      - "${OLAV_SERVER_PORT_EXTERNAL:-8000}:8000"  # API Server (host -> container 8000)
    volumes:
      - ./config:/app/config
      - ./data/suzieq-parquet:/app/data/suzieq-parquet:ro  # SuzieQ Parquet data (read-only)
      - ./data/documents:/app/data/documents  # Documents for RAG (writable for uploads)
      - ./data/generated_configs:/app/data/generated_configs  # Generated configs (writable)
      - ./data/inspection-reports:/app/data/inspection-reports  # Inspection reports (writable)
    # Override Dockerfile CMD: use single worker for async checkpointer compatibility
    command: ["uv", "run", "uvicorn", "olav.server.app:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
    environment:
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=8000
      - OLAV_USE_DYNAMIC_ROUTER=true
      - CORS_ORIGINS=http://localhost:3000
      # Container network URLs (use internal hostnames, NOT localhost)
      # These are hardcoded to prevent .env host-side values from leaking into container
      - POSTGRES_URI=postgresql://olav:${POSTGRES_PASSWORD:-olav}@postgres:5432/olav
      - OPENSEARCH_URL=http://opensearch:9200
      - REDIS_URL=${REDIS_URL:-}  # Optional: redis://olav-redis:6379 if using --profile cache
      - NETBOX_URL=http://netbox:8080
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -sf --max-time 8 http://localhost:8000/health | python -c \"import sys, json; j=json.load(sys.stdin); sys.exit(0 if j.get('orchestrator_ready') else 1)\" || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 40s
      retries: 3
    depends_on:
      postgres:
        condition: service_healthy
      opensearch:
        condition: service_healthy
      # olav-init dependency removed for testing (manual PostgreSQL init above)
    networks:
      - olav-network

  # OLAV E2E Tests - Integration Testing
  olav-tests:
    build:
      context: .
      dockerfile: Dockerfile.tests
    container_name: olav-tests
    environment:
      OLAV_SERVER_URL: http://olav-server:8000
      OLAV_API_URL: http://olav-server:8000
      # Container network URLs (hardcoded for Docker network)
      POSTGRES_URI: postgresql://olav:${POSTGRES_PASSWORD:-OlavPG123!}@postgres:5432/olav
      OPENSEARCH_URL: http://opensearch:9200
    depends_on:
      olav-server:
        condition: service_healthy
    networks:
      - olav-network
    profiles:
      - testing
    command: ["uv", "run", "pytest", "--timeout=300", "tests/e2e"]

volumes:
  opensearch_data:
  postgres_data:
  olav_redis_data:
  netbox_postgres_data:
  netbox_redis_data:
  netbox_redis_cache_data:

networks:
  olav-network:
    driver: bridge
