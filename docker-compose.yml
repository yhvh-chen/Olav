services:
  olav:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: olav
    image: olav:0.8
    
    environment:
      - PYTHONUNBUFFERED=1
      # Pass through environment variables from .env
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_API_KEY=${LLM_API_KEY}
      - LLM_MODEL_NAME=${LLM_MODEL_NAME:-gpt-4o}
      - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.1}
      - OLLAMA_BASE_URL=http://ollama:11434
    
    volumes:
      # Mount configuration and data directories
      - ./.olav:/app/.olav
      - ./data:/app/data
      - ./knowledge:/app/knowledge
    
    networks:
      - olav-network
    
    depends_on:
      - ollama
    
    # Interactive terminal for CLI
    stdin_open: true
    tty: true
    
    restart: unless-stopped

  # Ollama service for local LLM (optional - comment out if using OpenAI/Azure)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    
    volumes:
      - ollama-models:/root/.ollama
    
    networks:
      - olav-network
    
    ports:
      - "11434:11434"
    
    restart: unless-stopped

networks:
  olav-network:
    driver: bridge

volumes:
  ollama-models:
    driver: local
