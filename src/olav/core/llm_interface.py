"""Unified Data Layer - LLM Interface for Map-Reduce Workflow.

This module provides the LLM interface for Map-Reduce operations
following the unified data layer design (docs/0.md).

Core classes:
- MapReduceLLM: LLM interface for analyze_inspect, analyze_logs, generate_report
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Any, Literal

from langchain_core.messages import SystemMessage
from langchain_openai import ChatOpenAI

# =============================================================================
# Map-Reduce LLM Interface
# =============================================================================


class MapReduceLLM:
    """LLM interface for Map-Reduce workflow.

    This class provides methods for:
    - Map phase: analyze_inspect, analyze_logs (per-device/per-command analysis)
    - Reduce phase: generate_report (global correlation and summarization)

    Attributes:
        provider: LLM provider ("anthropic" or "openai")
        model: Model name
        max_concurrent: Maximum concurrent LLM calls in Map phase
        retry_count: Number of retries on failure
        retry_delay: Delay between retries (seconds)
    """

    def __init__(
        self,
        provider: str = "anthropic",
        model: str = "claude-sonnet-4-20250514",
        max_concurrent: int = 5,
        retry_count: int = 3,
        retry_delay: float = 1.0,
    ) -> None:
        """Initialize Map-Reduce LLM interface.

        Args:
            provider: LLM provider ("anthropic" or "openai")
            model: Model name
            max_concurrent: Maximum concurrent Map calls
            retry_count: Retry count on failure
            retry_delay: Delay between retries (seconds)
        """
        self.provider = provider
        self.model = model
        self.max_concurrent = max_concurrent
        self.retry_count = retry_count
        self.retry_delay = retry_delay

        # Initialize LLM client
        if provider == "anthropic":
            # Use Anthropic via OpenAI-compatible API
            self.llm = ChatOpenAI(
                model=model,
                temperature=0,
                max_tokens=2000,
                api_key=self._get_api_key("anthropic"),
                base_url="https://api.anthropic.com/v1/",
            )
        else:  # openai
            self.llm = ChatOpenAI(
                model=model,
                temperature=0,
                max_tokens=2000,
                api_key=self._get_api_key("openai"),
            )

    def _get_api_key(self, provider: str) -> str:
        """Get API key from environment.

        Args:
            provider: Provider name

        Returns:
            API key string
        """
        import os

        if provider == "anthropic":
            return os.getenv("ANTHROPIC_API_KEY", "")
        else:
            return os.getenv("OPENAI_API_KEY", "")

    # =============================================================================
    # Map Phase: analyze_inspect
    # =============================================================================

    async def analyze_inspect(
        self,
        device: str,
        layer: Literal["L1", "L2", "L3", "L4"],
        check_type: str,
        raw_output: str,
        parsed_data: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """Map phase: Analyze single check output.

        This method performs per-command analysis using LLM to determine
        if the check result is OK, WARNING, or CRITICAL based on thresholds
        defined in the L1-L4 checking framework.

        Args:
            device: Device name
            layer: OSI layer (L1/L2/L3/L4)
            check_type: Type of check (cpu, memory, interface_errors, ospf, bgp, temperature, etc.)
            raw_output: Raw command output
            parsed_data: Optional parsed data from TextFSM

        Returns:
            Dictionary with analysis result:
            {
                "device": "R1",
                "layer": "L4",
                "check": "cpu",
                "status": "ok|warning|critical",
                "value": "62%",
                "threshold": "50%",
                "detail": "CPUåˆ©ç”¨ç‡è¶…è¿‡è­¦å‘Šé˜ˆå€¼",
                "interface": "Gi0/1"  # only for interface checks
            }

        Examples:
            >>> result = await llm.analyze_inspect("R1", "L4", "cpu", "CPU utilization: 62%")
            >>> print(result["status"])
            "warning"
        """
        # Load skill prompt
        skill_prompt = self._load_inspect_skill()

        # Build prompt with context
        prompt = f"""
{skill_prompt}

## Input Data

Device: {device}
Layer: {layer}
Check Type: {check_type}

Raw Command Output:
{raw_output[:5000]}
"""

        if parsed_data:
            prompt += f"""

Parsed Data (JSON):
{json.dumps(parsed_data, indent=2)}
"""

        prompt += """

## Task

Analyze this command output and determine if the result is OK, WARNING, or CRITICAL.
Use the threshold table above. Output ONLY a valid JSON object (no markdown, no code blocks).
"""

        # Retry logic
        for attempt in range(self.retry_count):
            try:
                messages = [SystemMessage(content=prompt)]
                response = await self.llm.ainvoke(messages)

                # Parse response
                content = response.content.strip()

                # Remove markdown code blocks if present
                if content.startswith("```json"):
                    content = content[7:]
                if content.startswith("```"):
                    content = content[3:]
                if content.endswith("```"):
                    content = content[:-3]
                content = content.strip()

                result = json.loads(content)

                # Ensure required fields
                result.setdefault("device", device)
                result.setdefault("layer", layer)
                result.setdefault("check", check_type)
                result.setdefault("status", "ok")

                return result

            except (json.JSONDecodeError, Exception) as e:
                if attempt < self.retry_count - 1:
                    import asyncio

                    await asyncio.sleep(self.retry_delay)
                    continue
                else:
                    # Return error status
                    return {
                        "device": device,
                        "layer": layer,
                        "check": check_type,
                        "status": "error",
                        "error": str(e),
                    }

    def _load_inspect_skill(self) -> str:
        """Load inspect-analyzer skill prompt.

        Returns:
            Skill prompt content
        """
        skill_path = Path(".olav/skills/inspect-analyzer/SKILL.md")
        if skill_path.exists():
            return skill_path.read_text(encoding="utf-8")
        else:
            # Fallback: built-in prompt
            return """
## L1-L4 Checking Framework

### Threshold Table

| Check Type | WARNING | CRITICAL |
|------------|---------|----------|
| cpu | >50% | >80% |
| memory | >75% | >90% |
| temperature | >60Â°C | >70Â°C |
| interface_errors | >0 | >0.1% error rate |
| interface_drops | >0 |æŒç»­å¢é•¿ |
| ospf | state != FULL | å…¨éƒ¨é‚»å±…ä¸¢å¤± |
| bgp | state != ESTABLISHED | å…¨éƒ¨ä¼šè¯ down |
| power | ä»»ä¸€ inactive | å• PSU æ¨¡å¼ |
| fans | ä»»ä¸€ failed | - |

## Output Format

Normal (OK):
```json
{
  "device": "R1",
  "layer": "L4",
  "check": "cpu",
  "status": "ok",
  "value": "23%"
}
```

Warning:
```json
{
  "device": "R1",
  "layer": "L4",
  "check": "cpu",
  "status": "warning",
  "value": "62%",
  "threshold": "50%",
  "detail": "CPUåˆ©ç”¨ç‡è¶…è¿‡è­¦å‘Šé˜ˆå€¼"
}
```

Critical:
```json
{
  "device": "R1",
  "layer": "L4",
  "check": "cpu",
  "status": "critical",
  "value": "85%",
  "threshold": "80%",
  "detail": "CPUåˆ©ç”¨ç‡è¶…è¿‡ä¸´ç•Œé˜ˆå€¼"
}
```
"""

    # =============================================================================
    # Map Phase: analyze_logs
    # =============================================================================

    async def analyze_logs(self, device: str, events: list[dict[str, Any]]) -> dict[str, Any]:
        """Map phase: Analyze device log events.

        This method performs per-device log analysis using LLM to identify
        significant events that need to be reported in the daily summary.

        Args:
            device: Device name
            events: List of parsed log events (NetworkEvent dictionaries)

        Returns:
            Dictionary with analysis result:
            {
                "device": "R1",
                "status": "ok|warning",
                "event_count": 5,
                "events": [
                    {
                        "type": "ospf_neighbor_down",
                        "severity": "warning",
                        "count": 3,
                        "neighbors": ["10.1.1.2", "10.1.1.3"],
                        "first_seen": "2026-01-13T02:15:00Z",
                        "last_seen": "2026-01-13T05:30:00Z",
                        "recovered": false,
                        "detail": "3ä¸ªOSPFé‚»å±…DOWNè¶…è¿‡5åˆ†é’Ÿæœªæ¢å¤"
                    }
                ]
            }

        Examples:
            >>> result = await llm.analyze_logs("R1", parsed_events)
            >>> print(result["status"])
            "warning"
        """
        # Load skill prompt
        skill_prompt = self._load_log_skill()

        # Build prompt with events
        events_json = json.dumps(events[:100], indent=2)  # Limit to 100 events

        prompt = f"""
{skill_prompt}

## Input Data

Device: {device}

Events (JSON):
{events_json}
"""

        prompt += """

## Task

Analyze these log events and identify which ones need to be reported.
Use the keyword trigger table and anomaly pattern recognition rules above.
Output ONLY a valid JSON object (no markdown, no code blocks).
"""

        # Retry logic
        for attempt in range(self.retry_count):
            try:
                messages = [SystemMessage(content=prompt)]
                response = await self.llm.ainvoke(messages)

                # Parse response
                content = response.content.strip()

                # Remove markdown code blocks if present
                if content.startswith("```json"):
                    content = content[7:]
                if content.startswith("```"):
                    content = content[3:]
                if content.endswith("```"):
                    content = content[:-3]
                content = content.strip()

                result = json.loads(content)

                # Ensure required fields
                result.setdefault("device", device)
                result.setdefault("status", "ok")
                result.setdefault("event_count", 0)
                result.setdefault("events", [])

                return result

            except (json.JSONDecodeError, Exception) as e:
                if attempt < self.retry_count - 1:
                    import asyncio

                    await asyncio.sleep(self.retry_delay)
                    continue
                else:
                    # Return error status
                    return {
                        "device": device,
                        "status": "error",
                        "event_count": 0,
                        "events": [],
                        "error": str(e),
                    }

    def _load_log_skill(self) -> str:
        """Load log-analyzer skill prompt.

        Returns:
            Skill prompt content
        """
        skill_path = Path(".olav/skills/log-analyzer/SKILL.md")
        if skill_path.exists():
            return skill_path.read_text(encoding="utf-8")
        else:
            # Fallback: built-in prompt
            return """
## Keyword Trigger Rules

### ç¬¬ä¸€é˜¶æ®µ: å…³é”®è¯åŒ¹é… (å¿«é€Ÿè¿‡æ»¤)

| ç±»åˆ« | è§¦å‘å…³é”®è¯ | Severity |
|------|-----------|----------|
| **é”™è¯¯** | `%ERROR`, `%CRITICAL`, `%ALERT` | 0-3 |
| **æ¥å£** | `UPDOWN`, `LINK-3-UPDOWN`, `changed state to down` | 3 |
| **è·¯ç”±** | `OSPF-5-ADJCHG`, `ADJCHG`, `neighbor down`, `went down` | 5 |
| **BGP** | `BGP-5-ADJCHANGE`, `session reset`, `connection closed` | 5 |
| **STP** | `SPANTREE-2-`, `topology change`, `root change` | 2-5 |
| **ç¡¬ä»¶** | `FAN`, `POWER`, `TEMP`, `%ENVMON` | 2-4 |
| **å®‰å…¨** | `SEC_LOGIN`, `AUTHEN`, `failed`, `denied` | 4-5 |
| **é‡å¯** | `RESTART`, `RELOAD`, `BOOT`, `Initializing` | 5 |

### å¼‚å¸¸æ¨¡å¼è¯†åˆ«

| æ¨¡å¼ | å®šä¹‰ | çŠ¶æ€ |
|------|------|------|
| **Flapping** | åŒä¸€æ¥å£ >3 æ¬¡ UP/DOWN (1hå†…) | WARNING |
| **é‚»å±…ä¸¢å¤±** | OSPF/BGP neighbor DOWN æœªæ¢å¤ | WARNING |
| **æ‰¹é‡äº‹ä»¶** | >10 æ¡ç›¸åŒç±»å‹äº‹ä»¶ (1hå†…) | WARNING |
| **ä¸¥é‡äº‹ä»¶** | severity <= 3 | CRITICAL |
| **é‡å¯äº‹ä»¶** | éè®¡åˆ’é‡å¯ | CRITICAL |

## Output Format

æœ‰å¼‚å¸¸éœ€ä¸ŠæŠ¥:
```json
{
  "device": "R1",
  "status": "warning",
  "event_count": 5,
  "events": [
    {
      "type": "ospf_neighbor_down",
      "severity": "warning",
      "count": 3,
      "neighbors": ["10.1.1.2", "10.1.1.3"],
      "first_seen": "2026-01-13T02:15:00Z",
      "last_seen": "2026-01-13T05:30:00Z",
      "recovered": false,
      "detail": "3ä¸ªOSPFé‚»å±…DOWNè¶…è¿‡5åˆ†é’Ÿæœªæ¢å¤"
    }
  ]
}
```

æ— å¼‚å¸¸:
```json
{
  "device": "R2",
  "status": "ok",
  "event_count": 0,
  "events": []
}
```
"""

    # =============================================================================
    # Reduce Phase: generate_report
    # =============================================================================

    async def generate_report(
        self,
        inspect_summary: dict[str, Any],
        log_summary: dict[str, Any],
        topology_path: str,
    ) -> str:
        """Reduce phase: Generate final Markdown report.

        This method performs global correlation analysis and generates
        a comprehensive daily report in Markdown format.

        Args:
            inspect_summary: Aggregated inspect summary from map_tools.aggregate_inspect_maps
            log_summary: Aggregated log summary from map_tools.aggregate_log_maps
            topology_path: Path to topology HTML file

        Returns:
            Markdown formatted daily report

        Examples:
            >>> report = await llm.generate_report(inspect_summ, log_summ, "topology.html")
            >>> print(report[:100])
            "# ç½‘ç»œæ—¥æŠ¥ - 2026-01-13\\n\\n## ğŸ“Š æ‰§è¡Œæ‘˜è¦..."
        """
        # Load skill prompt
        skill_prompt = self._load_report_skill()

        # Build prompt with summaries
        inspect_json = json.dumps(inspect_summary, indent=2, ensure_ascii=False)
        log_json = json.dumps(log_summary, indent=2, ensure_ascii=False)

        prompt = f"""
{skill_prompt}

## Input Data

### æ£€æŸ¥æ‘˜è¦
{inspect_json}

### æ—¥å¿—æ‘˜è¦
{log_json}

### æ‹“æ‰‘å¯è§†åŒ–
é“¾æ¥: [./{topology_path}](./{topology_path})

## Task

åŸºäºä»¥ä¸Šæ‘˜è¦æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ç½‘ç»œæ—¥æŠ¥ã€‚
è¦æ±‚ï¼š
1. æ‰§è¡Œæ‘˜è¦ï¼šè®¾å¤‡æ€»æ•°ã€å¼‚å¸¸è®¾å¤‡æ•°ã€æ£€æŸ¥é¡¹ç»Ÿè®¡
2. é—®é¢˜åˆ—è¡¨ï¼šæŒ‰ä¼˜å…ˆçº§æ’åºï¼ŒåŒ…å«å…³è”åˆ†æå’Œå»ºè®®
3. å¼•ç”¨æ‹“æ‰‘å›¾
4. ä½¿ç”¨ä¸­æ–‡è¾“å‡º

è¾“å‡ºæ ¼å¼ï¼šçº¯Markdownï¼ˆä¸è¦ä»£ç å—ï¼‰
"""

        # Retry logic
        for attempt in range(self.retry_count):
            try:
                messages = [SystemMessage(content=prompt)]
                response = await self.llm.ainvoke(messages)

                # Parse response
                content = response.content.strip()

                # Remove markdown code blocks if present
                if content.startswith("```markdown"):
                    content = content[11:]
                if content.startswith("```"):
                    content = content[3:]
                if content.endswith("```"):
                    content = content[:-3]
                content = content.strip()

                return content

            except Exception:
                if attempt < self.retry_count - 1:
                    import asyncio

                    await asyncio.sleep(self.retry_delay)
                    continue
                else:
                    # Fallback: generate basic report
                    return self._generate_fallback_report(inspect_summary, log_summary, topology_path)

    def _load_report_skill(self) -> str:
        """Load daily-report skill prompt.

        Returns:
            Skill prompt content
        """
        skill_path = Path(".olav/skills/daily-report/SKILL.md")
        if skill_path.exists():
            return skill_path.read_text(encoding="utf-8")
        else:
            # Fallback: built-in prompt
            return """
## Daily Report Generation

Generate a structured network daily report in Markdown format.

### Report Structure

1. **æ‰§è¡Œæ‘˜è¦**
   - è®¾å¤‡æ€»æ•°
   - å¼‚å¸¸è®¾å¤‡æ•°
   - æ£€æŸ¥é¡¹ç»Ÿè®¡ (æ€»æ•°/æ­£å¸¸/è­¦å‘Š/ä¸¥é‡)

2. **ğŸ—ºï¸ ç½‘ç»œæ‹“æ‰‘**
   - å¼•ç”¨: [æŸ¥çœ‹å®Œæ•´æ‹“æ‰‘](./topology.html)

3. **ğŸ”´ éœ€è¦å…³æ³¨çš„é—®é¢˜**
   - æŒ‰ä¼˜å…ˆçº§æ’åº (CRITICAL > WARNING)
   - æ¯ä¸ªé—®é¢˜åŒ…å«ï¼š
     * ç°è±¡æè¿°
     * æ ¹å› åˆ†æ
     * å½±å“è¯„ä¼°
     * å¤„ç†å»ºè®®

4. **è¯¦ç»†æ•°æ®** (å¯é€‰ï¼Œæ”¾åœ¨ <details> æ ‡ç­¾å†…)
   - å®Œæ•´æ£€æŸ¥ç»“æœåˆ—è¡¨
   - å®Œæ•´äº‹ä»¶åˆ—è¡¨

### Correlation Analysis Guidelines

- CPU é«˜ + OSPF DOWN â†’ è·¯ç”±æŠ–åŠ¨å¯¼è‡´
- CRC é”™è¯¯ + BGP reset â†’ ç‰©ç†å±‚é—®é¢˜å¯¼è‡´è·¯ç”±é—®é¢˜
- å¤šè®¾å¤‡åŒæ—¶å‘Šè­¦ â†’ å¯èƒ½ç½‘ç»œäº‹ä»¶
- æ¸©åº¦é«˜ + é£æ‰‡å¤±è´¥ â†’ æ•£çƒ­ç³»ç»Ÿé—®é¢˜

### Report Template

```markdown
# ç½‘ç»œæ—¥æŠ¥ - {{date}}

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

| æŒ‡æ ‡ | å€¼ | çŠ¶æ€ |
|------|-----|------|
| è®¾å¤‡æ€»æ•° | {{count}} | {{status}} |
| å¼‚å¸¸è®¾å¤‡ | {{anomaly_count}} | {{anomaly_status}} |
| æ£€æŸ¥é¡¹æ­£å¸¸ç‡ | {{ok_rate}}% | {{ok_status}} |

## ğŸ—ºï¸ ç½‘ç»œæ‹“æ‰‘

[æŸ¥çœ‹å®Œæ•´æ‹“æ‰‘](./topology.html)

## ğŸ”´ éœ€è¦å…³æ³¨çš„é—®é¢˜

### 1. {{title}} (CRITICAL)

**ç°è±¡**: {{symptom}}

**æ ¹å› **: {{root_cause}}

**å½±å“**: {{impact}}

**å»ºè®®**: {{recommendation}}

---

<details>
<summary>è¯¦ç»†æ•°æ®</summary>

### æ£€æŸ¥ç»“æœè¯¦æƒ…

{{detailed_checks}}

### äº‹ä»¶è¯¦æƒ…

{{detailed_events}}

</details>
```
"""

    def _generate_fallback_report(
        self,
        inspect_summary: dict[str, Any],
        log_summary: dict[str, Any],
        topology_path: str,
    ) -> str:
        """Generate basic report without LLM.

        Args:
            inspect_summary: Inspect summary
            log_summary: Log summary
            topology_path: Path to topology

        Returns:
            Markdown report
        """
        date = datetime.now().strftime("%Y-%m-%d")

        summary = inspect_summary.get("summary", {})
        total_checks = summary.get("total_checks", 0)
        status_counts = summary.get("status_counts", {})
        ok_count = status_counts.get("ok", 0)
        warning_count = status_counts.get("warning", 0)
        critical_count = status_counts.get("critical", 0)

        anomalies = inspect_summary.get("anomalies", [])
        log_anomalies = log_summary.get("anomalies", [])

        lines = [
            f"# ç½‘ç»œæ—¥æŠ¥ - {date}",
            "",
            "> âš ï¸ LLM åˆ†æä¸å¯ç”¨ï¼Œä»…æ˜¾ç¤ºç»Ÿè®¡æ•°æ®",
            "",
            "## ğŸ“Š æ‰§è¡Œæ‘˜è¦",
            "",
            f"- è®¾å¤‡æ€»æ•°: {summary.get('total_devices', 0)}",
            f"- å¼‚å¸¸è®¾å¤‡: {len(set(a['device'] for a in anomalies))}",
            f"- æ£€æŸ¥é¡¹æ€»æ•°: {total_checks}",
            f"  - âœ… æ­£å¸¸: {ok_count}",
            f"  - âš ï¸  è­¦å‘Š: {warning_count}",
            f"  - ğŸ”´ ä¸¥é‡: {critical_count}",
            "",
            "## ğŸ—ºï¸ ç½‘ç»œæ‹“æ‰‘",
            "",
            f"[æŸ¥çœ‹å®Œæ•´æ‹“æ‰‘](./{topology_path})",
            "",
            "## å¼‚å¸¸åˆ—è¡¨",
            "",
        ]

        if anomalies:
            lines.append("### æ£€æŸ¥é¡¹å¼‚å¸¸")
            lines.append("")
            lines.append("| è®¾å¤‡ | æ£€æŸ¥é¡¹ | çŠ¶æ€ | å€¼ | è¯´æ˜ |")
            lines.append("|------|--------|------|-----|------|")

            for a in anomalies:
                device = a.get("device", "")
                check = a.get("check", "")
                status = a.get("status", "")
                value = a.get("value", "-")
                detail = a.get("detail", "-")

                status_icon = "ğŸ”´" if status == "critical" else "âš ï¸"
                lines.append(f"| {device} | {check} | {status_icon} {status} | {value} | {detail} |")

        if log_anomalies:
            lines.append("")
            lines.append("### æ—¥å¿—äº‹ä»¶å¼‚å¸¸")
            lines.append("")

            for a in log_anomalies:
                device = a.get("device", "")
                events = a.get("events", [])
                lines.append(f"#### {device}")

                for event in events:
                    etype = event.get("type", "")
                    detail = event.get("detail", "")
                    lines.append(f"- **{etype}**: {detail}")

        return "\n".join(lines)
