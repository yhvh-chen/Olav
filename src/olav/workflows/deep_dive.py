"""Deep Dive Workflow - Funnel Debugging with OSI Layer-Based Diagnosis.

This workflow implements **ÊºèÊñóÂºèÊéíÈîô (Funnel Debugging)**:
1. Topology Analysis: Identify fault scope and affected devices
2. Layered Hypothesis: Generate hypotheses per OSI layer (L1-L4+)
3. Macro Scan (SuzieQ): Broad sweep to narrow down problem area
4. Micro Diagnosis (NETCONF/CLI): Deep dive only where issues found
5. Root Cause Summary: Correlate findings and generate report

Key Principles:
- Start broad (macro), then narrow (micro)
- Lower layers first (L1‚ÜíL2‚ÜíL3‚ÜíL4+)
- SuzieQ for historical analysis, NETCONF for real-time details
- Stop drilling when root cause identified

Trigger scenarios:
- Neighbor issues: "R1 Âíå R2 ‰πãÈó¥ BGP ÈÇªÂ±ÖÈóÆÈ¢ò"
- Connectivity: "‰∏∫‰ªÄ‰πà A Êó†Ê≥ïËÆøÈóÆ B"
- Protocol failures: "OSPF ÈÇªÂ±ÖÂÖ≥Á≥ªÂºÇÂ∏∏"
- Batch audits: "ÂÆ°ËÆ°ÊâÄÊúâËæπÁïåË∑ØÁî±Âô®"

Usage:
    uv run olav.py -e "R1 Âíå R2 BGP ÈÇªÂ±ÖÂª∫Á´ãÂ§±Ë¥•"
    uv run olav.py --expert "‰ªé DataCenter-A Âà∞ DataCenter-B ‰∏çÈÄö"
"""

import asyncio
import sys

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

import json
import logging
import re
from operator import add
from typing import Annotated, Any, Literal, TypedDict

from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from langgraph.graph import END, StateGraph

from olav.core.llm import LLMFactory
from olav.core.memory_writer import get_memory_writer
from olav.core.prompt_manager import prompt_manager
from olav.core.settings import settings
from olav.workflows.base import BaseWorkflow
from olav.workflows.registry import WorkflowRegistry

logger = logging.getLogger(__name__)


# ============================================
# Type Definitions for Funnel Debugging
# ============================================


class LayerHypothesis(TypedDict):
    """Hypothesis for a specific OSI layer."""

    layer: Literal["L1", "L2", "L3", "L4"]
    issue: str
    probability: Literal["high", "medium", "low"]
    checks: list[str]  # SuzieQ tables to check


class PhaseCheck(TypedDict):
    """A single check within a diagnosis phase."""

    tool: str  # suzieq_query, netconf_tool, cli_tool
    table: str | None  # For SuzieQ
    filters: dict[str, Any]
    purpose: str
    result: dict[str, Any] | None
    status: Literal["pending", "running", "completed", "failed"] | None


class DiagnosisPhase(TypedDict):
    """A phase in the funnel diagnosis process."""

    phase: int
    layer: Literal["L1", "L2", "L3", "L4"]
    name: str
    checks: list[PhaseCheck]
    deep_dive_trigger: str | None
    findings: list[str]
    status: Literal["pending", "running", "completed", "skipped"]


class TopologyAnalysis(TypedDict):
    """Result of topology analysis."""

    source_device: str | None
    destination_device: str | None
    path_hypothesis: list[str]
    affected_devices: list[str]
    device_roles: dict[str, str]
    scope: Literal["single_device", "local", "path", "domain"]
    confidence: Literal["high", "medium", "low"]


class DiagnosisPlan(TypedDict):
    """Complete diagnosis plan from funnel analysis."""

    summary: str
    affected_scope: list[str]
    hypotheses: list[LayerHypothesis]
    phases: list[DiagnosisPhase]
    current_phase: int
    root_cause_identified: bool
    root_cause: str | None


class TodoItem(TypedDict):
    """Individual task item in the Todo List.

    Extended (Phase 2) with evaluator related fields.
    These optional fields support objective post-execution validation.
    """

    id: int
    task: str
    status: Literal["pending", "in-progress", "completed", "failed"]
    result: str | None
    deps: list[int]  # IDs of prerequisite todos
    # Schema investigation results
    feasibility: Literal["feasible", "uncertain", "infeasible"] | None
    recommended_table: str | None
    schema_notes: str | None
    # External evaluator (Phase 2) - no hardcoded fields needed
    evaluation_passed: bool | None
    evaluation_score: float | None
    failure_reason: str | None


# ============================================
# Internationalization (i18n) Strings
# ============================================
import contextlib

from config.settings import AgentConfig

I18N: dict[str, dict[str, str]] = {
    # Execution Plan Section
    "plan_title": {
        "zh": "## üìã ËØäÊñ≠ËÆ°Âàí\n",
        "en": "## üìã Diagnostic Plan\n",
        "ja": "## üìã Ë®∫Êñ≠Ë®àÁîª\n",
    },
    "ready_section": {
        "zh": "### ‚úÖ ÂáÜÂ§áÂ∞±Áª™ ({count} È°π)\n",
        "en": "### ‚úÖ Ready ({count} items)\n",
        "ja": "### ‚úÖ Ê∫ñÂÇôÂÆå‰∫Ü ({count} ‰ª∂)\n",
    },
    "uncertain_section": {
        "zh": "### ‚ö†Ô∏è ÈúÄË¶ÅÁ°ÆËÆ§ ({count} È°π)\n",
        "en": "### ‚ö†Ô∏è Needs Confirmation ({count} items)\n",
        "ja": "### ‚ö†Ô∏è Á¢∫Ë™ç„ÅåÂøÖË¶Å ({count} ‰ª∂)\n",
    },
    "infeasible_section": {
        "zh": "### ‚ùå ÊöÇ‰∏çÊîØÊåÅ ({count} È°π)\n",
        "en": "### ‚ùå Not Supported ({count} items)\n",
        "ja": "### ‚ùå Êú™ÂØæÂøú ({count} ‰ª∂)\n",
    },
    "plan_summary_partial": {
        "zh": "üìä **ËÆ°ÂàíÊëòË¶Å**: {ready}/{total} È°π‰ªªÂä°ÂáÜÂ§áÂ∞±Áª™\n",
        "en": "üìä **Plan Summary**: {ready}/{total} tasks ready\n",
        "ja": "üìä **Ë®àÁîªÊ¶ÇË¶Å**: {ready}/{total} ‰ª∂„ÅÆ„Çø„Çπ„ÇØ„ÅåÊ∫ñÂÇôÂÆå‰∫Ü\n",
    },
    "plan_summary_full": {
        "zh": "üìä **ËÆ°ÂàíÊëòË¶Å**: ÂÖ®ÈÉ® {total} È°π‰ªªÂä°ÂáÜÂ§áÂ∞±Áª™\n",
        "en": "üìä **Plan Summary**: All {total} tasks ready\n",
        "ja": "üìä **Ë®àÁîªÊ¶ÇË¶Å**: ÂÖ® {total} ‰ª∂„ÅÆ„Çø„Çπ„ÇØ„ÅåÊ∫ñÂÇôÂÆå‰∫Ü\n",
    },
    "plan_confirmation": {
        "zh": "ÈÉ®ÂàÜ‰ªªÂä°ÈúÄË¶ÅÁ°ÆËÆ§ÊàñÊöÇ‰∏çÊîØÊåÅÔºåÊòØÂê¶ÁªßÁª≠ÊâßË°åÂ∑≤Â∞±Áª™ÁöÑ‰ªªÂä°Ôºü\n",
        "en": "Some tasks need confirmation or are not supported. Continue with ready tasks?\n",
        "ja": "‰∏ÄÈÉ®„ÅÆ„Çø„Çπ„ÇØ„ÅØÁ¢∫Ë™ç„ÅåÂøÖË¶Å„Åã„ÄÅÊú™ÂØæÂøú„Åß„Åô„ÄÇÊ∫ñÂÇôÂÆå‰∫Ü„ÅÆ„Çø„Çπ„ÇØ„ÇíÁ∂öË°å„Åó„Åæ„Åô„ÅãÔºü\n",
    },
    "action_approve": {
        "zh": "Y / approve  ‚Üí  ÂºÄÂßãÊâßË°å",
        "en": "Y / approve  ‚Üí  Start execution",
        "ja": "Y / approve  ‚Üí  ÂÆüË°åÈñãÂßã",
    },
    "action_abort": {
        "zh": "N / abort    ‚Üí  ÂèñÊ∂àËÆ°Âàí",
        "en": "N / abort    ‚Üí  Cancel plan",
        "ja": "N / abort    ‚Üí  Ë®àÁîª‰∏≠Ê≠¢",
    },
    "action_modify": {
        "zh": "modify       ‚Üí  ‰øÆÊîπ‰ªªÂä°",
        "en": "modify       ‚Üí  Modify tasks",
        "ja": "modify       ‚Üí  „Çø„Çπ„ÇØ‰øÆÊ≠£",
    },
    "default_task": {
        "zh": "ÊâßË°åÊï∞ÊçÆÊü•ËØ¢",
        "en": "Execute data query",
        "ja": "„Éá„Éº„Çø„ÇØ„Ç®„É™„ÇíÂÆüË°å",
    },
    # Task Execution Results
    "query_complete": {
        "zh": "‚úÖ Êü•ËØ¢ÂÆåÊàê: {table}Ôºà{count} Êù°ËÆ∞ÂΩïÔºâ",
        "en": "‚úÖ Query complete: {table} ({count} records)",
        "ja": "‚úÖ „ÇØ„Ç®„É™ÂÆå‰∫Ü: {table}Ôºà{count} ‰ª∂Ôºâ",
    },
    "records_header": {
        "zh": "ÂÖ± {count} Êù°ËÆ∞ÂΩï:",
        "en": "{count} records:",
        "ja": "ÂÖ® {count} ‰ª∂:",
    },
    "records_header_truncated": {
        "zh": "ÂÖ± {total} Êù°ËÆ∞ÂΩïÔºåÊòæÁ§∫Ââç {showing} Êù°:",
        "en": "{total} records, showing first {showing}:",
        "ja": "ÂÖ® {total} ‰ª∂„ÄÅÊúÄÂàù„ÅÆ {showing} ‰ª∂„ÇíË°®Á§∫:",
    },
    "no_diagnostic_fields": {
        "zh": "Êó†ÂèØÁî®ÁöÑËØäÊñ≠Â≠óÊÆµ",
        "en": "No diagnostic fields available",
        "ja": "Ë®∫Êñ≠„Éï„Ç£„Éº„É´„Éâ„Åå„ÅÇ„Çä„Åæ„Åõ„Çì",
    },
    # State Values
    "state_established": {
        "zh": "‚úÖ Â∑≤Âª∫Á´ã",
        "en": "‚úÖ Established",
        "ja": "‚úÖ Á¢∫Á´ãÊ∏à„Åø",
    },
    "state_not_established": {
        "zh": "‚ùå Êú™Âª∫Á´ã",
        "en": "‚ùå Not Established",
        "ja": "‚ùå Êú™Á¢∫Á´ã",
    },
    "state_up": {
        "zh": "‚úÖ UP",
        "en": "‚úÖ UP",
        "ja": "‚úÖ UP",
    },
    "state_down": {
        "zh": "‚ùå DOWN",
        "en": "‚ùå DOWN",
        "ja": "‚ùå DOWN",
    },
    "timestamp_not_established": {
        "zh": "Êú™Âª∫Á´ã",
        "en": "Not established",
        "ja": "Êú™Á¢∫Á´ã",
    },
    # Field Labels
    "field_hostname": {"zh": "‰∏ªÊú∫", "en": "Host", "ja": "„Éõ„Çπ„Éà"},
    "field_peer": {"zh": "ÈÇªÂ±ÖÂú∞ÂùÄ", "en": "Peer", "ja": "„Éî„Ç¢"},
    "field_state": {"zh": "Áä∂ÊÄÅ", "en": "State", "ja": "Áä∂ÊÖã"},
    "field_ifname": {"zh": "Êé•Âè£Âêç", "en": "Interface", "ja": "„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ"},
    "field_adminState": {"zh": "ÁÆ°ÁêÜÁä∂ÊÄÅ", "en": "Admin State", "ja": "ÁÆ°ÁêÜÁä∂ÊÖã"},
    "field_ipAddressList": {"zh": "IPÂú∞ÂùÄ", "en": "IP Address", "ja": "IP„Ç¢„Éâ„É¨„Çπ"},
    "field_asn": {"zh": "ASÂè∑", "en": "ASN", "ja": "ASÁï™Âè∑"},
    "field_peerAsn": {"zh": "ÈÇªÂ±ÖASÂè∑", "en": "Peer ASN", "ja": "„Éî„Ç¢ASÁï™Âè∑"},
    "field_prefix": {"zh": "Ë∑ØÁî±ÂâçÁºÄ", "en": "Prefix", "ja": "„Éó„É¨„Éï„Ç£„ÉÉ„ÇØ„Çπ"},
    "field_nexthopIp": {"zh": "‰∏ã‰∏ÄË∑≥", "en": "Next Hop", "ja": "„Éç„ÇØ„Çπ„Éà„Éõ„ÉÉ„Éó"},
    "field_protocol": {"zh": "ÂçèËÆÆ", "en": "Protocol", "ja": "„Éó„É≠„Éà„Ç≥„É´"},
    "field_vrf": {"zh": "VRF", "en": "VRF", "ja": "VRF"},
    "field_sqvers": {"zh": "ÁâàÊú¨", "en": "Version", "ja": "„Éê„Éº„Ç∏„Éß„É≥"},
    "field_origPeer": {"zh": "ÂéüÂßãÈÇªÂ±Ö", "en": "Origin Peer", "ja": "ÂÖÉ„Éî„Ç¢"},
    "field_afi": {"zh": "Âú∞ÂùÄÊóè", "en": "AFI", "ja": "AFI"},
    "field_safi": {"zh": "Â≠êÂú∞ÂùÄÊóè", "en": "SAFI", "ja": "SAFI"},
    # Table Names
    "table_bgp": {"zh": "BGP ÈÇªÂ±ÖË°®", "en": "BGP Neighbors", "ja": "BGP„Éç„Ç§„Éê„Éº"},
    "table_interfaces": {"zh": "Êé•Âè£Áä∂ÊÄÅË°®", "en": "Interfaces", "ja": "„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ"},
    "table_routes": {"zh": "Ë∑ØÁî±Ë°®", "en": "Routes", "ja": "„É´„Éº„Éà"},
    "table_device": {"zh": "ËÆæÂ§á‰ø°ÊÅØË°®", "en": "Devices", "ja": "„Éá„Éê„Ç§„Çπ"},
    "table_lldp": {"zh": "LLDP ÈÇªÂ±ÖË°®", "en": "LLDP Neighbors", "ja": "LLDP„Éç„Ç§„Éê„Éº"},
    "table_ospfIf": {"zh": "OSPF Êé•Âè£Ë°®", "en": "OSPF Interfaces", "ja": "OSPF„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„Çπ"},
    "table_ospfNbr": {"zh": "OSPF ÈÇªÂ±ÖË°®", "en": "OSPF Neighbors", "ja": "OSPF„Éç„Ç§„Éê„Éº"},
    "table_macs": {"zh": "MAC Âú∞ÂùÄË°®", "en": "MAC Table", "ja": "MAC„ÉÜ„Éº„Éñ„É´"},
    "table_arpnd": {"zh": "ARP/ND Ë°®", "en": "ARP/ND Table", "ja": "ARP/ND„ÉÜ„Éº„Éñ„É´"},
    "table_vlan": {"zh": "VLAN Ë°®", "en": "VLANs", "ja": "VLAN"},
    "table_inventory": {"zh": "ËÆæÂ§áÊ∏ÖÂçï", "en": "Inventory", "ja": "„Ç§„É≥„Éô„É≥„Éà„É™"},
    "table_devconfig": {"zh": "ËÆæÂ§áÈÖçÁΩÆ", "en": "Device Config", "ja": "„Éá„Éê„Ç§„ÇπË®≠ÂÆö"},
    # Record placeholder
    "record_placeholder": {"zh": "ËÆ∞ÂΩï", "en": "Record", "ja": "„É¨„Ç≥„Éº„Éâ"},
    # Task completion messages
    "task_complete_msg": {
        "zh": "‰ªªÂä° {task_id} ÂÆåÊàê: ‰ªé {table} Ëé∑Âèñ‰∫Ü {count} Êù°Êï∞ÊçÆ",
        "en": "Task {task_id} complete: Retrieved {count} records from {table}",
        "ja": "„Çø„Çπ„ÇØ {task_id} ÂÆå‰∫Ü: {table} „Åã„Çâ {count} ‰ª∂ÂèñÂæó",
    },
    "task_complete_simple": {
        "zh": "‰ªªÂä°ÂÆåÊàê: ‰ªé {table} Ëé∑Âèñ‰∫Ü {count} Êù°Êï∞ÊçÆ",
        "en": "Task complete: Retrieved {count} records from {table}",
        "ja": "„Çø„Çπ„ÇØÂÆå‰∫Ü: {table} „Åã„Çâ {count} ‰ª∂ÂèñÂæó",
    },
}


def tr(key: str, **kwargs: Any) -> str:
    """Get translated string for current language.

    Args:
        key: String key in I18N dictionary
        **kwargs: Format arguments for the string

    Returns:
        Translated and formatted string
    """
    lang = AgentConfig.LANGUAGE
    if key not in I18N:
        return key  # Fallback to key itself
    translations = I18N[key]
    text = translations.get(lang, translations.get("en", key))
    if kwargs:
        try:
            return text.format(**kwargs)
        except KeyError:
            return text
    return text


class ExecutionPlan(TypedDict):
    """Execution plan generated from schema investigation."""

    feasible_tasks: list[int]  # Todo IDs that can be executed
    uncertain_tasks: list[int]  # Need user clarification
    infeasible_tasks: list[int]  # Cannot be executed (no schema support)
    recommendations: dict[int, str]  # Todo ID -> recommended approach
    user_approval_required: bool


class DeepDiveState(TypedDict):
    """State for Deep Dive Workflow with Funnel Debugging.

    Funnel Debugging Flow:
        1. topology_analysis: Identify affected devices and scope
        2. diagnosis_plan: OSI layer-based hypothesis and phases
        3. macro_scan: SuzieQ broad sweep per layer
        4. micro_diagnosis: NETCONF/CLI deep dive (if needed)
        5. root_cause_summary: Correlate findings

    Fields:
        messages: Conversation history
        topology: Result of topology analysis
        diagnosis_plan: Layered diagnosis phases
        todos: Legacy todo list (for backward compat)
        execution_plan: Schema investigation results
        current_phase: Current diagnosis phase (0-based)
        findings: Accumulated diagnostic findings
        completed_results: Mapping of check_id -> result
        recursion_depth: Current recursion level
        max_depth: Maximum recursion depth
        expert_mode: Whether expert mode is enabled
        user_approval: HITL approval status
    """

    messages: Annotated[list[BaseMessage], add]
    # Funnel Debugging state
    topology: TopologyAnalysis | None
    diagnosis_plan: DiagnosisPlan | None
    current_phase: int
    findings: list[str]
    # Legacy state (backward compat)
    todos: list[TodoItem]
    execution_plan: ExecutionPlan | None
    current_todo_id: int | None
    completed_results: dict[int, str]
    recursion_depth: int
    max_depth: int
    expert_mode: bool
    trigger_recursion: bool | None
    user_approval: str | None


@WorkflowRegistry.register(
    name="deep_dive",
    description="Deep Dive ÊºèÊñóÂºèÊéíÈîôÔºàÊãìÊâëÂàÜÊûê ‚Üí ÂàÜÂ±ÇÂÅáËÆæ ‚Üí ÂÆèËßÇÊâ´Êèè ‚Üí ÂæÆËßÇËØäÊñ≠Ôºâ",
    examples=[
        "R1 Âíå R2 ‰πãÈó¥ BGP ÈÇªÂ±ÖÂª∫Á´ãÂ§±Ë¥•",
        "‰∏∫‰ªÄ‰πà DataCenter-A Êó†Ê≥ïËÆøÈóÆ DataCenter-B",
        "OSPF ÈÇªÂ±ÖÂÖ≥Á≥ªÂºÇÂ∏∏ÔºåÈúÄË¶ÅÊéíÊü•",
        "ÂÆ°ËÆ°ÊâÄÊúâËæπÁïåË∑ØÁî±Âô®ÁöÑ BGP ÈÖçÁΩÆ",
        "‰ªé Core-R1 Âà∞ Edge-R3 Ë∑ØÁî±‰∏çÈÄö",
        "Êé•Âè£ Gi0/0/1 È¢ëÁπÅ flapping",
    ],
    triggers=[
        r"ÈÇªÂ±Ö.*ÈóÆÈ¢ò",
        r"ÈÇªÂ±Ö.*Â§±Ë¥•",
        r"Êó†Ê≥ïËÆøÈóÆ",
        r"‰∏çÈÄö",
        r"‰∏∫‰ªÄ‰πà",
        r"ÊéíÊü•",
        r"ËØäÊñ≠",
        r"ÂÆ°ËÆ°",
        r"ÊâπÈáè",
        r"‰ªé.*Âà∞",
        r"flapping",
        r"ÂºÇÂ∏∏",
    ],
)
class DeepDiveWorkflow(BaseWorkflow):
    """Deep Dive Workflow implementing Funnel Debugging methodology.

    Flow:
        1. topology_analysis_node: Parse query, identify affected devices
        2. funnel_planning_node: Generate OSI layer-based diagnosis plan
        3. [HITL] User approves diagnosis plan
        4. macro_scan_node: Execute SuzieQ checks per layer
        5. evaluate_findings_node: Decide if micro diagnosis needed
        6. micro_diagnosis_node: NETCONF/CLI deep dive (if needed)
        7. root_cause_summary_node: Correlate and summarize
    """

    @property
    def name(self) -> str:
        return "deep_dive"

    @property
    def description(self) -> str:
        return "Deep Dive ÊºèÊñóÂºèÊéíÈîôÔºàÊãìÊâëÂàÜÊûê ‚Üí ÂàÜÂ±ÇÂÅáËÆæ ‚Üí ÂÆèËßÇÊâ´Êèè ‚Üí ÂæÆËßÇËØäÊñ≠Ôºâ"

    @property
    def tools_required(self) -> list[str]:
        return [
            "suzieq_query",
            "suzieq_schema_search",
            "netconf_tool",
            "cli_tool",
            "search_openconfig_schema",
        ]

    async def validate_input(self, user_query: str) -> tuple[bool, str]:
        """Check if query requires Deep Dive workflow.

        Deep Dive triggers:
        - Neighbor issues ("ÈÇªÂ±ÖÈóÆÈ¢ò", "ÈÇªÂ±ÖÂ§±Ë¥•", "peer down")
        - Connectivity ("Êó†Ê≥ïËÆøÈóÆ", "‰∏çÈÄö", "unreachable")
        - Diagnostics ("‰∏∫‰ªÄ‰πà", "ÊéíÊü•", "ËØäÊñ≠")
        - Audit ("ÂÆ°ËÆ°", "ÊâπÈáè")
        - Path issues ("‰ªé...Âà∞", "between")
        """
        import re

        from olav.sync.rules.loader import get_deep_dive_trigger_patterns

        # Load trigger patterns from config
        triggers = get_deep_dive_trigger_patterns()

        for pattern in triggers:
            if re.search(pattern, user_query, re.IGNORECASE):
                return (True, f"Deep Dive trigger detected: '{pattern}'")

        return (False, "Query does not require Deep Dive workflow")

    def __init__(self) -> None:
        self.llm = LLMFactory.get_chat_model(json_mode=False)
        self.llm_json = LLMFactory.get_chat_model(json_mode=True)

        # OSI Layer to SuzieQ table mapping - loaded from config
        from olav.sync.rules.loader import get_osi_layer_tables

        self.layer_tables: dict[str, list[str]] = get_osi_layer_tables()

    # ============================================
    # NEW: Funnel Debugging Nodes
    # ============================================

    async def _search_historical_diagnostics(
        self, user_query: str, max_results: int = 3
    ) -> dict[str, Any] | None:
        """Search episodic memory for historical similar diagnostics (Agentic RAG Read).

        This implements RAG optimization for DeepDive: if we've successfully
        diagnosed a similar issue before, reuse the diagnostic approach.

        Args:
            user_query: User's diagnostic query
            max_results: Maximum historical patterns to retrieve

        Returns:
            Dict with historical diagnostic pattern if found, else None

        Example:
            >>> pattern = await self._search_historical_diagnostics("R1 BGP ÈÇªÂ±ÖÈóÆÈ¢ò")
            >>> pattern
            {
                "intent": "R1 R2 BGP ÈÇªÂ±ÖÂª∫Á´ãÂ§±Ë¥•",
                "phases_completed": 3,
                "findings_count": 2,
                "affected_devices": ["R1", "R2"],
                "root_cause": "BGP peer IP ÈÖçÁΩÆÈîôËØØ",
                "confidence": 0.85
            }
        """
        # Check if RAG read is enabled
        if not settings.enable_deep_dive_memory:
            logger.debug("Deep Dive memory disabled, skipping historical search")
            return None

        try:
            from olav.tools.opensearch_tool import search_episodic_memory

            result = await search_episodic_memory.ainvoke({
                "intent": user_query,
                "max_results": max_results,
                "only_successful": True,
            })

            if not result.get("success") or not result.get("data"):
                logger.debug(f"No historical diagnostics found for: {user_query[:50]}...")
                return None

            # Get best match (first result, highest relevance)
            best_match = result["data"][0]

            # Filter for DeepDive-specific patterns
            tool_used = best_match.get("context", {}).get("tool_used", "")
            if tool_used not in ("deep_dive_workflow", "deep_dive_funnel"):
                logger.debug(f"Historical pattern is not from DeepDive: {tool_used}")
                return None

            # Calculate similarity confidence (simple heuristic)
            historical_intent = best_match.get("intent", "")
            query_words = set(user_query.lower().split())
            historical_words = set(historical_intent.lower().split())

            if query_words and historical_words:
                intersection = query_words & historical_words
                union = query_words | historical_words
                similarity = len(intersection) / len(union)
            else:
                similarity = 0.0

            # Only use if similarity is above threshold
            if similarity < 0.6:
                logger.debug(
                    f"Historical pattern similarity {similarity:.2f} below threshold"
                )
                return None

            context = best_match.get("context", {})
            return {
                "intent": historical_intent,
                "phases_completed": context.get("phases_completed", 0),
                "findings_count": context.get("findings_count", 0),
                "affected_devices": context.get("affected_devices", []),
                "result_summary": context.get("result_summary", ""),
                "confidence": similarity,
            }

        except Exception as e:
            logger.warning(f"Historical diagnostic search failed: {e}")
            return None

    async def topology_analysis_node(self, state: DeepDiveState) -> dict:
        """Analyze user query to identify affected devices and fault scope.

        This is the first step in funnel debugging:
        1. [NEW] Search historical diagnostics (Agentic RAG Read)
        2. Extract device names from query
        3. Infer device roles (router, switch, firewall)
        4. Determine fault scope (single, local, path, domain)
        5. Query LLDP/topology if available

        Returns:
            Updated state with topology analysis
        """
        user_query = state["messages"][-1].content if state["messages"] else ""

        # ============================================
        # Step 0: Agentic RAG - Search Historical Diagnostics
        # ============================================
        historical_pattern = await self._search_historical_diagnostics(user_query)
        historical_context = ""
        if historical_pattern:
            logger.info(
                f"Found historical diagnostic pattern: {historical_pattern['intent'][:50]}... "
                f"(confidence: {historical_pattern['confidence']:.2f})"
            )
            historical_context = (
                f"\n\nüìö **ÂéÜÂè≤ÂèÇËÄÉ**: ÂèëÁé∞Á±ª‰ººÈóÆÈ¢òÁöÑÊàêÂäüËØäÊñ≠ËÆ∞ÂΩï\n"
                f"- ÂéÜÂè≤ÈóÆÈ¢ò: {historical_pattern['intent'][:100]}\n"
                f"- ËØäÊñ≠Èò∂ÊÆµ: {historical_pattern['phases_completed']} ‰∏™\n"
                f"- ÂèëÁé∞ÈóÆÈ¢ò: {historical_pattern['findings_count']} È°π\n"
                f"- ÂèÇËÄÉ‰ª∑ÂÄº: {historical_pattern['confidence']:.0%}\n"
            )

        # Extract device names using regex
        device_pattern = r"\b([A-Z]{1,4}[-_]?[A-Z0-9]*[-_]?[A-Z0-9]*\d+)\b"
        devices_mentioned = list(set(re.findall(device_pattern, user_query, re.IGNORECASE)))

        # Also catch common patterns like "R1", "SW1", "Core-R1"
        simple_pattern = r"\b([RSF][A-Za-z]*[-_]?\d+)\b"
        simple_devices = list(set(re.findall(simple_pattern, user_query, re.IGNORECASE)))
        devices_mentioned = list(set(devices_mentioned + simple_devices))

        logger.info(f"Topology analysis: devices mentioned = {devices_mentioned}")

        # If we have devices, try to get more context from SuzieQ LLDP
        topology_context = ""
        if devices_mentioned:
            try:
                from olav.tools.suzieq_parquet_tool import suzieq_query

                # Query LLDP for physical neighbors
                lldp_result = await suzieq_query.ainvoke(
                    {
                        "table": "lldp",
                        "method": "get",
                        "hostname": devices_mentioned[0] if len(devices_mentioned) == 1 else None,
                    }
                )
                if lldp_result.get("data"):
                    neighbors = [
                        f"{r.get('hostname')} ‚Üî {r.get('peerHostname')}"
                        for r in lldp_result["data"][:10]
                        if r.get("hostname") and r.get("peerHostname")
                    ]
                    topology_context = f"LLDPÈÇªÂ±Ö: {', '.join(neighbors)}"
            except Exception as e:
                logger.warning(f"LLDP query failed: {e}")

        # Use LLM to analyze topology (with historical context if available)
        prompt = prompt_manager.load_prompt(
            category="workflows/deep_dive",
            name="topology_analysis",
            user_query=user_query,
            devices_mentioned=", ".join(devices_mentioned) if devices_mentioned else "Êú™ÊòéÁ°ÆÊåáÂÆö",
        )

        # Enhance prompt with historical context
        if historical_pattern:
            prompt += f"\n\nÂéÜÂè≤ÂèÇËÄÉ‰ø°ÊÅØ:\n{historical_context}"

        response = await self.llm_json.ainvoke(
            [
                SystemMessage(content=prompt),
                HumanMessage(content=user_query),
            ]
        )

        try:
            analysis = json.loads(response.content)
            topology = TopologyAnalysis(
                source_device=analysis.get("topology_analysis", {}).get("source_device"),
                destination_device=analysis.get("topology_analysis", {}).get("destination_device"),
                path_hypothesis=analysis.get("topology_analysis", {}).get("path_hypothesis", []),
                affected_devices=analysis.get("topology_analysis", {}).get(
                    "affected_devices", devices_mentioned
                ),
                device_roles=analysis.get("topology_analysis", {}).get("device_roles", {}),
                scope=analysis.get("topology_analysis", {}).get("scope", "local"),
                confidence=analysis.get("topology_analysis", {}).get("confidence", "medium"),
            )
        except (json.JSONDecodeError, KeyError) as e:
            logger.warning(f"Topology analysis parse error: {e}")
            topology = TopologyAnalysis(
                source_device=devices_mentioned[0] if devices_mentioned else None,
                destination_device=devices_mentioned[1] if len(devices_mentioned) > 1 else None,
                path_hypothesis=devices_mentioned,
                affected_devices=devices_mentioned,
                device_roles=dict.fromkeys(devices_mentioned, "router"),
                scope="local",
                confidence="low",
            )

        # Generate user-friendly message
        scope_desc = {
            "single_device": "ÂçïËÆæÂ§áÈóÆÈ¢ò",
            "local": "Êú¨Âú∞ÈìæË∑Ø/ÈÇªÂ±ÖÈóÆÈ¢ò",
            "path": "Á´ØÂà∞Á´ØË∑ØÂæÑÈóÆÈ¢ò",
            "domain": "Âå∫Âüü/ÂüüÈóÆÈ¢ò",
        }

        msg = f"""## üó∫Ô∏è ÊãìÊâëÂàÜÊûê

**ÊïÖÈöúËåÉÂõ¥**: {scope_desc.get(topology["scope"], topology["scope"])}
**ÂèóÂΩ±ÂìçËÆæÂ§á**: {", ".join(topology["affected_devices"]) or "ÂæÖÁ°ÆÂÆö"}
**ÁΩÆ‰ø°Â∫¶**: {topology["confidence"]}

{topology_context if topology_context else ""}{historical_context}

Ê≠£Âú®ÁîüÊàêÂàÜÂ±ÇËØäÊñ≠ËÆ°Âàí..."""

        return {
            "topology": topology,
            "findings": [],
            "current_phase": 0,
            "messages": [AIMessage(content=msg)],
        }

    async def funnel_planning_node(self, state: DeepDiveState) -> dict:
        """Generate OSI layer-based diagnosis plan.

        Based on topology analysis, create a phased diagnosis plan:
        - Phase 1: L1 Physical (interfaces, LLDP)
        - Phase 2: L2 Data Link (MAC, VLAN) - if needed
        - Phase 3: L3 Network (ARP, routes)
        - Phase 4: L4+ Application (BGP, OSPF)

        Returns:
            Updated state with diagnosis_plan
        """
        user_query = state["messages"][-1].content if state["messages"] else ""
        topology = state.get("topology") or {}
        affected_devices = topology.get("affected_devices", [])

        # Build context for LLM
        topology_context = f"""
ÂèóÂΩ±ÂìçËÆæÂ§á: {", ".join(affected_devices)}
ÊïÖÈöúËåÉÂõ¥: {topology.get("scope", "unknown")}
Ë∑ØÂæÑÂÅáËÆæ: {" ‚Üí ".join(topology.get("path_hypothesis", []))}
"""

        # Use LLM to generate funnel diagnosis plan
        prompt = prompt_manager.load_prompt(
            category="workflows/deep_dive",
            name="funnel_diagnosis",
            user_query=user_query,
            topology_context=topology_context,
            affected_devices=", ".join(affected_devices),
        )

        response = await self.llm_json.ainvoke(
            [
                SystemMessage(content=prompt),
                HumanMessage(content=user_query),
            ]
        )

        try:
            plan_data = json.loads(response.content)

            # Convert to DiagnosisPlan
            phases: list[DiagnosisPhase] = []
            for p in plan_data.get("phases", []):
                checks: list[PhaseCheck] = []
                for c in p.get("checks", []):
                    checks.append(
                        PhaseCheck(
                            tool=c.get("tool", "suzieq_query"),
                            table=c.get("table"),
                            filters=c.get("filters", {}),
                            purpose=c.get("purpose", ""),
                            result=None,
                            status="pending",
                        )
                    )
                phases.append(
                    DiagnosisPhase(
                        phase=p.get("phase", 0),
                        layer=p.get("layer", "L1"),
                        name=p.get("name", ""),
                        checks=checks,
                        deep_dive_trigger=p.get("deep_dive_trigger"),
                        findings=[],
                        status="pending",
                    )
                )

            hypotheses: list[LayerHypothesis] = []
            for h in plan_data.get("diagnosis_plan", {}).get("hypothesis", []):
                hypotheses.append(
                    LayerHypothesis(
                        layer=h.get("layer", "L4"),
                        issue=h.get("issue", ""),
                        probability=h.get("probability", "medium"),
                        checks=[],
                    )
                )

            diagnosis_plan = DiagnosisPlan(
                summary=plan_data.get("diagnosis_plan", {}).get("summary", ""),
                affected_scope=plan_data.get("diagnosis_plan", {}).get(
                    "affected_scope", affected_devices
                ),
                hypotheses=hypotheses,
                phases=phases,
                current_phase=0,
                root_cause_identified=False,
                root_cause=None,
            )

        except (json.JSONDecodeError, KeyError) as e:
            logger.warning(f"Funnel plan parse error: {e}, using default plan")
            # Create default L1‚ÜíL4 plan
            diagnosis_plan = self._create_default_diagnosis_plan(affected_devices)

        # Format plan for user approval
        plan_msg = self._format_diagnosis_plan(diagnosis_plan)

        # Create execution_plan for HITL compatibility
        execution_plan: ExecutionPlan = {
            "feasible_tasks": list(range(1, len(diagnosis_plan["phases"]) + 1)),
            "uncertain_tasks": [],
            "infeasible_tasks": [],
            "recommendations": {},
            "user_approval_required": True,
        }

        return {
            "diagnosis_plan": diagnosis_plan,
            "execution_plan": execution_plan,
            "messages": [AIMessage(content=plan_msg)],
        }

    def _create_default_diagnosis_plan(self, affected_devices: list[str]) -> DiagnosisPlan:
        """Create default L1‚ÜíL4 diagnosis plan."""
        hostname_filter = {"hostname": affected_devices} if affected_devices else {}

        phases = [
            DiagnosisPhase(
                phase=1,
                layer="L1",
                name="Áâ©ÁêÜÂ±ÇÊ£ÄÊü•",
                checks=[
                    PhaseCheck(
                        tool="suzieq_query",
                        table="interfaces",
                        filters=hostname_filter,
                        purpose="Ê£ÄÊü•Êé•Âè£Áä∂ÊÄÅ",
                        result=None,
                        status="pending",
                    ),
                    PhaseCheck(
                        tool="suzieq_query",
                        table="lldp",
                        filters=hostname_filter,
                        purpose="È™åËØÅÁâ©ÁêÜÈÇªÂ±Ö",
                        result=None,
                        status="pending",
                    ),
                ],
                deep_dive_trigger="Êé•Âè£ down Êàñ LLDP ÈÇªÂ±ÖÁº∫Â§±",
                findings=[],
                status="pending",
            ),
            DiagnosisPhase(
                phase=2,
                layer="L3",
                name="ÁΩëÁªúÂ±ÇÊ£ÄÊü•",
                checks=[
                    PhaseCheck(
                        tool="suzieq_query",
                        table="arpnd",
                        filters=hostname_filter,
                        purpose="Ê£ÄÊü• ARP/ND Ë°®",
                        result=None,
                        status="pending",
                    ),
                    PhaseCheck(
                        tool="suzieq_query",
                        table="routes",
                        filters=hostname_filter,
                        purpose="Ê£ÄÊü•Ë∑ØÁî±Ë°®",
                        result=None,
                        status="pending",
                    ),
                ],
                deep_dive_trigger="ARP Áº∫Â§±ÊàñË∑ØÁî±‰∏çÂ≠òÂú®",
                findings=[],
                status="pending",
            ),
            DiagnosisPhase(
                phase=3,
                layer="L4",
                name="ÂçèËÆÆÂ±ÇÊ£ÄÊü•",
                checks=[
                    PhaseCheck(
                        tool="suzieq_query",
                        table="bgp",
                        filters=hostname_filter,
                        purpose="Ê£ÄÊü• BGP ÈÇªÂ±ÖÁä∂ÊÄÅ",
                        result=None,
                        status="pending",
                    ),
                ],
                deep_dive_trigger="BGP state != Established",
                findings=[],
                status="pending",
            ),
        ]

        return DiagnosisPlan(
            summary="ÈªòËÆ§ÂàÜÂ±ÇËØäÊñ≠ËÆ°Âàí: L1 Áâ©ÁêÜÂ±Ç ‚Üí L3 ÁΩëÁªúÂ±Ç ‚Üí L4 ÂçèËÆÆÂ±Ç",
            affected_scope=affected_devices,
            hypotheses=[
                LayerHypothesis(layer="L4", issue="ÂçèËÆÆÈÇªÂ±ÖÊú™Âª∫Á´ã", probability="high", checks=[]),
                LayerHypothesis(layer="L1", issue="Áâ©ÁêÜÊé•Âè£ÊïÖÈöú", probability="medium", checks=[]),
            ],
            phases=phases,
            current_phase=0,
            root_cause_identified=False,
            root_cause=None,
        )

    def _format_diagnosis_plan(self, plan: DiagnosisPlan) -> str:
        """Format diagnosis plan for user review."""
        lines = [
            "## üìã ÊºèÊñóÂºèËØäÊñ≠ËÆ°Âàí\n",
            f"**Ê¶ÇËø∞**: {plan['summary']}\n",
            f"**ÂèóÂΩ±ÂìçËåÉÂõ¥**: {', '.join(plan['affected_scope'])}\n",
        ]

        if plan["hypotheses"]:
            lines.append("\n### üîç ÂàùÊ≠•ÂÅáËÆæ\n")
            for h in plan["hypotheses"]:
                prob_emoji = {"high": "üî¥", "medium": "üü°", "low": "üü¢"}.get(h["probability"], "‚ö™")
                lines.append(
                    f"- {prob_emoji} **{h['layer']}**: {h['issue']} (Ê¶ÇÁéá: {h['probability']})"
                )

        lines.append("\n### üìä ËØäÊñ≠Èò∂ÊÆµ\n")
        for phase in plan["phases"]:
            layer_emoji = {"L1": "üîå", "L2": "üîó", "L3": "üåê", "L4": "üì°"}.get(phase["layer"], "üìã")
            lines.append(
                f"\n**Phase {phase['phase']}: {layer_emoji} {phase['name']}** ({phase['layer']})"
            )
            for check in phase["checks"]:
                lines.append(f"  - `{check['table']}`: {check['purpose']}")
            if phase["deep_dive_trigger"]:
                lines.append(f"  - ‚ö° Ê∑±ÂÖ•Êù°‰ª∂: {phase['deep_dive_trigger']}")

        lines.append("\n---")
        lines.append(f"\nüìä **ËÆ°ÂàíÊëòË¶Å**: {len(plan['phases'])} ‰∏™ËØäÊñ≠Èò∂ÊÆµ")
        lines.append("\n```")
        lines.append(f"  {tr('action_approve')}")
        lines.append(f"  {tr('action_abort')}")
        lines.append("```")

        return "\n".join(lines)

    async def macro_scan_node(self, state: DeepDiveState) -> dict:
        """Execute SuzieQ checks for current phase (Macro Scan).

        This node:
        1. Gets current phase from diagnosis_plan
        2. Executes all SuzieQ checks for that phase
        3. Collects findings (anomalies)
        4. Updates phase status

        Returns:
            Updated state with check results and findings
        """
        from config.settings import AgentConfig
        from langgraph.types import interrupt

        diagnosis_plan = state.get("diagnosis_plan")
        if not diagnosis_plan:
            return {"messages": [AIMessage(content="‚ùå ËØäÊñ≠ËÆ°ÂàíÁº∫Â§±")]}

        user_approval = state.get("user_approval")

        # YOLO mode: auto-approve
        if AgentConfig.YOLO_MODE and user_approval is None:
            logger.info("[YOLO] Auto-approving diagnosis plan...")
            user_approval = "approved"

        # HITL: Check if approval needed
        execution_plan = state.get("execution_plan", {})
        if execution_plan.get("user_approval_required") and user_approval is None:
            approval_response = interrupt(
                {
                    "action": "approval_required",
                    "execution_plan": execution_plan,
                    "diagnosis_plan": diagnosis_plan,
                    "message": "ËØ∑ÂÆ°ÊâπËØäÊñ≠ËÆ°ÂàíÔºöY=ÁªßÁª≠, N=ÁªàÊ≠¢",
                }
            )

            if isinstance(approval_response, dict):
                if approval_response.get("approved"):
                    user_approval = "approved"
                    return {
                        "user_approval": user_approval,
                        "messages": [AIMessage(content="‚úÖ ËØäÊñ≠ËÆ°ÂàíÂ∑≤ÊâπÂáÜÔºåÂºÄÂßãÂÆèËßÇÊâ´Êèè...")],
                    }
                return {
                    "user_approval": "aborted",
                    "messages": [AIMessage(content="‚õî Áî®Êà∑Â∑≤‰∏≠Ê≠¢ËØäÊñ≠„ÄÇ")],
                }
            return {
                "user_approval": "approved",
                "messages": [AIMessage(content="‚úÖ ËØäÊñ≠ËÆ°ÂàíÂ∑≤ÊâπÂáÜÔºåÂºÄÂßãÂÆèËßÇÊâ´Êèè...")],
            }

        # Execute current phase
        current_phase_idx = state.get("current_phase", 0)
        phases = diagnosis_plan.get("phases", [])

        if current_phase_idx >= len(phases):
            return {"messages": [AIMessage(content="ÊâÄÊúâËØäÊñ≠Èò∂ÊÆµÂ∑≤ÂÆåÊàê„ÄÇ")]}

        phase = phases[current_phase_idx]
        phase["status"] = "running"

        logger.info(f"Executing Phase {phase['phase']}: {phase['name']}")

        # Execute checks
        from olav.tools.suzieq_parquet_tool import suzieq_query

        phase_findings: list[str] = []
        check_results: list[str] = []

        for check in phase["checks"]:
            check["status"] = "running"
            try:
                result = await suzieq_query.ainvoke(
                    {
                        "table": check["table"],
                        "method": "get",
                        **check["filters"],
                    }
                )
                check["result"] = result
                check["status"] = "completed"

                # Analyze result for anomalies
                findings = self._analyze_check_result(check["table"], result, check["purpose"])
                phase_findings.extend(findings)

                # Format result summary
                count = result.get("count", len(result.get("data", [])))
                table_name = self._get_table_display_name(check["table"])
                check_results.append(f"‚úÖ {table_name}: {count} Êù°ËÆ∞ÂΩï")

                if findings:
                    for f in findings:
                        check_results.append(f"  ‚ö†Ô∏è {f}")

            except Exception as e:
                check["status"] = "failed"
                check["result"] = {"error": str(e)}
                check_results.append(f"‚ùå {check['table']}: {e}")

        phase["findings"] = phase_findings
        phase["status"] = "completed"
        state["findings"].extend(phase_findings)

        # Format phase result
        layer_emoji = {"L1": "üîå", "L2": "üîó", "L3": "üåê", "L4": "üì°"}.get(phase["layer"], "üìã")
        msg = f"""## {layer_emoji} Phase {phase["phase"]}: {phase["name"]} ÂÆåÊàê

### Ê£ÄÊü•ÁªìÊûú
{chr(10).join(check_results)}

### ÂèëÁé∞ ({len(phase_findings)} È°π)
{chr(10).join(f"- {f}" for f in phase_findings) if phase_findings else "- Êú™ÂèëÁé∞ÂºÇÂ∏∏"}
"""

        # Move to next phase
        new_phase_idx = current_phase_idx + 1

        return {
            "diagnosis_plan": diagnosis_plan,
            "current_phase": new_phase_idx,
            "findings": state["findings"],
            "user_approval": user_approval,
            "messages": [AIMessage(content=msg)],
        }

    def _analyze_check_result(self, table: str, result: dict, purpose: str) -> list[str]:
        """Analyze SuzieQ query result for anomalies."""
        findings = []
        data = result.get("data", [])

        if not data:
            findings.append(f"{table}: Êó†Êï∞ÊçÆÔºàÂèØËÉΩÈááÈõÜÈóÆÈ¢òÊàñËåÉÂõ¥ÈîôËØØÔºâ")
            return findings

        # Table-specific anomaly detection
        if table == "interfaces":
            down_ifs = [
                r for r in data if r.get("state") == "down" and r.get("adminState") != "down"
            ]
            if down_ifs:
                for iface in down_ifs[:5]:
                    findings.append(
                        f"Êé•Âè£ {iface.get('hostname')}:{iface.get('ifname')} Áä∂ÊÄÅÂºÇÂ∏∏ (adminUp, operDown)"
                    )

        elif table == "bgp":
            not_estd = [r for r in data if r.get("state") != "Established"]
            if not_estd:
                for peer in not_estd[:5]:
                    reason = peer.get("reason") or peer.get("notificnReason") or "Êú™Áü•"
                    findings.append(
                        f"BGP {peer.get('hostname')} ‚Üî {peer.get('peer')}: {peer.get('state')} ({reason})"
                    )

        elif table in {"ospfNbr", "ospfIf"}:
            not_full = [r for r in data if r.get("state") not in ("full", "Full", "dr", "bdr")]
            if not_full:
                for nbr in not_full[:5]:
                    findings.append(
                        f"OSPF {nbr.get('hostname')}:{nbr.get('ifname')} ÈÇªÂ±ÖÁä∂ÊÄÅ: {nbr.get('state')}"
                    )

        elif table == "lldp":
            # Check for missing expected neighbors (would need topology baseline)
            if len(data) == 0:
                findings.append("LLDP: Êú™ÂèëÁé∞ÈÇªÂ±ÖÔºàÁâ©ÁêÜËøûÊé•ÂèØËÉΩÊñ≠ÂºÄÔºâ")

        elif table == "arpnd":
            # Check for incomplete ARP entries
            incomplete = [r for r in data if r.get("state") in ("incomplete", "INCOMPLETE")]
            if incomplete:
                for arp in incomplete[:5]:
                    findings.append(f"ARP {arp.get('hostname')}: {arp.get('ipAddress')} Áä∂ÊÄÅ‰∏çÂÆåÊï¥")

        return findings

    async def evaluate_findings_node(self, state: DeepDiveState) -> dict:
        """Evaluate findings and decide next step.

        Decision logic:
        1. If critical findings ‚Üí trigger micro diagnosis
        2. If more phases ‚Üí continue macro scan
        3. If all done ‚Üí go to summary

        Returns:
            Updated state with next action decision
        """
        diagnosis_plan = state.get("diagnosis_plan")
        if not diagnosis_plan:
            return {"trigger_recursion": False}

        current_phase = state.get("current_phase", 0)
        phases = diagnosis_plan.get("phases", [])
        findings = state.get("findings", [])

        # Check if we have critical findings that need micro diagnosis
        critical_keywords = ["down", "ÂºÇÂ∏∏", "Â§±Ë¥•", "NotEstd", "incomplete"]
        critical_findings = [f for f in findings if any(k in f for k in critical_keywords)]

        if critical_findings and current_phase < len(phases):
            # Found issues - may need micro diagnosis
            logger.info(f"Critical findings detected: {len(critical_findings)}")

        # Check if more phases to run
        if current_phase < len(phases):
            return {"trigger_recursion": True}  # Continue to next phase

        # All phases done
        return {"trigger_recursion": False}

    async def realtime_verification_node(self, state: DeepDiveState) -> dict:
        """Verify SuzieQ findings with real-time CLI/NETCONF data.

        CRITICAL: SuzieQ data is historical (Parquet snapshots).
        This node uses live device queries to confirm findings.

        Flow:
        1. Parse findings to extract device/interface/peer info
        2. Execute targeted CLI commands for real-time state
        3. Compare with SuzieQ findings
        4. Update findings with verification status

        Returns:
            Updated state with verified/unverified findings
        """
        from olav.tools.nornir_tool import CLITool

        findings = state.get("findings", [])
        topology = state.get("topology") or {}
        affected_devices = topology.get("affected_devices", [])

        if not findings:
            return {
                "messages": [AIMessage(content="üìã SuzieQ Êú™ÂèëÁé∞ÂºÇÂ∏∏ÔºåË∑≥ËøáÂÆûÊó∂È™åËØÅ„ÄÇ")],
                "realtime_verified": True,
            }

        logger.info(f"Starting real-time verification for {len(findings)} findings...")

        # Initialize CLI tool
        try:
            cli_tool = CLITool()
        except Exception as e:
            logger.warning(f"CLI tool initialization failed: {e}")
            return {
                "messages": [
                    AIMessage(
                        content=f"‚ö†Ô∏è Êó†Ê≥ïÂàùÂßãÂåñ CLI Â∑•ÂÖ∑: {e}\nÂ∞Ü‰ΩøÁî® SuzieQ ÂéÜÂè≤Êï∞ÊçÆ‰Ωú‰∏∫ÂèÇËÄÉ„ÄÇ"
                    )
                ],
                "realtime_verified": False,
            }

        verified_findings: list[str] = []
        realtime_data: dict[str, list] = {}
        verification_results: list[str] = []

        # Determine which commands to run based on findings
        commands_to_run: dict[str, list[str]] = {}

        for finding in findings:
            finding_lower = finding.lower()

            # Parse devices from finding
            devices = []
            for device in affected_devices:
                if device.lower() in finding_lower:
                    devices.append(device)

            if not devices:
                devices = affected_devices[:2]  # Default to first 2 devices

            for device in devices:
                if device not in commands_to_run:
                    commands_to_run[device] = []

                # Add relevant commands based on finding type
                if "bgp" in finding_lower or "notestd" in finding_lower:
                    commands_to_run[device].extend(
                        [
                            "show ip bgp summary",
                            "show ip bgp neighbors",
                        ]
                    )
                elif (
                    "Êé•Âè£" in finding_lower
                    or "interface" in finding_lower
                    or "down" in finding_lower
                ):
                    commands_to_run[device].extend(
                        [
                            "show ip interface brief",
                            "show interfaces status",
                        ]
                    )
                elif "ospf" in finding_lower:
                    commands_to_run[device].extend(
                        [
                            "show ip ospf neighbor",
                            "show ip ospf interface brief",
                        ]
                    )
                elif "arp" in finding_lower:
                    commands_to_run[device].append("show arp")
                elif "lldp" in finding_lower:
                    commands_to_run[device].append("show lldp neighbors")
                elif "route" in finding_lower or "Ë∑ØÁî±" in finding_lower:
                    commands_to_run[device].append("show ip route summary")

        # Deduplicate commands per device
        for device, cmds in commands_to_run.items():
            commands_to_run[device] = list(set(cmds))

        # Execute commands on each device
        for device, commands in commands_to_run.items():
            realtime_data[device] = []

            for command in commands[:3]:  # Limit to 3 commands per device
                try:
                    result = await cli_tool.execute(device=device, command=command)

                    if result.error:
                        verification_results.append(f"‚ùå {device} `{command}`: {result.error}")
                    else:
                        realtime_data[device].append(
                            {
                                "command": command,
                                "data": result.data,
                            }
                        )
                        verification_results.append(
                            f"‚úÖ {device} `{command}`: {len(result.data)} Êù°ËÆ∞ÂΩï"
                        )

                except Exception as e:
                    verification_results.append(f"‚ùå {device} `{command}`: ÊâßË°åÂ§±Ë¥• - {e}")

        # Verify findings against real-time data
        for finding in findings:
            verified = False
            finding_lower = finding.lower()

            # Check if any real-time data confirms the finding
            for _device, data_list in realtime_data.items():
                for data_entry in data_list:
                    data = data_entry.get("data", [])

                    # BGP verification
                    if "bgp" in finding_lower and "bgp" in data_entry.get("command", "").lower():
                        for row in data:
                            state = str(row.get("state", row.get("State", ""))).lower()
                            if state in ("idle", "active", "connect", "opensent", "openconfirm"):
                                verified = True
                                break

                    # Interface verification
                    elif (
                        "down" in finding_lower or "Êé•Âè£" in finding_lower
                    ) and "interface" in data_entry.get("command", "").lower():
                        for row in data:
                            status = str(row.get("status", row.get("Status", ""))).lower()
                            if status in ("down", "administratively down", "notconnect"):
                                verified = True
                                break

            if verified:
                verified_findings.append(f"‚úÖ [ÂÆûÊó∂Á°ÆËÆ§] {finding}")
            else:
                verified_findings.append(f"‚ö†Ô∏è [ÂéÜÂè≤Êï∞ÊçÆ] {finding}")

        # Format verification report
        msg = f"""## üîç ÂÆûÊó∂È™åËØÅÁªìÊûú

### ÊâßË°åÁöÑÂëΩ‰ª§
{chr(10).join(verification_results) if verification_results else "- Êó†Ê≥ïÊâßË°åÂÆûÊó∂ÂëΩ‰ª§"}

### È™åËØÅÂêéÁöÑÂèëÁé∞
{chr(10).join(f"- {f}" for f in verified_findings)}

**ËØ¥Êòé**:
- ‚úÖ [ÂÆûÊó∂Á°ÆËÆ§] = CLI ÂÆûÊó∂Êï∞ÊçÆËØÅÂÆû‰∫Ü SuzieQ ÁöÑÂèëÁé∞
- ‚ö†Ô∏è [ÂéÜÂè≤Êï∞ÊçÆ] = ‰ªÖÊúâ SuzieQ ÂéÜÂè≤ËÆ∞ÂΩïÔºåÊú™ËÉΩÂÆûÊó∂È™åËØÅ
"""

        return {
            "findings": [
                f.replace("‚úÖ [ÂÆûÊó∂Á°ÆËÆ§] ", "").replace("‚ö†Ô∏è [ÂéÜÂè≤Êï∞ÊçÆ] ", "")
                for f in verified_findings
            ],
            "realtime_data": realtime_data,
            "realtime_verified": len(verification_results) > 0,
            "messages": [AIMessage(content=msg)],
        }

    async def root_cause_summary_node(self, state: DeepDiveState) -> dict:
        """Generate root cause analysis summary.

        Correlates all findings across phases and generates:
        1. Root cause identification
        2. Evidence trail
        3. Recommended actions

        Returns:
            Final summary message
        """
        diagnosis_plan = state.get("diagnosis_plan") or {}
        topology = state.get("topology") or {}
        findings = state.get("findings", [])
        user_query = ""
        for msg in state.get("messages", []):
            if isinstance(msg, HumanMessage):
                user_query = msg.content
                break

        # Prepare summary context
        phases_summary = []
        for phase in diagnosis_plan.get("phases", []):
            phase_findings = phase.get("findings", [])
            phases_summary.append(
                f"**{phase['name']}** ({phase['layer']}): {len(phase_findings)} findings"
            )
            for f in phase_findings[:3]:
                phases_summary.append(f"  - {f}")

        # Format findings for prompt
        findings_text = "\n".join(f"- {f}" for f in findings) if findings else "- No obvious anomalies found"

        # Load prompt from YAML
        try:
            prompt = prompt_manager.load_prompt(
                "workflows/deep_dive",
                "root_cause_report",
                user_query=user_query,
                affected_devices=", ".join(topology.get("affected_devices", [])),
                scope=topology.get("scope", "unknown"),
                phases_summary=chr(10).join(phases_summary),
                findings=findings_text,
            )
        except (FileNotFoundError, ValueError) as e:
            logger.warning(f"Failed to load root_cause_report prompt: {e}, using fallback")
            prompt = f"Generate root cause analysis for: {user_query}\nFindings: {findings_text}"

        response = await self.llm.ainvoke(
            [
                SystemMessage(content=prompt),
            ]
        )

        # Save to episodic memory if enabled
        if settings.enable_deep_dive_memory and findings:
            try:
                memory_writer = get_memory_writer()
                await memory_writer.memory.store_episodic_memory(
                    intent=user_query,
                    xpath=f"funnel_diagnosis:{len(findings)} findings",
                    success=len([f for f in findings if "down" in f or "ÂºÇÂ∏∏" in f]) == 0,
                    context={
                        "tool_used": "deep_dive_funnel",
                        "phases_completed": len(diagnosis_plan.get("phases", [])),
                        "findings_count": len(findings),
                        "affected_devices": topology.get("affected_devices", []),
                    },
                )
            except Exception as e:
                logger.warning(f"Failed to save to episodic memory: {e}")

        return {
            "messages": [AIMessage(content=response.content)],
        }

    async def should_continue_funnel(
        self, state: DeepDiveState
    ) -> Literal["macro_scan", "realtime_verification"]:
        """Decide whether to continue scanning or proceed to verification.

        Returns:
            "macro_scan" if more phases to process
            "realtime_verification" if all SuzieQ phases done, need live verification
        """
        if state.get("trigger_recursion"):
            return "macro_scan"
        return "realtime_verification"

    # ============================================
    # LEGACY: Task Planning Nodes (backward compat)
    # ============================================

    async def task_planning_node(self, state: DeepDiveState) -> dict:
        """Generate Todo List from user query using LLM.

        Args:
            state: Current workflow state

        Returns:
            Updated state with generated todos
        """
        user_query = state["messages"][-1].content if state["messages"] else ""

        # Load task planning prompt
        prompt = prompt_manager.load_prompt(
            category="workflows/deep_dive",
            name="task_planning",
            user_query=user_query,
            recursion_depth=state.get("recursion_depth", 0),
            max_depth=state.get("max_depth", 3),
        )

        # LLM generates structured Todo List
        messages = [SystemMessage(content=prompt), HumanMessage(content=user_query)]
        response = await self.llm_json.ainvoke(messages)

        # Parse JSON response to TodoItem list

        try:
            todo_data = json.loads(response.content)
            todos = [
                TodoItem(
                    id=item["id"],
                    task=item["task"],
                    status="pending",
                    result=None,
                    deps=item.get("deps", []),
                )
                for item in todo_data.get("todos", [])
            ]
        except (json.JSONDecodeError, KeyError):
            # Fallback: Create single todo from query
            todos = [TodoItem(id=1, task=user_query, status="pending", result=None, deps=[])]

        return {
            "todos": todos,
            "execution_plan": None,
            "completed_results": {},
            "recursion_depth": state.get("recursion_depth", 0),
            "max_depth": state.get("max_depth", 3),
            "trigger_recursion": False,
        }

    async def schema_investigation_node(self, state: DeepDiveState) -> dict:
        """Investigate schema feasibility for all planned tasks.

        This node:
        1. Calls suzieq_schema_search for each task to discover available tables
        2. Validates keyword mapping against schema results
        3. Categorizes tasks as feasible/uncertain/infeasible
        4. Generates execution plan with recommendations

        Returns:
            Updated state with execution_plan for user approval
        """
        from olav.tools.suzieq_parquet_tool import suzieq_schema_search

        todos = state["todos"]
        feasible_tasks = []
        uncertain_tasks = []
        infeasible_tasks = []
        recommendations = {}

        for todo in todos:
            task_text = todo["task"]
            task_id = todo["id"]

            # Step 1: Keyword-based mapping (heuristic)
            heuristic_mapping = self._map_task_to_table(task_text)

            # Step 2: Schema search (ground truth)
            try:
                schema_result = await suzieq_schema_search.ainvoke({"query": task_text})
                available_tables = schema_result.get("tables", [])

                if not available_tables:
                    # No schema match at all
                    todo["feasibility"] = "infeasible"
                    todo["schema_notes"] = "Á≥ªÁªü‰∏≠Ê≤°ÊúâÊâæÂà∞Áõ∏ÂÖ≥ÁöÑÊï∞ÊçÆË°®ÔºåÂèØËÉΩÈúÄË¶ÅÁõ¥Êé•ËøûÊé•ËÆæÂ§áÊü•ËØ¢"
                    infeasible_tasks.append(task_id)
                    recommendations[task_id] = (
                        "Âª∫ËÆÆÈÄöËøá NETCONF Áõ¥Êé•Êü•ËØ¢ËÆæÂ§áÔºåÊàñÊ£ÄÊü•Êï∞ÊçÆÈááÈõÜÊòØÂê¶Ê≠£Â∏∏"
                    )

                elif heuristic_mapping:
                    # Validate heuristic against schema
                    heuristic_table = heuristic_mapping[0]
                    if heuristic_table in available_tables:
                        # Perfect match
                        todo["feasibility"] = "feasible"
                        todo["recommended_table"] = heuristic_table
                        # Get human-readable field names
                        fields = schema_result.get(heuristic_table, {}).get("fields", [])[:5]
                        field_desc = self._humanize_fields(fields)
                        todo["schema_notes"] = (
                            f"Â∞Ü‰ªé {heuristic_table} Ë°®Êü•ËØ¢ÔºåÂåÖÂê´ {field_desc} Á≠âÂ≠óÊÆµ"
                        )
                        feasible_tasks.append(task_id)
                        recommendations[task_id] = f"ÊâßË°åÊü•ËØ¢: {heuristic_table} Ë°®"
                    else:
                        # Heuristic mismatch - use first schema suggestion
                        suggested_table = available_tables[0]
                        todo["feasibility"] = "uncertain"
                        todo["recommended_table"] = suggested_table
                        todo["schema_notes"] = (
                            f"‰ªªÂä°ÊèèËø∞ÊåáÂêë {heuristic_table}Ôºå‰ΩÜÁ≥ªÁªüÂª∫ËÆÆ‰ΩøÁî® {suggested_table} Ë°®"
                        )
                        uncertain_tasks.append(task_id)
                        recommendations[task_id] = (
                            f"ËØ∑Á°ÆËÆ§Ôºö‰ΩøÁî® {suggested_table} ËøòÊòØ {heuristic_table}Ôºü"
                        )

                else:
                    # No heuristic mapping, but schema has suggestions
                    suggested_table = available_tables[0]
                    todo["feasibility"] = "uncertain"
                    todo["recommended_table"] = suggested_table
                    tables_desc = "„ÄÅ".join(available_tables[:3])
                    todo["schema_notes"] = f"Êó†Ê≥ïËá™Âä®ËØÜÂà´Êï∞ÊçÆÊ∫êÔºåÂèØËÉΩÁöÑË°®: {tables_desc}"
                    uncertain_tasks.append(task_id)
                    recommendations[task_id] = f"Âª∫ËÆÆ‰ΩøÁî® {suggested_table} Ë°®ÔºåÊàñÊåáÂÆöÂÖ∂‰ªñÊï∞ÊçÆÊ∫ê"

            except Exception as e:
                # Schema search failed
                todo["feasibility"] = "uncertain"
                todo["schema_notes"] = f"Êü•ËØ¢Êï∞ÊçÆÊ∫êÊó∂Âá∫Èîô: {e!s}"
                uncertain_tasks.append(task_id)
                recommendations[task_id] = "ËØ∑ÈáçËØïÊàñÊâãÂä®ÊåáÂÆöÊï∞ÊçÆÊ∫ê"

        # Generate execution plan
        # HITL: DeepDive always requires user approval before execution
        # This is a safety measure to prevent unintended operations
        execution_plan: ExecutionPlan = {
            "feasible_tasks": feasible_tasks,
            "uncertain_tasks": uncertain_tasks,
            "infeasible_tasks": infeasible_tasks,
            "recommendations": recommendations,
            "user_approval_required": True,  # Always require approval for DeepDive
        }

        # Generate plan summary message
        plan_summary = self._format_execution_plan(todos, execution_plan)

        return {
            "todos": todos,
            "execution_plan": execution_plan,
            "messages": [AIMessage(content=plan_summary)],
        }

    def _format_execution_plan(self, todos: list[TodoItem], plan: ExecutionPlan) -> str:
        """Format execution plan for user review with human-friendly descriptions."""
        lines = [tr("plan_title")]

        if plan["feasible_tasks"]:
            lines.append(tr("ready_section", count=len(plan["feasible_tasks"])))
            for task_id in plan["feasible_tasks"]:
                todo = next(td for td in todos if td["id"] == task_id)
                # Clean up task description for readability
                task_desc = self._humanize_task(todo["task"])
                lines.append(f"**{task_id}.** {task_desc}")
                lines.append(f"   ‚Ü≥ {todo['schema_notes']}\n")

        if plan["uncertain_tasks"]:
            lines.append(tr("uncertain_section", count=len(plan["uncertain_tasks"])))
            for task_id in plan["uncertain_tasks"]:
                todo = next(td for td in todos if td["id"] == task_id)
                task_desc = self._humanize_task(todo["task"])
                lines.append(f"**{task_id}.** {task_desc}")
                lines.append(f"   ‚Ü≥ {todo['schema_notes']}")
                lines.append(f"   üí° {plan['recommendations'][task_id]}\n")

        if plan["infeasible_tasks"]:
            lines.append(tr("infeasible_section", count=len(plan["infeasible_tasks"])))
            for task_id in plan["infeasible_tasks"]:
                todo = next(td for td in todos if td["id"] == task_id)
                task_desc = self._humanize_task(todo["task"])
                lines.append(f"**{task_id}.** {task_desc}")
                lines.append(f"   ‚Ü≥ {todo['schema_notes']}")
                lines.append(f"   üí° {plan['recommendations'][task_id]}\n")

        # Approval prompt
        lines.append("\n---")
        total = (
            len(plan["feasible_tasks"])
            + len(plan["uncertain_tasks"])
            + len(plan["infeasible_tasks"])
        )
        ready = len(plan["feasible_tasks"])

        if plan.get("uncertain_tasks") or plan.get("infeasible_tasks"):
            lines.append(f"\n{tr('plan_summary_partial', ready=ready, total=total)}")
            lines.append(tr("plan_confirmation"))
        else:
            lines.append(f"\n{tr('plan_summary_full', total=total)}")

        lines.append("```")
        lines.append(f"  {tr('action_approve')}")
        lines.append(f"  {tr('action_abort')}")
        lines.append(f"  {tr('action_modify')}")
        lines.append("```")

        return "\n".join(lines)

    def _humanize_task(self, task: str) -> str:
        """Convert machine-style task description to human-readable format."""
        # Remove suzieq_query prefix patterns
        import re

        # Language-specific device config query replacement
        device_config_label = {
            "zh": "ËÆæÂ§áÈÖçÁΩÆÊü•ËØ¢: ",
            "en": "Device config query: ",
            "ja": "„Éá„Éê„Ç§„ÇπË®≠ÂÆö„ÇØ„Ç®„É™: ",
        }.get(AgentConfig.LANGUAGE, "Device config query: ")

        task = re.sub(r"suzieq_query\s*:?\s*", "", task, flags=re.IGNORECASE)
        task = re.sub(r"table\s*=\s*\w+\s*", "", task)
        task = re.sub(r"hostname\s*=\s*\[?['\"]?\w+['\"]?\]?\s*", "", task)
        task = re.sub(r"netconf_tool\s*:?\s*", device_config_label, task, flags=re.IGNORECASE)

        # Clean up extra whitespace
        task = re.sub(r"\s+", " ", task).strip()

        # Remove leading commas or punctuation
        task = re.sub(r"^[,\s]+", "", task)

        return task if task else tr("default_task")

    def _humanize_fields(self, fields: list[str]) -> str:
        """Convert field names to human-readable descriptions."""
        readable = []
        for f in fields[:4]:  # Limit to 4 fields
            # Try to get translated field label
            label = tr(f"field_{f}")
            # If not found (returns the key itself), use original field name
            readable.append(label if label != f"field_{f}" else f)

        # Use language-appropriate separator
        separator = {"zh": "„ÄÅ", "en": ", ", "ja": "„ÄÅ"}.get(AgentConfig.LANGUAGE, ", ")
        return separator.join(readable)

    def _get_table_display_name(self, table: str) -> str:
        """Get human-readable display name for a table."""
        # Try to get translated table name
        label = tr(f"table_{table.lower()}")
        # If not found (returns the key itself), use original table name
        return label if label != f"table_{table.lower()}" else table

    async def execute_todo_node(self, state: DeepDiveState) -> dict:
        """Execute next eligible todo with real tool invocation where possible.

        This node first checks if HITL approval is needed:
        - If execution_plan.user_approval_required and not yet approved, interrupt()
        - User can approve, modify, or abort
        - After approval, execute feasible tasks

        Priority:
        1. Heuristic keyword mapping (device, interface, routes, bgp, etc.)
        2. Schema existence check via suzieq_schema_search
        3. Distinguish SCHEMA_NOT_FOUND vs NO_DATA_FOUND vs OK
        4. Fallback to LLM-driven execution prompt if mapping fails or table unsupported
        """
        import asyncio  # Local import to avoid global side-effects

        from config.settings import AgentConfig
        from langgraph.types import interrupt

        todos = state["todos"]
        completed_results = state.get("completed_results", {})
        execution_plan = state.get("execution_plan", {})
        user_approval = state.get("user_approval")

        # YOLO mode: auto-approve without user interaction
        if AgentConfig.YOLO_MODE and user_approval is None:
            print("[YOLO] Auto-approving execution plan...")
            user_approval = "approved"

        # HITL: Check if approval is needed before first execution
        if (
            execution_plan
            and execution_plan.get("user_approval_required")
            and user_approval is None
        ):
            # Interrupt for user approval
            approval_response = interrupt(
                {
                    "action": "approval_required",
                    "execution_plan": execution_plan,
                    "todos": todos,
                    "message": "ËØ∑ÂÆ°ÊâπÊâßË°åËÆ°ÂàíÔºöapprove=ÁªßÁª≠, abort=ÁªàÊ≠¢, ÊàñËæìÂÖ•‰øÆÊîπËØ∑Ê±Ç",
                }
            )

            # Process approval response (returned by Command(resume=...))
            if isinstance(approval_response, dict):
                if approval_response.get("approved"):
                    user_approval = approval_response.get("user_approval", "approved")
                    if approval_response.get("modified_plan"):
                        execution_plan = approval_response["modified_plan"]
                        # Return immediately to update state with new plan
                        return {
                            "user_approval": user_approval,
                            "execution_plan": execution_plan,
                        }
                    # CRITICAL: Return immediately after approval to persist state
                    # Then next loop iteration will have user_approval set
                    return {
                        "user_approval": user_approval,
                        "messages": [AIMessage(content="‚úÖ Áî®Êà∑Â∑≤ÊâπÂáÜÊâßË°åËÆ°ÂàíÔºåÂºÄÂßãÊâßË°å‰ªªÂä°...")],
                    }
                # User aborted
                return {
                    "messages": [AIMessage(content="‚õî Áî®Êà∑Â∑≤‰∏≠Ê≠¢ÊâßË°åËÆ°Âàí„ÄÇ")],
                    "user_approval": "aborted",
                }
            # Simple resume value (just approval) - also return immediately
            return {
                "user_approval": "approved",
                "messages": [AIMessage(content="‚úÖ Áî®Êà∑Â∑≤ÊâπÂáÜÊâßË°åËÆ°ÂàíÔºåÂºÄÂßãÊâßË°å‰ªªÂä°...")],
            }

        # ------------------------------------------------------------------
        # Parallel batch execution (Phase 3.2)
        # Strategy: Identify all ready & dependency-satisfied todos without deps.
        # Run up to parallel_batch_size concurrently. Falls back to serial path
        # when <=1 independent ready todo.
        # ------------------------------------------------------------------
        parallel_batch_size = state.get("parallel_batch_size", 5)

        ready: list[TodoItem] = []
        for todo in todos:
            if todo["status"] == "pending":
                deps_ok = all(
                    any(t["id"] == dep_id and t["status"] in {"completed", "failed"} for t in todos)
                    for dep_id in todo["deps"]
                )
                if deps_ok:
                    ready.append(todo)

        independent = [t for t in ready if not t["deps"]]

        if len(independent) > 1:
            batch = independent[:parallel_batch_size]
            # Mark batch in-progress
            for t in batch:
                t["status"] = "in-progress"

            async def _execute_single(todo: TodoItem) -> tuple[TodoItem, list[BaseMessage]]:
                task_text = todo["task"].strip()
                mapping = self._map_task_to_table(task_text)
                tool_result: dict | None = None
                messages: list[BaseMessage] = []
                if mapping:
                    table, method, extra_filters = mapping

                    # Special handling for syslog - uses OpenSearch instead of SuzieQ
                    if table == "syslog":
                        try:
                            from olav.tools.syslog_tool import syslog_search

                            syslog_keywords = self._extract_syslog_keywords(task_text)
                            device_ip = extra_filters.get("device_ip")

                            syslog_result = await syslog_search.ainvoke(
                                {
                                    "keyword": syslog_keywords,
                                    "device_ip": device_ip,
                                    "start_time": "now-1h",
                                    "limit": 50,
                                }
                            )

                            if syslog_result.get("success") and syslog_result.get("data"):
                                tool_result = {
                                    "status": "SUCCESS",
                                    "table": "syslog",
                                    "data": syslog_result["data"],
                                    "count": len(syslog_result["data"]),
                                    "columns": ["timestamp", "device_ip", "severity", "message"],
                                }
                            else:
                                tool_result = {
                                    "status": "NO_DATA_FOUND",
                                    "table": "syslog",
                                    "message": syslog_result.get("error")
                                    or "No syslog entries found",
                                    "hint": f"Keywords: {syslog_keywords}",
                                }
                        except Exception as e:
                            tool_result = {
                                "status": "TOOL_ERROR",
                                "table": "syslog",
                                "error": str(e),
                            }
                    else:
                        # Standard SuzieQ table query
                        tool_input = {"table": table, "method": method, **extra_filters}
                        try:
                            from olav.tools.suzieq_parquet_tool import (  # type: ignore
                                suzieq_query,
                                suzieq_schema_search,
                            )

                            schema = await suzieq_schema_search.ainvoke({"query": table})
                            available_tables = schema.get("tables", [])
                            if table in available_tables:
                                tool_result = await suzieq_query.ainvoke(tool_input)
                            else:
                                tool_result = {
                                    "status": "SCHEMA_NOT_FOUND",
                                    "table": table,
                                    "message": f"Table '{table}' not present in discovered schema tables.",
                                    "available_tables": available_tables,
                                }
                        except Exception as e:
                            tool_result = {
                                "status": "TOOL_ERROR",
                                "error": str(e),
                                "table": table,
                                "method": method,
                                "input": tool_input,
                            }

                if tool_result:
                    classified = self._classify_tool_result(tool_result)
                    # Failure statuses propagate directly
                    if classified["status"] in {
                        "SCHEMA_NOT_FOUND",
                        "NO_DATA_FOUND",
                        "DATA_NOT_RELEVANT",
                        "TOOL_ERROR",
                    }:
                        todo["status"] = "failed"
                        todo["result"] = (
                            f"‚ö†Ô∏è ÊâπÈáè‰ªªÂä°Â§±Ë¥•: {classified['status']} table={classified['table']}"
                        )
                        completed_results[todo["id"]] = todo["result"]
                        return todo, [AIMessage(content=todo["result"])]

                    # ‰ΩøÁî®Êô∫ËÉΩÂ≠óÊÆµÊèêÂèñÔºåÈÅøÂÖçÊà™Êñ≠ÂÖ≥ÈîÆËØäÊñ≠Êï∞ÊçÆ
                    data = tool_result.get("data", [])
                    tbl = classified.get("table", "unknown")
                    if isinstance(data, list) and data:
                        diagnostic_summary = self._extract_diagnostic_fields(
                            data, tbl, max_records=10
                        )
                    else:
                        diagnostic_summary = str(tool_result)[:400]

                    # Human-friendly task completion message
                    table_name_cn = self._get_table_display_name(tbl)
                    todo["status"] = "completed"
                    todo["result"] = (
                        f"{tr('query_complete', table=table_name_cn, count=classified['count'])}\n{diagnostic_summary}"
                    )
                    messages.append(
                        AIMessage(
                            content=tr(
                                "task_complete_msg",
                                task_id=todo["id"],
                                table=table_name_cn,
                                count=classified["count"],
                            )
                        )
                    )
                else:
                    # Fallback LLM path
                    prompt = prompt_manager.load_prompt(
                        category="workflows/deep_dive",
                        name="execute_todo",
                        task=task_text,
                        available_tools="suzieq_query, syslog_search, netconf_tool, search_openconfig_schema",
                    )
                    llm_resp = await self.llm.ainvoke(
                        [
                            SystemMessage(content=prompt),
                            HumanMessage(content=f"Execute task: {task_text}"),
                        ]
                    )
                    todo["status"] = "completed"
                    todo["result"] = llm_resp.content
                    messages.append(
                        AIMessage(content=f"Parallel task {todo['id']} completed via LLM fallback")
                    )

                completed_results[todo["id"]] = todo["result"]
                return todo, messages

            results = await asyncio.gather(
                *[_execute_single(t) for t in batch], return_exceptions=True
            )
            aggregated_messages: list[BaseMessage] = []
            for res in results:
                if isinstance(res, Exception):  # Defensive: unexpected batch error
                    aggregated_messages.append(AIMessage(content=f"ÊâπÈáèÊâßË°åÂá∫Áé∞Êú™ÊçïËé∑ÂºÇÂ∏∏: {res}"))
                else:
                    _todo, msgs = res
                    aggregated_messages.extend(msgs)

            # Decide next step message
            aggregated_messages.append(AIMessage(content=f"Âπ∂Ë°åÊâπÊ¨°ÂÆåÊàê: {len(batch)} ‰∏™‰ªªÂä°."))
            return {
                "todos": todos,
                "current_todo_id": batch[-1]["id"],
                "completed_results": completed_results,
                "messages": aggregated_messages,
                "user_approval": user_approval,  # Persist approval across iterations
            }

        # ------------------------------------------------------------------
        # Serial execution fallback (original logic) when 0 or 1 independent
        # ------------------------------------------------------------------
        next_todo: TodoItem | None = None
        for todo in todos:
            if todo["status"] == "pending":
                deps_ok = all(
                    any(t["id"] == dep_id and t["status"] in {"completed", "failed"} for t in todos)
                    for dep_id in todo["deps"]
                )
                if deps_ok or not todo["deps"]:
                    next_todo = todo
                    break

        if not next_todo:
            return {
                "messages": [AIMessage(content="All pending tasks processed.")],
                "user_approval": user_approval,
            }

        # Mark in-progress
        next_todo["status"] = "in-progress"
        task_text = next_todo["task"].strip()
        tool_result: dict | None = None
        mapping = self._map_task_to_table(task_text)
        tool_messages: list[BaseMessage] = []

        if mapping:
            table, method, extra_filters = mapping

            # Special handling for syslog - uses OpenSearch instead of SuzieQ
            if table == "syslog":
                try:
                    from olav.tools.syslog_tool import syslog_search

                    # Extract keyword from task for syslog search
                    syslog_keywords = self._extract_syslog_keywords(task_text)
                    device_ip = extra_filters.get("device_ip")

                    syslog_result = await syslog_search.ainvoke(
                        {
                            "keyword": syslog_keywords,
                            "device_ip": device_ip,
                            "start_time": "now-1h",  # Default: last hour
                            "limit": 50,
                        }
                    )

                    if syslog_result.get("success") and syslog_result.get("data"):
                        tool_result = {
                            "status": "SUCCESS",
                            "table": "syslog",
                            "data": syslog_result["data"],
                            "count": len(syslog_result["data"]),
                            "columns": ["timestamp", "device_ip", "severity", "message"],
                        }
                    else:
                        tool_result = {
                            "status": "NO_DATA_FOUND",
                            "table": "syslog",
                            "message": syslog_result.get("error") or "No syslog entries found",
                            "hint": f"Â∞ùËØïÁöÑÂÖ≥ÈîÆËØç: {syslog_keywords}",
                        }
                except Exception as e:
                    tool_result = {
                        "status": "TOOL_ERROR",
                        "table": "syslog",
                        "error": str(e),
                    }
            else:
                # Standard SuzieQ table query
                tool_input = {"table": table, "method": method, **extra_filters}
                try:
                    # Local import to avoid global dependency issues
                    from olav.tools.suzieq_parquet_tool import (  # type: ignore
                        suzieq_query,
                        suzieq_schema_search,
                    )

                    # Discover available tables; suzieq_schema_search returns {"tables": [...], "bgp": {...}, ...}
                    schema = await suzieq_schema_search.ainvoke({"query": table})
                    available_tables = schema.get("tables", [])

                    if table in available_tables:
                        tool_result = await suzieq_query.ainvoke(tool_input)

                        # ÊñπÊ°à2: Â≠óÊÆµËØ≠‰πâÈ™åËØÅ - Ê£ÄÊü•ËøîÂõûÂ≠óÊÆµÊòØÂê¶‰∏é‰ªªÂä°Áõ∏ÂÖ≥
                        if (
                            tool_result
                            and "columns" in tool_result
                            and tool_result.get("status") != "NO_DATA_FOUND"
                        ):
                            is_relevant = self._validate_field_relevance(
                                task_text=task_text,
                                returned_columns=tool_result["columns"],
                                queried_table=table,
                            )
                            if not is_relevant:
                                # Data returned but not relevant to task
                                tool_result = {
                                    "status": "DATA_NOT_RELEVANT",
                                    "table": table,
                                    "returned_columns": tool_result["columns"],
                                    "message": f"Ë°® '{table}' ËøîÂõû‰∫ÜÊï∞ÊçÆÔºå‰ΩÜÂ≠óÊÆµ‰∏é‰ªªÂä°ÈúÄÊ±Ç‰∏çÂåπÈÖç„ÄÇ",
                                    "hint": f"‰ªªÂä°ÂÖ≥ÈîÆËØç: {self._extract_task_keywords(task_text)}ÔºåËøîÂõûÂ≠óÊÆµ: {tool_result['columns'][:5]}",
                                    "suggestion": "ÂèØËÉΩÈúÄË¶Å‰ΩøÁî® NETCONF Êü•ËØ¢ÊàñÈáçÊñ∞ËßÑÂàí‰ªªÂä°„ÄÇ",
                                }
                    else:
                        tool_result = {
                            "status": "SCHEMA_NOT_FOUND",
                            "table": table,
                            "message": f"Table '{table}' not present in discovered schema tables.",
                            "hint": "Use suzieq_schema_search with a broader query or verify poller collection.",
                            "available_tables": available_tables,
                        }
                except Exception as e:
                    tool_result = {
                        "status": "TOOL_ERROR",
                        "error": str(e),
                        "table": table,
                        "method": method,
                        "input": tool_input,
                    }

        if tool_result:
            classified = self._classify_tool_result(tool_result)
            (
                f"TOOL_CALL table={classified['table']} status={classified['status']} "
                f"count={classified['count']}"
            )

            # CRITICAL: Èò≤Ê≠¢ LLM ÂπªËßâ - Âú®ÈÅáÂà∞ÈîôËØØÁä∂ÊÄÅÊó∂Áõ¥Êé•ËøîÂõûÂ§±Ë¥•Ôºå‰∏çÁªßÁª≠Â§ÑÁêÜ
            if classified["status"] in {
                "SCHEMA_NOT_FOUND",
                "NO_DATA_FOUND",
                "DATA_NOT_RELEVANT",
                "TOOL_ERROR",
            }:
                error_msg = (
                    f"‚ö†Ô∏è ‰ªªÂä°ÊâßË°åÂ§±Ë¥•: {classified['status']}\n"
                    f"Ë°®: {classified['table']}\n"
                    f"ÂéüÂõ†: {tool_result.get('message') or tool_result.get('error', 'Êú™Áü•ÈîôËØØ')}\n"
                    f"ÊèêÁ§∫: {tool_result.get('hint', 'N/A')}\n"
                )

                # DATA_NOT_RELEVANT ÈúÄË¶ÅÈ¢ùÂ§ñËØ¥Êòé
                if classified["status"] == "DATA_NOT_RELEVANT":
                    error_msg += (
                        f"\n‚ö†Ô∏è **Êï∞ÊçÆËØ≠‰πâ‰∏çÂåπÈÖç**: Êü•ËØ¢ÁöÑË°®ËøîÂõû‰∫ÜÊï∞ÊçÆÔºå‰ΩÜÂ≠óÊÆµ‰∏é‰ªªÂä°ÈúÄÊ±Ç‰∏çÁõ∏ÂÖ≥„ÄÇ\n"
                        f"Âª∫ËÆÆ: {tool_result.get('suggestion', 'ÈáçÊñ∞ËßÑÂàí‰ªªÂä°Êàñ‰ΩøÁî® NETCONF Áõ¥Êé•Êü•ËØ¢')}\n"
                    )

                error_msg += (
                    "\n‚õî **‰∏•Ê†ºÁ¶ÅÊ≠¢ÁºñÈÄ†Êï∞ÊçÆ** - Êó†Áõ∏ÂÖ≥Êï∞ÊçÆÂç≥Êä•ÂëäÂ§±Ë¥•Ôºå‰∏çÊé®ÊµãÊàñÁîüÊàêËôöÂÅáÁªìÊûú„ÄÇ"
                )

                next_todo["status"] = "failed"
                next_todo["result"] = error_msg
                completed_results[next_todo["id"]] = error_msg

                return {
                    "todos": todos,
                    "current_todo_id": next_todo["id"],
                    "completed_results": completed_results,
                    "messages": [AIMessage(content=error_msg)],
                    "user_approval": user_approval,
                }

            # ÊàêÂäüÁä∂ÊÄÅÔºö‰ΩøÁî®Êô∫ËÉΩÂ≠óÊÆµÊèêÂèñÔºåÈÅøÂÖçÊà™Êñ≠ÂÖ≥ÈîÆËØäÊñ≠Êï∞ÊçÆÔºàÂ¶Ç state, reason Á≠âÔºâ
            data = tool_result.get("data", [])
            table = classified.get("table", "unknown")
            table_name_cn = self._get_table_display_name(table)
            if isinstance(data, list) and data:
                diagnostic_summary = self._extract_diagnostic_fields(data, table)
            else:
                diagnostic_summary = str(tool_result)[:800]  # Fallback for non-list data
            result_text = f"{tr('query_complete', table=table_name_cn, count=classified['count'])}\n\n{diagnostic_summary}"
            tool_messages.append(
                AIMessage(
                    content=tr(
                        "task_complete_simple", table=table_name_cn, count=classified["count"]
                    )
                )
            )
        else:
            # Fallback to LLM execution strategy
            prompt = prompt_manager.load_prompt(
                category="workflows/deep_dive",
                name="execute_todo",
                task=task_text,
                available_tools="suzieq_query, syslog_search, netconf_tool, search_openconfig_schema",
            )
            messages = [
                SystemMessage(content=prompt),
                HumanMessage(content=f"Execute task: {task_text}"),
            ]
            llm_resp = await self.llm.ainvoke(messages)
            result_text = llm_resp.content

        # Complete todo (only if not already marked failed above)
        if next_todo["status"] != "failed":
            next_todo["status"] = "completed"
        next_todo["result"] = result_text

        # ------------------------------------------------------------------
        # Phase 2: External Evaluator integration (Schema-Aware dynamic)
        # ------------------------------------------------------------------
        try:
            if next_todo["status"] == "completed" and tool_result:
                from olav.evaluators.config_compliance import ConfigComplianceEvaluator

                evaluator = ConfigComplianceEvaluator()
                eval_result = await evaluator.evaluate(next_todo, tool_result)

                next_todo["evaluation_passed"] = eval_result.passed
                next_todo["evaluation_score"] = eval_result.score

                if not eval_result.passed:
                    next_todo["failure_reason"] = eval_result.feedback
                    # Reclassify status to failed and append evaluator feedback
                    next_todo["status"] = "failed"
                    appended = f"\nüîç ËØÑ‰º∞Êú™ÈÄöËøá: {eval_result.feedback}"
                    next_todo["result"] = (next_todo["result"] or "") + appended
        except Exception as eval_err:
            # Non-fatal ‚Äì store failure_reason for visibility
            next_todo["evaluation_passed"] = False
            next_todo["evaluation_score"] = 0.0
            next_todo["failure_reason"] = f"Evaluator error: {eval_err}"

        completed_results[next_todo["id"]] = next_todo["result"]

        completion = AIMessage(content=f"Completed task {next_todo['id']}: {result_text[:600]}")
        return {
            "todos": todos,
            "current_todo_id": next_todo["id"],
            "completed_results": completed_results,
            "messages": [*tool_messages, completion],
            "user_approval": user_approval,
        }

    def _map_task_to_table(self, task: str) -> tuple[str, str, dict] | None:
        """Map natural language task to (table, method, filters) using ordered specificity.

        Order matters: more specific/general inventory tasks first, then protocol.
        Returns None if no mapping found (will trigger schema investigation).

        Method selection:
        - 'get': For detailed data queries (default for troubleshooting)
        - 'summarize': Only for explicit aggregation requests (ÁªüËÆ°, Ê±áÊÄª, Ê¶ÇËßà)
        """
        lower = task.lower()

        # Determine method based on task intent
        # Use 'summarize' only for explicit aggregation requests
        needs_summary = any(
            k in lower for k in ["ÁªüËÆ°", "Ê±áÊÄª", "Ê¶ÇËßà", "ÊÄªÊï∞", "count", "summary", "overview"]
        )
        method = "summarize" if needs_summary else "get"

        candidates: list[tuple[list[str], str]] = [
            # Inventory / device list
            (["ËÆæÂ§áÂàóË°®", "ÊâÄÊúâËÆæÂ§á", "ÂÆ°ËÆ°ËÆæÂ§á", "device", "ËÆæÂ§á"], "device"),
            # Interfaces
            (["Êé•Âè£", "Á´ØÂè£", "interface", "Áâ©ÁêÜ", "rx", "tx", "ÈìæË∑Ø"], "interfaces"),
            # Routing / prefixes
            (["Ë∑ØÁî±", "ÂâçÁºÄ", "routes", "lpm"], "routes"),
            # OSPF
            (["ospf"], "ospfIf"),
            # LLDP
            (["lldp"], "lldp"),
            # MAC
            (["mac", "‰∫åÂ±Ç"], "macs"),
            # BGP (put later to avoid greedy matching of 'ËæπÁïå')
            (["bgp", "peer", "ÈÇªÂ±Ö", "ËæπÁïå", "ebgp", "ibgp"], "bgp"),
            # Syslog (event-driven diagnostics - maps to OpenSearch, not SuzieQ)
            (["syslog", "Êó•Âøó", "log", "‰∫ã‰ª∂", "ÂëäË≠¶", "down", "error", "warning"], "syslog"),
        ]
        for keywords, table in candidates:
            if any(k in lower for k in keywords):
                import re

                hosts = re.findall(r"\b([A-Za-z]{1,4}\d{1,2})\b", task)
                filters: dict[str, Any] = {}
                if hosts:
                    filters["hostname"] = hosts[0]
                return table, method, filters
        return None

    def _extract_syslog_keywords(self, task: str) -> str:
        """Extract relevant keywords from task for syslog search.

        Maps common network issues to syslog keywords.
        Returns pipe-separated keywords for OR search.

        Args:
            task: Task description in natural language

        Returns:
            Pipe-separated keywords for syslog_search (e.g., "BGP|DOWN|NEIGHBOR")
        """
        lower = task.lower()
        keywords = []

        # Protocol-specific keywords
        keyword_mappings = {
            "bgp": ["BGP", "ADJCHANGE", "NEIGHBOR", "NOTIFICATION"],
            "ospf": ["OSPF", "ADJACENCY", "NBRSTATE"],
            "Êé•Âè£": ["LINK", "INTERFACE", "UPDOWN", "CARRIER"],
            "interface": ["LINK", "INTERFACE", "UPDOWN", "CARRIER"],
            "ÈìæË∑Ø": ["LINK", "UPDOWN", "DOWN"],
            "ÈÖçÁΩÆ": ["CONFIG", "COMMIT", "CONFIGURATION"],
            "config": ["CONFIG", "COMMIT", "CONFIGURATION"],
            "cpu": ["CPU", "MEMORY", "UTILIZATION"],
            "ÂÜÖÂ≠ò": ["MEMORY", "CPU"],
            "Ê∏©Â∫¶": ["TEMPERATURE", "SENSOR", "FAN"],
            "ËÆ§ËØÅ": ["AUTH", "LOGIN", "DENIED", "FAILED"],
            "auth": ["AUTH", "LOGIN", "DENIED", "FAILED"],
        }

        for trigger, kws in keyword_mappings.items():
            if trigger in lower:
                keywords.extend(kws)

        # Common error keywords
        error_triggers = ["ÊïÖÈöú", "ÈóÆÈ¢ò", "Â§±Ë¥•", "ÂºÇÂ∏∏", "ÈîôËØØ", "error", "fail", "down"]
        if any(t in lower for t in error_triggers):
            keywords.extend(["DOWN", "ERROR", "FAIL", "CRITICAL"])

        # Dedupe and join
        unique_keywords = list(dict.fromkeys(keywords))

        # Default to common events if no specific match
        if not unique_keywords:
            unique_keywords = ["DOWN", "ERROR", "WARNING", "CRITICAL"]

        return "|".join(unique_keywords[:8])  # Limit to 8 keywords

    def _extract_diagnostic_fields(
        self, data: list[dict[str, Any]], table: str, max_records: int = 20
    ) -> str:
        """Extract key diagnostic fields from query results to prevent truncation of critical data.

        Instead of blindly truncating the full data dict (which loses important fields like 'state'),
        this method extracts only the most important fields for each table type.

        Args:
            data: List of records from suzieq_query
            table: Table name to determine which fields to extract
            max_records: Maximum number of records to include

        Returns:
            Formatted string with key diagnostic information
        """
        if not data:
            return "Êó†Êï∞ÊçÆËÆ∞ÂΩï"

        # Define key fields per table type (most important for diagnostics first)
        table_key_fields: dict[str, list[str]] = {
            "bgp": [
                "hostname",
                "peer",
                "state",
                "asn",
                "peerAsn",
                "afi",
                "safi",
                "reason",
                "notificnReason",
                "estdTime",
                "pfxRx",
                "pfxTx",
                "vrf",
            ],
            "ospfIf": ["hostname", "ifname", "state", "area", "networkType", "cost", "passive"],
            "ospfNbr": ["hostname", "ifname", "nbrHostname", "state", "area", "nbrPriority"],
            "interfaces": [
                "hostname",
                "ifname",
                "state",
                "adminState",
                "speed",
                "mtu",
                "ipAddressList",
            ],
            "routes": [
                "hostname",
                "vrf",
                "prefix",
                "nexthopIp",
                "protocol",
                "preference",
                "metric",
            ],
            "device": ["hostname", "model", "version", "vendor", "uptime", "serialNumber"],
            "lldp": ["hostname", "ifname", "peerHostname", "peerIfname", "capability"],
            "macs": ["hostname", "vlan", "macaddr", "interface", "moveCount"],
        }

        # Human-readable field labels
        field_labels = {
            "hostname": "‰∏ªÊú∫",
            "peer": "ÈÇªÂ±Ö",
            "state": "Áä∂ÊÄÅ",
            "asn": "Êú¨Âú∞AS",
            "peerAsn": "ÈÇªÂ±ÖAS",
            "afi": "Âú∞ÂùÄÊóè",
            "safi": "Â≠êÊóè",
            "reason": "ÂéüÂõ†",
            "notificnReason": "ÈÄöÁü•ÂéüÂõ†",
            "estdTime": "Âª∫Á´ãÊó∂Èó¥",
            "pfxRx": "Êî∂Âà∞ÂâçÁºÄ",
            "pfxTx": "ÂèëÈÄÅÂâçÁºÄ",
            "vrf": "VRF",
            "ifname": "Êé•Âè£",
            "adminState": "ÁÆ°ÁêÜÁä∂ÊÄÅ",
            "speed": "ÈÄüÁéá",
            "mtu": "MTU",
            "ipAddressList": "IP",
            "prefix": "ÂâçÁºÄ",
            "nexthopIp": "‰∏ã‰∏ÄË∑≥",
            "protocol": "ÂçèËÆÆ",
            "preference": "‰ºòÂÖàÁ∫ß",
            "metric": "Â∫¶ÈáèÂÄº",
            "model": "ÂûãÂè∑",
            "version": "ÁâàÊú¨",
            "vendor": "ÂéÇÂïÜ",
            "uptime": "ËøêË°åÊó∂Èó¥",
            "area": "Âå∫Âüü",
            "cost": "ÂºÄÈîÄ",
            "peerHostname": "ÈÇªÂ±Ö‰∏ªÊú∫",
            "peerIfname": "ÈÇªÂ±ÖÊé•Âè£",
        }

        # Get fields for this table, or use common fallback fields
        fields = table_key_fields.get(table.lower(), ["hostname", "state", "status"])

        # Build formatted output with better readability
        lines = []
        for i, record in enumerate(data[:max_records]):
            if not isinstance(record, dict):
                continue

            # Extract available key fields from this record
            field_values = []
            for field in fields:
                if field not in record:
                    continue
                value = record[field]
                # Skip empty/null values (handle numpy arrays specially)
                try:
                    import numpy as np

                    if isinstance(value, np.ndarray):
                        if value.size == 0:
                            continue
                        value = value.tolist()  # Convert to list for display
                    elif value in (None, "", [], {}):
                        continue
                except (ImportError, ValueError, TypeError):
                    if value in (None, "", [], {}):
                        continue

                # Format timestamp as readable date
                if field == "estdTime" and isinstance(value, (int, float)):
                    if value > 1e12:
                        from datetime import datetime

                        with contextlib.suppress(Exception):
                            value = datetime.fromtimestamp(value / 1000).strftime("%m-%d %H:%M")
                    elif value == 0:
                        value = tr("timestamp_not_established")

                # Format state values with i18n
                if field == "state":
                    state_map = {
                        "Established": tr("state_established"),
                        "NotEstd": tr("state_not_established"),
                        "up": tr("state_up"),
                        "down": tr("state_down"),
                    }
                    value = state_map.get(str(value), value)

                # Use translated label
                label = tr(f"field_{field}")
                if label == f"field_{field}":
                    # No translation found, use field_labels fallback or raw field name
                    label = field_labels.get(field, field)
                field_values.append(f"{label}: {value}")

            if field_values:
                # Format as bullet point with hostname highlighted
                hostname = record.get("hostname", f"{tr('record_placeholder')}{i + 1}")
                host_label = tr("field_hostname")
                other_fields = [f for f in field_values if not f.startswith(f"{host_label}:")]
                lines.append(f"  ‚Ä¢ **{hostname}** ‚Üí " + " | ".join(other_fields))

        if not lines:
            return tr("no_diagnostic_fields")

        # Add header with record count
        showing = min(len(data), max_records)
        if len(data) > max_records:
            header = tr("records_header_truncated", total=len(data), showing=showing)
        else:
            header = tr("records_header", count=len(data))
        return header + "\n" + "\n".join(lines)

    def _classify_tool_result(self, result: dict[str, Any]) -> dict[str, Any]:
        """Normalize tool result into status/count/table for summary lines."""
        status = "OK"
        table = result.get("table", "unknown")
        count = result.get("count")
        if count is None and isinstance(result.get("data"), list):
            count = len(result.get("data", []))

        # Priority 1: Explicit DATA_NOT_RELEVANT status (field validation failed)
        if result.get("status") == "DATA_NOT_RELEVANT":
            status = "DATA_NOT_RELEVANT"
        # Priority 2: Explicit error field (tool execution failed)
        elif "error" in result:
            error_msg = str(result["error"])
            # Check if error indicates unknown table (schema validation)
            if "Unknown table" in error_msg or "available_tables" in result:
                status = "SCHEMA_NOT_FOUND"
            else:
                status = "TOOL_ERROR"
        # Priority 3: Explicit schema not found status (from our validation)
        elif result.get("status") == "SCHEMA_NOT_FOUND":
            status = "SCHEMA_NOT_FOUND"
        # Priority 4: NO_DATA_FOUND sentinel in first data record
        elif isinstance(result.get("data"), list) and result["data"]:
            first = result["data"][0]
            if isinstance(first, dict) and first.get("status") == "NO_DATA_FOUND":
                status = "NO_DATA_FOUND"
        # Priority 5: Empty data list
        elif isinstance(result.get("data"), list) and len(result.get("data", [])) == 0:
            status = "NO_DATA_FOUND"

        return {"status": status, "table": table, "count": count if count is not None else 0}

    def _validate_field_relevance(
        self, task_text: str, returned_columns: list[str], queried_table: str
    ) -> bool:
        """Validate if returned columns are semantically relevant to task (ÊñπÊ°à2).

        Args:
            task_text: Original task description
            returned_columns: Field names returned from query
            queried_table: Table that was queried

        Returns:
            True if fields appear relevant, False otherwise
        """
        # Strategy: Be lenient - if the table matches the task intent, accept the data
        # The real validation should be in the final summary, not here

        # 1. If table name matches any task keyword, data is relevant
        task_keywords = self._extract_task_keywords(task_text)
        if queried_table.lower() in task_keywords:
            return True

        # 2. Mapping of tables to their core field groups
        table_core_fields = {
            "bgp": ["peer", "neighbor", "state", "afi", "safi", "asn", "pfx"],
            "ospf": ["area", "neighbor", "state", "cost", "dr"],
            "interfaces": ["ifname", "state", "admin", "speed", "mtu", "ip"],
            "routes": ["prefix", "nexthop", "protocol", "metric", "vrf"],
            "device": ["hostname", "model", "version", "vendor"],
            "lldp": ["neighbor", "port", "chassis"],
            "macs": ["mac", "vlan", "port", "interface"],
        }

        # 3. Check if returned columns contain any core fields for the queried table
        core_fields = table_core_fields.get(queried_table.lower(), [])
        columns_str = " ".join(returned_columns).lower()

        if any(field in columns_str for field in core_fields):
            return True

        # 4. Special case: device/interfaces are generic inventory, acceptable for most tasks
        if queried_table in {"device", "interfaces"}:
            return True

        # 5. If returned columns contain common network fields, accept
        common_network_fields = ["hostname", "namespace", "timestamp", "state", "status"]
        if any(field in columns_str for field in common_network_fields):
            return True

        # No semantic match - but only reject if we have very specific mismatches
        # e.g., asking for "mpls" but getting "mac" table
        return len(task_keywords) == 0  # If no keywords extracted, accept anything

    def _extract_task_keywords(self, task_text: str) -> list[str]:
        """Extract technical keywords from task description."""
        lower = task_text.lower()
        # Common network protocol/feature keywords
        keywords = [
            "mpls",
            "ldp",
            "rsvp",
            "bgp",
            "ospf",
            "eigrp",
            "isis",
            "vlan",
            "vxlan",
            "evpn",
            "interface",
            "route",
            "prefix",
            "neighbor",
            "peer",
            "session",
            "tunnel",
            "policy",
            "qos",
            "acl",
            "nat",
            "firewall",
            "vpn",
        ]
        return [kw for kw in keywords if kw in lower]

    async def should_continue(
        self, state: DeepDiveState
    ) -> Literal["execute_todo", "recursive_check"]:
        """Decide whether to continue executing todos or move to recursive check.

        Args:
            state: Current workflow state

        Returns:
            Next node to execute
        """
        todos = state["todos"]
        pending_count = sum(1 for t in todos if t["status"] == "pending")

        if pending_count > 0:
            return "execute_todo"
        return "recursive_check"

    async def recursive_check_node(self, state: DeepDiveState) -> dict:
        """Check if recursive deep dive is needed.

        Phase 3.4 Enhancement: Handles multiple failures in parallel, not just the first one.
        Creates focused sub-tasks for each failed todo (up to max_failures_per_recursion).

        Args:
            state: Current workflow state

        Returns:
            Updated state with potential new sub-todos for all failures
        """
        recursion_depth = state.get("recursion_depth", 0)
        max_depth = state.get("max_depth", 3)
        max_failures_per_recursion = (
            3  # Limit parallel failure investigation to avoid prompt explosion
        )

        # Depth guard
        if recursion_depth >= max_depth:
            return {
                "messages": [
                    AIMessage(
                        content=f"Max recursion depth ({max_depth}) reached. Moving to summary."
                    )
                ],
                "trigger_recursion": False,
            }

        todos = state.get("todos", [])
        failed_todos = [t for t in todos if t.get("status") == "failed"]

        if not failed_todos:
            return {
                "messages": [AIMessage(content="No deeper analysis needed.")],
                "trigger_recursion": False,
            }

        # PHASE 3.4: Handle multiple failures (not just first one)
        # Limit to top N failures to avoid overwhelming prompt/planning
        failures_to_analyze = failed_todos[:max_failures_per_recursion]

        # Build recursive prompt for ALL selected failures
        failure_summaries = []
        for failed in failures_to_analyze:
            parent_task_id = failed["id"]
            parent_task_text = failed["task"]
            parent_result = (failed.get("result") or "")[
                :400
            ]  # Truncate per failure to fit multiple
            parent_reason = failed.get("failure_reason", "Unknown")

            failure_summaries.append(
                f"  ‚Ä¢ Â§±Ë¥•‰ªªÂä° {parent_task_id}: {parent_task_text}\n"
                f"    Â§±Ë¥•ÂéüÂõ†: {parent_reason}\n"
                f"    ËæìÂá∫ÊëòË¶Å: {parent_result}\n"
            )

        recursive_prompt = (
            f"ÈÄíÂΩíÊ∑±ÂÖ•ÂàÜÊûê: Ê£ÄÊµãÂà∞ {len(failures_to_analyze)} ‰∏™Â§±Ë¥•‰ªªÂä°ÔºåÈúÄË¶ÅÁîüÊàêÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑÂ≠ê‰ªªÂä°„ÄÇ\n\n"
            "Â§±Ë¥•‰ªªÂä°ÂàóË°®:\n" + "\n".join(failure_summaries) + "\n\n"
            "ËØ∑ÈÅµÂæ™Ë¶ÅÊ±Ç: \n"
            f"1) ‰∏∫ÊØè‰∏™Â§±Ë¥•‰ªªÂä°ÁîüÊàê 1-2 ‰∏™Êõ¥ÂÖ∑‰ΩìÁöÑÂ≠ê‰ªªÂä°ÔºàÊÄªÂÖ± {len(failures_to_analyze) * 2} ‰∏™Â∑¶Âè≥Ôºâ„ÄÇ\n"
            "2) Â≠ê‰ªªÂä°ÈúÄÊõ¥ÂÖ∑‰ΩìÔºå‰æãÂ¶ÇËÅöÁÑ¶ÊüêÂçèËÆÆÂÆû‰æã„ÄÅÈÇªÂ±Ö„ÄÅÊé•Âè£ÊàñÂ≠óÊÆµ„ÄÇ\n"
            "3) ÈÅøÂÖç‰∏éÁà∂‰ªªÂä°ÂÆåÂÖ®ÈáçÂ§ç„ÄÇ\n"
            '4) ‰ΩøÁî® JSON ËæìÂá∫: {\n  "todos": [ {"id": <int>, "task": <str>, "deps": [] } ]\n}„ÄÇ\n'
            "5) ID ‰ªéÁé∞ÊúâÊúÄÂ§ß ID + 1 ÂºÄÂßãÈÄíÂ¢û„ÄÇ\n"
            "6) Âú® task ÊñáÊú¨‰∏≠ÂåÖÂê´Áà∂‰ªªÂä°ÂºïÁî®: '(parent:<id>)'Ôºå‰æãÂ¶Ç 'Ê£ÄÊü• R1 BGP ÈÖçÁΩÆ (parent:3)'„ÄÇ\n"
            "7) Â¶ÇÊûúÊüêÂ§±Ë¥•‰ªªÂä°Êó†Ê≥ïËøõ‰∏ÄÊ≠•ÁªÜÂåñÔºåÁîüÊàê‰∏Ä‰∏™È™åËØÅÊÄß‰ªªÂä°Ôºå‰æãÂ¶Ç 'È™åËØÅÈááÈõÜÊòØÂê¶Áº∫Â§± (parent:<id>)'„ÄÇ\n"
        )

        return {
            "messages": [HumanMessage(content=recursive_prompt)],
            "recursion_depth": recursion_depth + 1,
            "trigger_recursion": True,
        }

    async def should_recurse(
        self, state: DeepDiveState
    ) -> Literal["final_summary", "task_planning"]:
        """Decide whether to recurse or finalize.

        Args:
            state: Current workflow state

        Returns:
            Next node to execute
        """
        if state.get("trigger_recursion"):
            return "task_planning"
        return "final_summary"

    async def final_summary_node(self, state: DeepDiveState) -> dict:
        """Generate final summary report from all completed todos.

        Args:
            state: Current workflow state

        Returns:
            Updated state with final summary message
        """
        import logging

        logger = logging.getLogger(__name__)

        todos = state["todos"]
        completed_results = state.get("completed_results", {})
        messages = state.get("messages", [])

        # Extract original user query from first HumanMessage
        user_query = ""
        for msg in messages:
            if isinstance(msg, HumanMessage):
                user_query = msg.content
                break

        # Load summary prompt
        prompt = prompt_manager.load_prompt(
            category="workflows/deep_dive",
            name="final_summary",
            todos=str(todos),
            results=str(completed_results),
        )

        llm_messages = [SystemMessage(content=prompt)]
        response = await self.llm.ainvoke(llm_messages)
        final_report = response.content

        # Count successful/failed tasks for evaluation
        successful_tasks = sum(1 for t in todos if t.get("status") == "completed")
        failed_tasks = sum(1 for t in todos if t.get("status") == "failed")
        total_tasks = len(todos)

        # Save troubleshooting report to episodic memory (Agentic RAG)
        # Only save if Deep Dive memory is enabled in settings
        if settings.enable_deep_dive_memory and successful_tasks > 0 and user_query:
            try:
                memory_writer = get_memory_writer()
                await memory_writer.memory.store_episodic_memory(
                    intent=user_query,
                    xpath=f"deep_dive:{successful_tasks}/{total_tasks} tasks",
                    success=failed_tasks == 0,  # Fully successful if no failures
                    context={
                        "tool_used": "deep_dive_workflow",
                        "device_type": "multi-device",
                        "strategy_used": "deep_dive",
                        "execution_time_ms": 0,  # Not tracked at workflow level
                        "parameters": {
                            "todos_count": total_tasks,
                            "successful": successful_tasks,
                            "failed": failed_tasks,
                        },
                        "result_summary": final_report[:500],  # Truncate for storage
                        "full_report_available": len(final_report) > 500,
                    },
                )
                logger.info(f"‚úì Saved Deep Dive report to episodic memory: {user_query[:50]}...")
            except Exception as e:
                # Don't fail workflow on memory save error
                logger.warning(f"Failed to save Deep Dive report to memory: {e}")
        elif not settings.enable_deep_dive_memory:
            logger.debug("Deep Dive memory disabled, skipping report save")

        return {
            "messages": [AIMessage(content=final_report)],
        }

    def build_graph(self, checkpointer: AsyncPostgresSaver) -> StateGraph:
        """Build Deep Dive Workflow graph with Funnel Debugging methodology.

        NEW Flow (Funnel Debugging):
        1. topology_analysis ‚Üí Identify affected devices and scope
        2. funnel_planning ‚Üí Generate OSI layer-based diagnosis plan
        3. [INTERRUPT] ‚Üí Wait for user approval
        4. macro_scan ‚Üí Execute SuzieQ checks per layer (loop)
        5. evaluate_findings ‚Üí Decide if more scanning needed
        6. realtime_verification ‚Üí Verify findings with live CLI data
        7. root_cause_summary ‚Üí Generate final report

        CRITICAL: SuzieQ data is historical (Parquet).
        Real-time verification uses NETCONF/CLI for live state.

        Args:
            checkpointer: PostgreSQL checkpointer for state persistence

        Returns:
            Compiled StateGraph with HITL interrupts
        """
        workflow = StateGraph(DeepDiveState)

        # Add Funnel Debugging nodes
        workflow.add_node("topology_analysis", self.topology_analysis_node)
        workflow.add_node("funnel_planning", self.funnel_planning_node)
        workflow.add_node("macro_scan", self.macro_scan_node)
        workflow.add_node("evaluate_findings", self.evaluate_findings_node)
        workflow.add_node("realtime_verification", self.realtime_verification_node)
        workflow.add_node("root_cause_summary", self.root_cause_summary_node)

        # Define edges for Funnel Debugging flow
        workflow.set_entry_point("topology_analysis")
        workflow.add_edge("topology_analysis", "funnel_planning")
        workflow.add_edge("funnel_planning", "macro_scan")
        workflow.add_edge("macro_scan", "evaluate_findings")

        workflow.add_conditional_edges(
            "evaluate_findings",
            self.should_continue_funnel,
            {
                "macro_scan": "macro_scan",  # Continue to next phase
                "realtime_verification": "realtime_verification",  # All phases done, verify
            },
        )

        # After verification, generate summary
        workflow.add_edge("realtime_verification", "root_cause_summary")
        workflow.add_edge("root_cause_summary", END)

        # Compile with checkpointer
        # HITL is handled by interrupt() in macro_scan_node
        return workflow.compile(
            checkpointer=checkpointer,
        )

    def build_legacy_graph(self, checkpointer: AsyncPostgresSaver) -> StateGraph:
        """Build legacy Deep Dive graph (task planning style).

        Use this for backward compatibility with existing audit workflows.

        Flow:
        1. task_planning ‚Üí Generate todos
        2. schema_investigation ‚Üí Validate feasibility
        3. execute_todo ‚Üí Execute tasks
        4. recursive_check ‚Üí Deeper analysis if needed
        5. final_summary ‚Üí Generate report
        """
        workflow = StateGraph(DeepDiveState)

        # Add nodes
        workflow.add_node("task_planning", self.task_planning_node)
        workflow.add_node("schema_investigation", self.schema_investigation_node)
        workflow.add_node("execute_todo", self.execute_todo_node)
        workflow.add_node("recursive_check", self.recursive_check_node)
        workflow.add_node("final_summary", self.final_summary_node)

        # Define edges
        workflow.set_entry_point("task_planning")
        workflow.add_edge("task_planning", "schema_investigation")
        workflow.add_edge("schema_investigation", "execute_todo")

        workflow.add_conditional_edges(
            "execute_todo",
            self.should_continue,
            {
                "execute_todo": "execute_todo",
                "recursive_check": "recursive_check",
            },
        )
        workflow.add_conditional_edges(
            "recursive_check",
            self.should_recurse,
            {
                "task_planning": "task_planning",
                "final_summary": "final_summary",
            },
        )
        workflow.add_edge("final_summary", END)

        return workflow.compile(
            checkpointer=checkpointer,
        )
