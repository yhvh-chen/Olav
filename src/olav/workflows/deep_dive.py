"""Deep Dive Workflow - Complex Multi-Step Task Execution.

This workflow handles complex diagnostic and audit tasks that require:
1. Automatic task decomposition (LLM generates Todo List)
2. Recursive diagnostics (max 3 levels deep)
3. Batch parallel execution (30+ devices)
4. Progress tracking with Checkpointer (resume on interruption)

Trigger scenarios:
- Batch audits: "å®¡è®¡æ‰€æœ‰è¾¹ç•Œè·¯ç”±å™¨çš„ BGP å®‰å…¨é…ç½®"
- Cross-domain troubleshooting: "ä¸ºä»€ä¹ˆæ•°æ®ä¸­å¿ƒ A æ— æ³•è®¿é—®æ•°æ®ä¸­å¿ƒ Bï¼Ÿ"
- Recursive diagnostics: "æ·±å…¥åˆ†æ OSPF é‚»å±…å…³ç³»å¼‚å¸¸"

Usage:
    uv run olav.py -e "å®¡è®¡æ‰€æœ‰è¾¹ç•Œè·¯ç”±å™¨"
    uv run olav.py --expert "è·¨åŸŸæ•…éšœæ·±åº¦åˆ†æ"
"""

import asyncio
import sys

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

from operator import add
from typing import Annotated, Any, Literal, TypedDict

from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from langgraph.graph import END, StateGraph

from olav.core.llm import LLMFactory
from olav.core.prompt_manager import prompt_manager
from olav.workflows.base import BaseWorkflow
from olav.workflows.registry import WorkflowRegistry

# Tools will be called via ToolNode, not directly imported


class TodoItem(TypedDict):
    """Individual task item in the Todo List.

    Extended (Phase 2) with evaluator related fields.
    These optional fields support objective post-execution validation.
    """

    id: int
    task: str
    status: Literal["pending", "in-progress", "completed", "failed"]
    result: str | None
    deps: list[int]  # IDs of prerequisite todos
    # Schema investigation results
    feasibility: Literal["feasible", "uncertain", "infeasible"] | None
    recommended_table: str | None
    schema_notes: str | None
    # External evaluator (Phase 2) - no hardcoded fields needed
    evaluation_passed: bool | None
    evaluation_score: float | None
    failure_reason: str | None


class ExecutionPlan(TypedDict):
    """Execution plan generated from schema investigation."""

    feasible_tasks: list[int]  # Todo IDs that can be executed
    uncertain_tasks: list[int]  # Need user clarification
    infeasible_tasks: list[int]  # Cannot be executed (no schema support)
    recommendations: dict[int, str]  # Todo ID -> recommended approach
    user_approval_required: bool


class DeepDiveState(TypedDict):
    """State for Deep Dive Workflow.

    Fields:
        messages: Conversation history
        todos: List of tasks to execute
        execution_plan: Schema investigation results and execution plan
        current_todo_id: ID of task being executed
        completed_results: Mapping of todo_id -> execution result
        recursion_depth: Current recursion level (0-based)
        max_depth: Maximum allowed recursion depth (default: 3)
        expert_mode: Whether expert mode is enabled
    """

    messages: Annotated[list[BaseMessage], add]
    todos: list[TodoItem]
    execution_plan: ExecutionPlan | None
    current_todo_id: int | None
    completed_results: dict[int, str]
    recursion_depth: int
    max_depth: int
    expert_mode: bool
    # Recursion control flag (Phase 3): set True by recursive_check_node to trigger re-planning
    trigger_recursion: bool | None


@WorkflowRegistry.register(
    name="deep_dive",
    description="Deep Dive å¤æ‚å¤šæ­¥ä»»åŠ¡ï¼ˆä»»åŠ¡åˆ†è§£ + é€’å½’è¯Šæ–­ + æ‰¹é‡æ‰§è¡Œï¼‰",
    examples=[
        "å®¡è®¡æ‰€æœ‰è¾¹ç•Œè·¯ç”±å™¨çš„ BGP é…ç½®å®Œæ•´æ€§",
        "æ‰¹é‡æ£€æŸ¥ 30+ è®¾å¤‡çš„æ¥å£å…‰åŠŸç‡",
        "ä» A æ— æ³•è®¿é—® Bï¼Œè¯·æ’æŸ¥",
        "ä¸ºä»€ä¹ˆä¸šåŠ¡æŠ¥éšœï¼ŒWeb è®¿é—®æ…¢ï¼Ÿ",
        "æ£€æŸ¥æ‰€æœ‰æ ¸å¿ƒäº¤æ¢æœºæ˜¯å¦ç¬¦åˆå®‰å…¨ç­–ç•¥",
        "å·¡æ£€æ‰€æœ‰è®¾å¤‡çš„ CPU å’Œå†…å­˜ä½¿ç”¨ç‡",
        "åˆ†æè·¨åŸŸè¿é€šæ€§é—®é¢˜",
    ],
    triggers=[
        r"å®¡è®¡",
        r"æ‰¹é‡",
        r"æ‰€æœ‰è®¾å¤‡",
        r"æ‰€æœ‰è·¯ç”±å™¨",
        r"å¤šå°è®¾å¤‡",
        r"ä¸ºä»€ä¹ˆ",
        r"æ’æŸ¥",
        r"è¯Šæ–­é—®é¢˜",
        r"ä».*åˆ°",
    ],
)
class DeepDiveWorkflow(BaseWorkflow):
    """Deep Dive Workflow for complex multi-step tasks."""

    @property
    def name(self) -> str:
        return "deep_dive"

    @property
    def description(self) -> str:
        return "Deep Dive å¤æ‚å¤šæ­¥ä»»åŠ¡ï¼ˆä»»åŠ¡åˆ†è§£ + é€’å½’è¯Šæ–­ + æ‰¹é‡æ‰§è¡Œï¼‰"

    @property
    def tools_required(self) -> list[str]:
        return [
            "suzieq_query",
            "suzieq_schema_search",
            "netconf_tool",
            "cli_tool",
            "search_openconfig_schema",
        ]

    async def validate_input(self, user_query: str) -> tuple[bool, str]:
        """Check if query requires Deep Dive workflow.

        Deep Dive triggers (aligned with Orchestrator classification):
        - Audit tasks ("å®¡è®¡", "audit", "æ£€æŸ¥å®Œæ•´æ€§")
        - Batch operations ("æ‰¹é‡", "æ‰€æœ‰è®¾å¤‡", "æ‰€æœ‰è·¯ç”±å™¨", "å¤šå°è®¾å¤‡")
        - Complex diagnostics ("ä¸ºä»€ä¹ˆ", "è¯Šæ–­é—®é¢˜", "æ’æŸ¥æ•…éšœ", "æ ¹å› åˆ†æ")
        - Cross-domain troubleshooting ("ä» A åˆ° B", "è·¨")
        - Recursive diagnostics ("æ·±å…¥åˆ†æ", "è¯¦ç»†æ’æŸ¥", "å½»åº•æ£€æŸ¥")
        """
        import re

        triggers = [
            # å®¡è®¡ç±» (Audit)
            r"å®¡è®¡",
            r"audit",
            r"æ£€æŸ¥.*å®Œæ•´æ€§",
            r"check.*integrity",
            r"é…ç½®.*å®Œæ•´",
            # æ‰¹é‡æ“ä½œ (Batch)
            r"å®¡è®¡æ‰€æœ‰",
            r"æ‰¹é‡",
            r"å…¨éƒ¨è®¾å¤‡",
            r"æ‰€æœ‰è®¾å¤‡",
            r"æ‰€æœ‰.*è·¯ç”±å™¨",
            r"all.*router",
            r"å¤š.*è®¾å¤‡",
            r"multiple.*device",
            r"å¤šå°",
            r"\d+å°",
            # å¤æ‚è¯Šæ–­
            r"ä¸ºä»€ä¹ˆ",
            r"why",
            r"è¯Šæ–­.*é—®é¢˜",
            r"diagnose.*issue",
            r"æ’æŸ¥.*æ•…éšœ",
            r"troubleshoot",
            r"æ ¹å› ",
            r"root.*cause",
            r"å½±å“èŒƒå›´",
            r"impact.*scope",
            r"ä¸ºä»€ä¹ˆ.*æ— æ³•è®¿é—®",
            r"ä».*åˆ°.*",
            r"è·¨",
            r"æ·±å…¥åˆ†æ",
            r"è¯¦ç»†æ’æŸ¥",
            r"å½»åº•æ£€æŸ¥",
            r"é€’å½’",
            # ç‰¹å®šåè®®æ·±åº¦åˆ†æ
            r"MPLS.*é…ç½®",
            r"BGP.*å®‰å…¨",
            r"OSPF.*é‚»å±…",
            r"ISIS.*æ‹“æ‰‘",
        ]

        for pattern in triggers:
            if re.search(pattern, user_query, re.IGNORECASE):
                return (True, f"Deep Dive trigger detected: '{pattern}'")

        return (False, "Query does not require Deep Dive workflow")

    def __init__(self) -> None:
        self.llm = LLMFactory.get_chat_model(json_mode=False)
        self.llm_json = LLMFactory.get_chat_model(json_mode=True)

        # Tools are imported as functions, not classes
        # Available: suzieq_query, suzieq_schema_search, netconf_get_config

    async def task_planning_node(self, state: DeepDiveState) -> dict:
        """Generate Todo List from user query using LLM.

        Args:
            state: Current workflow state

        Returns:
            Updated state with generated todos
        """
        user_query = state["messages"][-1].content if state["messages"] else ""

        # Load task planning prompt
        prompt = prompt_manager.load_prompt(
            category="workflows/deep_dive",
            name="task_planning",
            user_query=user_query,
            recursion_depth=state.get("recursion_depth", 0),
            max_depth=state.get("max_depth", 3),
        )

        # LLM generates structured Todo List
        messages = [SystemMessage(content=prompt), HumanMessage(content=user_query)]
        response = await self.llm_json.ainvoke(messages)

        # Parse JSON response to TodoItem list
        import json

        try:
            todo_data = json.loads(response.content)
            todos = [
                TodoItem(
                    id=item["id"],
                    task=item["task"],
                    status="pending",
                    result=None,
                    deps=item.get("deps", []),
                )
                for item in todo_data.get("todos", [])
            ]
        except (json.JSONDecodeError, KeyError):
            # Fallback: Create single todo from query
            todos = [TodoItem(id=1, task=user_query, status="pending", result=None, deps=[])]

        return {
            "todos": todos,
            "execution_plan": None,
            "completed_results": {},
            "recursion_depth": state.get("recursion_depth", 0),
            "max_depth": state.get("max_depth", 3),
            "trigger_recursion": False,
        }

    async def schema_investigation_node(self, state: DeepDiveState) -> dict:
        """Investigate schema feasibility for all planned tasks.

        This node:
        1. Calls suzieq_schema_search for each task to discover available tables
        2. Validates keyword mapping against schema results
        3. Categorizes tasks as feasible/uncertain/infeasible
        4. Generates execution plan with recommendations

        Returns:
            Updated state with execution_plan for user approval
        """
        from olav.tools.suzieq_parquet_tool import suzieq_schema_search

        todos = state["todos"]
        feasible_tasks = []
        uncertain_tasks = []
        infeasible_tasks = []
        recommendations = {}

        for todo in todos:
            task_text = todo["task"]
            task_id = todo["id"]

            # Step 1: Keyword-based mapping (heuristic)
            heuristic_mapping = self._map_task_to_table(task_text)

            # Step 2: Schema search (ground truth)
            try:
                schema_result = await suzieq_schema_search.ainvoke({"query": task_text})
                available_tables = schema_result.get("tables", [])

                if not available_tables:
                    # No schema match at all
                    todo["feasibility"] = "infeasible"
                    todo["schema_notes"] = (
                        "âŒ SuzieQ schema æœªæ‰¾åˆ°ç›¸å…³è¡¨ã€‚å¯èƒ½éœ€è¦ NETCONF ç›´æ¥æŸ¥è¯¢ã€‚"
                    )
                    infeasible_tasks.append(task_id)
                    recommendations[task_id] = (
                        "å»ºè®®ä½¿ç”¨ NETCONF æŸ¥è¯¢è®¾å¤‡é…ç½®ï¼Œæˆ–ç¡®è®¤ SuzieQ poller æ˜¯å¦å¯ç”¨ç›¸å…³é‡‡é›†åŠŸèƒ½ã€‚"
                    )

                elif heuristic_mapping:
                    # Validate heuristic against schema
                    heuristic_table = heuristic_mapping[0]
                    if heuristic_table in available_tables:
                        # Perfect match
                        todo["feasibility"] = "feasible"
                        todo["recommended_table"] = heuristic_table
                        todo["schema_notes"] = (
                            f"âœ… è¡¨ '{heuristic_table}' å¯ç”¨ï¼Œå­—æ®µ: {', '.join(schema_result.get(heuristic_table, {}).get('fields', [])[:5])}"
                        )
                        feasible_tasks.append(task_id)
                        recommendations[task_id] = (
                            f"ä½¿ç”¨ suzieq_query(table='{heuristic_table}', method='summarize')"
                        )
                    else:
                        # Heuristic mismatch - use first schema suggestion
                        suggested_table = available_tables[0]
                        todo["feasibility"] = "uncertain"
                        todo["recommended_table"] = suggested_table
                        todo["schema_notes"] = (
                            f"âš ï¸ å…³é”®è¯æ˜ å°„åˆ° '{heuristic_table}'ï¼Œä½† schema å»ºè®® '{suggested_table}'ã€‚"
                            f"å¯ç”¨è¡¨: {', '.join(available_tables)}"
                        )
                        uncertain_tasks.append(task_id)
                        recommendations[task_id] = (
                            f"å»ºè®®ç¡®è®¤ï¼šä»»åŠ¡æ˜¯å¦éœ€è¦ '{suggested_table}' è¡¨ï¼Ÿ"
                            f"æˆ–è€…ä½¿ç”¨ '{heuristic_table}' ä½†å¯èƒ½æ— ç›¸å…³æ•°æ®ã€‚"
                        )

                else:
                    # No heuristic mapping, but schema has suggestions
                    suggested_table = available_tables[0]
                    todo["feasibility"] = "uncertain"
                    todo["recommended_table"] = suggested_table
                    todo["schema_notes"] = (
                        f"âš ï¸ æ— å…³é”®è¯æ˜ å°„ï¼Œschema å»ºè®®: {', '.join(available_tables[:3])}"
                    )
                    uncertain_tasks.append(task_id)
                    recommendations[task_id] = (
                        f"å»ºè®®ä½¿ç”¨ '{suggested_table}' æˆ–ç”±ç”¨æˆ·æŒ‡å®šå…·ä½“è¡¨åã€‚"
                    )

            except Exception as e:
                # Schema search failed
                todo["feasibility"] = "uncertain"
                todo["schema_notes"] = f"âš ï¸ Schema è°ƒæŸ¥å¤±è´¥: {e!s}"
                uncertain_tasks.append(task_id)
                recommendations[task_id] = "Schema æŸ¥è¯¢å¼‚å¸¸ï¼Œå»ºè®®äººå·¥ç¡®è®¤æˆ–é‡è¯•ã€‚"

        # Generate execution plan
        execution_plan: ExecutionPlan = {
            "feasible_tasks": feasible_tasks,
            "uncertain_tasks": uncertain_tasks,
            "infeasible_tasks": infeasible_tasks,
            "recommendations": recommendations,
            "user_approval_required": len(uncertain_tasks) > 0 or len(infeasible_tasks) > 0,
        }

        # Generate plan summary message
        plan_summary = self._format_execution_plan(todos, execution_plan)

        return {
            "todos": todos,
            "execution_plan": execution_plan,
            "messages": [AIMessage(content=plan_summary)],
        }

    def _format_execution_plan(self, todos: list[TodoItem], plan: ExecutionPlan) -> str:
        """Format execution plan for user review."""
        lines = ["## ğŸ“‹ æ‰§è¡Œè®¡åˆ’ï¼ˆSchema è°ƒç ”ç»“æœï¼‰\n"]

        if plan["feasible_tasks"]:
            lines.append(f"### âœ… å¯æ‰§è¡Œä»»åŠ¡ ({len(plan['feasible_tasks'])} ä¸ª)\n")
            for task_id in plan["feasible_tasks"]:
                todo = next(t for t in todos if t["id"] == task_id)
                lines.append(f"- **ä»»åŠ¡ {task_id}**: {todo['task']}")
                lines.append(f"  - {todo['schema_notes']}")
                lines.append(f"  - {plan['recommendations'][task_id]}\n")

        if plan["uncertain_tasks"]:
            lines.append(f"### âš ï¸ ä¸ç¡®å®šä»»åŠ¡ ({len(plan['uncertain_tasks'])} ä¸ª) - éœ€è¦ç¡®è®¤\n")
            for task_id in plan["uncertain_tasks"]:
                todo = next(t for t in todos if t["id"] == task_id)
                lines.append(f"- **ä»»åŠ¡ {task_id}**: {todo['task']}")
                lines.append(f"  - {todo['schema_notes']}")
                lines.append(f"  - {plan['recommendations'][task_id]}\n")

        if plan["infeasible_tasks"]:
            lines.append(f"### âŒ æ— æ³•æ‰§è¡Œä»»åŠ¡ ({len(plan['infeasible_tasks'])} ä¸ª)\n")
            for task_id in plan["infeasible_tasks"]:
                todo = next(t for t in todos if t["id"] == task_id)
                lines.append(f"- **ä»»åŠ¡ {task_id}**: {todo['task']}")
                lines.append(f"  - {todo['schema_notes']}")
                lines.append(f"  - {plan['recommendations'][task_id]}\n")

        if plan["user_approval_required"]:
            lines.append("\n---\n")
            lines.append(
                "**â¸ï¸ ç­‰å¾…ç”¨æˆ·å®¡æ‰¹**: å­˜åœ¨ä¸ç¡®å®šæˆ–æ— æ³•æ‰§è¡Œçš„ä»»åŠ¡ï¼Œè¯·ç¡®è®¤æ˜¯å¦ç»§ç»­æ‰§è¡Œå¯è¡Œä»»åŠ¡ï¼Œæˆ–ä¿®æ”¹è®¡åˆ’ã€‚\n"
            )
            lines.append("- è¾“å…¥ `approve` ç»§ç»­æ‰§è¡Œå¯è¡Œä»»åŠ¡")
            lines.append("- è¾“å…¥ `modify` ä¿®æ”¹ä»»åŠ¡è®¡åˆ’")
            lines.append("- è¾“å…¥ `abort` ç»ˆæ­¢æ‰§è¡Œ")

        return "\n".join(lines)

    async def execute_todo_node(self, state: DeepDiveState) -> dict:
        """Execute next eligible todo with real tool invocation where possible.

        Priority:
        1. Heuristic keyword mapping (device, interface, routes, bgp, etc.)
        2. Schema existence check via suzieq_schema_search
        3. Distinguish SCHEMA_NOT_FOUND vs NO_DATA_FOUND vs OK
        4. Fallback to LLM-driven execution prompt if mapping fails or table unsupported
        """
        import asyncio  # Local import to avoid global side-effects

        todos = state["todos"]
        completed_results = state.get("completed_results", {})

        # ------------------------------------------------------------------
        # Parallel batch execution (Phase 3.2)
        # Strategy: Identify all ready & dependency-satisfied todos without deps.
        # Run up to parallel_batch_size concurrently. Falls back to serial path
        # when <=1 independent ready todo.
        # ------------------------------------------------------------------
        parallel_batch_size = state.get("parallel_batch_size", 5)

        ready: list[TodoItem] = []
        for todo in todos:
            if todo["status"] == "pending":
                deps_ok = all(
                    any(t["id"] == dep_id and t["status"] in {"completed", "failed"} for t in todos)
                    for dep_id in todo["deps"]
                )
                if deps_ok:
                    ready.append(todo)

        independent = [t for t in ready if not t["deps"]]

        if len(independent) > 1:
            batch = independent[:parallel_batch_size]
            # Mark batch in-progress
            for t in batch:
                t["status"] = "in-progress"

            async def _execute_single(todo: TodoItem) -> tuple[TodoItem, list[BaseMessage]]:
                task_text = todo["task"].strip()
                mapping = self._map_task_to_table(task_text)
                tool_result: dict | None = None
                messages: list[BaseMessage] = []
                if mapping:
                    table, method, extra_filters = mapping
                    tool_input = {"table": table, "method": method, **extra_filters}
                    try:
                        from olav.tools.suzieq_parquet_tool import (  # type: ignore
                            suzieq_query,
                            suzieq_schema_search,
                        )

                        schema = await suzieq_schema_search.ainvoke({"query": table})
                        available_tables = schema.get("tables", [])
                        if table in available_tables:
                            tool_result = await suzieq_query.ainvoke(tool_input)
                        else:
                            tool_result = {
                                "status": "SCHEMA_NOT_FOUND",
                                "table": table,
                                "message": f"Table '{table}' not present in discovered schema tables.",
                                "available_tables": available_tables,
                            }
                    except Exception as e:
                        tool_result = {
                            "status": "TOOL_ERROR",
                            "error": str(e),
                            "table": table,
                            "method": method,
                            "input": tool_input,
                        }

                if tool_result:
                    classified = self._classify_tool_result(tool_result)
                    # Failure statuses propagate directly
                    if classified["status"] in {
                        "SCHEMA_NOT_FOUND",
                        "NO_DATA_FOUND",
                        "DATA_NOT_RELEVANT",
                        "TOOL_ERROR",
                    }:
                        todo["status"] = "failed"
                        todo["result"] = (
                            f"âš ï¸ æ‰¹é‡ä»»åŠ¡å¤±è´¥: {classified['status']} table={classified['table']}"
                        )
                        completed_results[todo["id"]] = todo["result"]
                        return todo, [AIMessage(content=todo["result"])]

                    raw_trunc = str(tool_result.get("data", tool_result))[:400]
                    todo["status"] = "completed"
                    todo["result"] = (
                        f"âœ… å¹¶è¡Œä»»åŠ¡å®Œæˆ table={classified['table']} count={classified['count']}\n{raw_trunc}"
                    )
                    messages.append(
                        AIMessage(
                            content=f"Parallel task {todo['id']} completed on {classified['table']}"
                        )
                    )
                else:
                    # Fallback LLM path
                    prompt = prompt_manager.load_prompt(
                        category="workflows/deep_dive",
                        name="execute_todo",
                        task=task_text,
                        available_tools="suzieq_query, netconf_tool, search_openconfig_schema",
                    )
                    llm_resp = await self.llm.ainvoke(
                        [
                            SystemMessage(content=prompt),
                            HumanMessage(content=f"Execute task: {task_text}"),
                        ]
                    )
                    todo["status"] = "completed"
                    todo["result"] = llm_resp.content
                    messages.append(
                        AIMessage(content=f"Parallel task {todo['id']} completed via LLM fallback")
                    )

                completed_results[todo["id"]] = todo["result"]
                return todo, messages

            results = await asyncio.gather(
                *[_execute_single(t) for t in batch], return_exceptions=True
            )
            aggregated_messages: list[BaseMessage] = []
            for res in results:
                if isinstance(res, Exception):  # Defensive: unexpected batch error
                    aggregated_messages.append(AIMessage(content=f"æ‰¹é‡æ‰§è¡Œå‡ºç°æœªæ•è·å¼‚å¸¸: {res}"))
                else:
                    _todo, msgs = res
                    aggregated_messages.extend(msgs)

            # Decide next step message
            aggregated_messages.append(AIMessage(content=f"å¹¶è¡Œæ‰¹æ¬¡å®Œæˆ: {len(batch)} ä¸ªä»»åŠ¡."))
            return {
                "todos": todos,
                "current_todo_id": batch[-1]["id"],
                "completed_results": completed_results,
                "messages": aggregated_messages,
            }

        # ------------------------------------------------------------------
        # Serial execution fallback (original logic) when 0 or 1 independent
        # ------------------------------------------------------------------
        next_todo: TodoItem | None = None
        for todo in todos:
            if todo["status"] == "pending":
                deps_ok = all(
                    any(t["id"] == dep_id and t["status"] in {"completed", "failed"} for t in todos)
                    for dep_id in todo["deps"]
                )
                if deps_ok or not todo["deps"]:
                    next_todo = todo
                    break

        if not next_todo:
            return {"messages": [AIMessage(content="All pending tasks processed.")]}

        # Mark in-progress
        next_todo["status"] = "in-progress"
        task_text = next_todo["task"].strip()
        tool_result: dict | None = None
        mapping = self._map_task_to_table(task_text)
        tool_messages: list[BaseMessage] = []

        if mapping:
            table, method, extra_filters = mapping
            tool_input = {"table": table, "method": method, **extra_filters}
            try:
                # Local import to avoid global dependency issues
                from olav.tools.suzieq_parquet_tool import (  # type: ignore
                    suzieq_query,
                    suzieq_schema_search,
                )

                # Discover available tables; suzieq_schema_search returns {"tables": [...], "bgp": {...}, ...}
                schema = await suzieq_schema_search.ainvoke({"query": table})
                available_tables = schema.get("tables", [])

                if table in available_tables:
                    tool_result = await suzieq_query.ainvoke(tool_input)

                    # æ–¹æ¡ˆ2: å­—æ®µè¯­ä¹‰éªŒè¯ - æ£€æŸ¥è¿”å›å­—æ®µæ˜¯å¦ä¸ä»»åŠ¡ç›¸å…³
                    if (
                        tool_result
                        and "columns" in tool_result
                        and tool_result.get("status") != "NO_DATA_FOUND"
                    ):
                        is_relevant = self._validate_field_relevance(
                            task_text=task_text,
                            returned_columns=tool_result["columns"],
                            queried_table=table,
                        )
                        if not is_relevant:
                            # Data returned but not relevant to task
                            tool_result = {
                                "status": "DATA_NOT_RELEVANT",
                                "table": table,
                                "returned_columns": tool_result["columns"],
                                "message": f"è¡¨ '{table}' è¿”å›äº†æ•°æ®ï¼Œä½†å­—æ®µä¸ä»»åŠ¡éœ€æ±‚ä¸åŒ¹é…ã€‚",
                                "hint": f"ä»»åŠ¡å…³é”®è¯: {self._extract_task_keywords(task_text)}ï¼Œè¿”å›å­—æ®µ: {tool_result['columns'][:5]}",
                                "suggestion": "å¯èƒ½éœ€è¦ä½¿ç”¨ NETCONF æŸ¥è¯¢æˆ–é‡æ–°è§„åˆ’ä»»åŠ¡ã€‚",
                            }
                else:
                    tool_result = {
                        "status": "SCHEMA_NOT_FOUND",
                        "table": table,
                        "message": f"Table '{table}' not present in discovered schema tables.",
                        "hint": "Use suzieq_schema_search with a broader query or verify poller collection.",
                        "available_tables": available_tables,
                    }
            except Exception as e:
                tool_result = {
                    "status": "TOOL_ERROR",
                    "error": str(e),
                    "table": table,
                    "method": method,
                    "input": tool_input,
                }

        if tool_result:
            classified = self._classify_tool_result(tool_result)
            summary = (
                f"TOOL_CALL table={classified['table']} status={classified['status']} "
                f"count={classified['count']}"
            )

            # CRITICAL: é˜²æ­¢ LLM å¹»è§‰ - åœ¨é‡åˆ°é”™è¯¯çŠ¶æ€æ—¶ç›´æ¥è¿”å›å¤±è´¥ï¼Œä¸ç»§ç»­å¤„ç†
            if classified["status"] in {
                "SCHEMA_NOT_FOUND",
                "NO_DATA_FOUND",
                "DATA_NOT_RELEVANT",
                "TOOL_ERROR",
            }:
                error_msg = (
                    f"âš ï¸ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {classified['status']}\n"
                    f"è¡¨: {classified['table']}\n"
                    f"åŸå› : {tool_result.get('message') or tool_result.get('error', 'æœªçŸ¥é”™è¯¯')}\n"
                    f"æç¤º: {tool_result.get('hint', 'N/A')}\n"
                )

                # DATA_NOT_RELEVANT éœ€è¦é¢å¤–è¯´æ˜
                if classified["status"] == "DATA_NOT_RELEVANT":
                    error_msg += (
                        f"\nâš ï¸ **æ•°æ®è¯­ä¹‰ä¸åŒ¹é…**: æŸ¥è¯¢çš„è¡¨è¿”å›äº†æ•°æ®ï¼Œä½†å­—æ®µä¸ä»»åŠ¡éœ€æ±‚ä¸ç›¸å…³ã€‚\n"
                        f"å»ºè®®: {tool_result.get('suggestion', 'é‡æ–°è§„åˆ’ä»»åŠ¡æˆ–ä½¿ç”¨ NETCONF ç›´æ¥æŸ¥è¯¢')}\n"
                    )

                error_msg += (
                    "\nâ›” **ä¸¥æ ¼ç¦æ­¢ç¼–é€ æ•°æ®** - æ— ç›¸å…³æ•°æ®å³æŠ¥å‘Šå¤±è´¥ï¼Œä¸æ¨æµ‹æˆ–ç”Ÿæˆè™šå‡ç»“æœã€‚"
                )

                next_todo["status"] = "failed"
                next_todo["result"] = error_msg
                completed_results[next_todo["id"]] = error_msg

                return {
                    "todos": todos,
                    "current_todo_id": next_todo["id"],
                    "completed_results": completed_results,
                    "messages": [AIMessage(content=error_msg)],
                }

            # æˆåŠŸçŠ¶æ€ï¼šæ ¼å¼åŒ–ç»“æœ
            raw_trunc = str(tool_result.get("data", tool_result))[:800]
            result_text = f"{summary}\n\nâœ… æ•°æ®æ‘˜è¦:\n{raw_trunc}"
            tool_messages.append(
                AIMessage(
                    content=f"Used suzieq_query on {classified['table']} status={classified['status']} count={classified['count']}"
                )
            )
        else:
            # Fallback to LLM execution strategy
            prompt = prompt_manager.load_prompt(
                category="workflows/deep_dive",
                name="execute_todo",
                task=task_text,
                available_tools="suzieq_query, netconf_tool, search_openconfig_schema",
            )
            messages = [
                SystemMessage(content=prompt),
                HumanMessage(content=f"Execute task: {task_text}"),
            ]
            llm_resp = await self.llm.ainvoke(messages)
            result_text = llm_resp.content

        # Complete todo (only if not already marked failed above)
        if next_todo["status"] != "failed":
            next_todo["status"] = "completed"
        next_todo["result"] = result_text

        # ------------------------------------------------------------------
        # Phase 2: External Evaluator integration (Schema-Aware dynamic)
        # ------------------------------------------------------------------
        try:
            if next_todo["status"] == "completed" and tool_result:
                from olav.evaluators.config_compliance import ConfigComplianceEvaluator

                evaluator = ConfigComplianceEvaluator()
                eval_result = await evaluator.evaluate(next_todo, tool_result)

                next_todo["evaluation_passed"] = eval_result.passed
                next_todo["evaluation_score"] = eval_result.score

                if not eval_result.passed:
                    next_todo["failure_reason"] = eval_result.feedback
                    # Reclassify status to failed and append evaluator feedback
                    next_todo["status"] = "failed"
                    appended = f"\nğŸ” è¯„ä¼°æœªé€šè¿‡: {eval_result.feedback}"
                    next_todo["result"] = (next_todo["result"] or "") + appended
        except Exception as eval_err:
            # Non-fatal â€“ store failure_reason for visibility
            next_todo["evaluation_passed"] = False
            next_todo["evaluation_score"] = 0.0
            next_todo["failure_reason"] = f"Evaluator error: {eval_err}"

        completed_results[next_todo["id"]] = next_todo["result"]

        completion = AIMessage(content=f"Completed task {next_todo['id']}: {result_text[:600]}")
        return {
            "todos": todos,
            "current_todo_id": next_todo["id"],
            "completed_results": completed_results,
            "messages": [*tool_messages, completion],
        }

    def _map_task_to_table(self, task: str) -> tuple[str, str, dict] | None:
        """Map natural language task to (table, method, filters) using ordered specificity.

        Order matters: more specific/general inventory tasks first, then protocol.
        Returns None if no mapping found (will trigger schema investigation).
        """
        lower = task.lower()

        candidates: list[tuple[list[str], str, str]] = [
            # Inventory / device list
            (["è®¾å¤‡åˆ—è¡¨", "æ‰€æœ‰è®¾å¤‡", "å®¡è®¡è®¾å¤‡", "device", "è®¾å¤‡"], "device", "summarize"),
            # Interfaces
            (["æ¥å£", "ç«¯å£", "interface", "ç‰©ç†", "rx", "tx", "é“¾è·¯"], "interfaces", "summarize"),
            # Routing / prefixes
            (["è·¯ç”±", "å‰ç¼€", "routes", "lpm"], "routes", "summarize"),
            # OSPF
            (["ospf"], "ospfIf", "summarize"),
            # LLDP
            (["lldp"], "lldp", "summarize"),
            # MAC
            (["mac", "äºŒå±‚"], "macs", "summarize"),
            # BGP (put later to avoid greedy matching of 'è¾¹ç•Œ')
            (["bgp", "peer", "é‚»å±…", "è¾¹ç•Œ"], "bgp", "summarize"),
        ]
        for keywords, table, method in candidates:
            if any(k in lower for k in keywords):
                import re

                hosts = re.findall(r"\b([A-Za-z]{1,4}\d{1,2})\b", task)
                filters: dict[str, Any] = {}
                if hosts:
                    filters["hostname"] = hosts[0]
                return table, method, filters
        return None

    def _classify_tool_result(self, result: dict[str, Any]) -> dict[str, Any]:
        """Normalize tool result into status/count/table for summary lines."""
        status = "OK"
        table = result.get("table", "unknown")
        count = result.get("count")
        if count is None and isinstance(result.get("data"), list):
            count = len(result.get("data", []))

        # Priority 1: Explicit DATA_NOT_RELEVANT status (field validation failed)
        if result.get("status") == "DATA_NOT_RELEVANT":
            status = "DATA_NOT_RELEVANT"
        # Priority 2: Explicit error field (tool execution failed)
        elif "error" in result:
            error_msg = str(result["error"])
            # Check if error indicates unknown table (schema validation)
            if "Unknown table" in error_msg or "available_tables" in result:
                status = "SCHEMA_NOT_FOUND"
            else:
                status = "TOOL_ERROR"
        # Priority 3: Explicit schema not found status (from our validation)
        elif result.get("status") == "SCHEMA_NOT_FOUND":
            status = "SCHEMA_NOT_FOUND"
        # Priority 4: NO_DATA_FOUND sentinel in first data record
        elif isinstance(result.get("data"), list) and result["data"]:
            first = result["data"][0]
            if isinstance(first, dict) and first.get("status") == "NO_DATA_FOUND":
                status = "NO_DATA_FOUND"
        # Priority 5: Empty data list
        elif isinstance(result.get("data"), list) and len(result.get("data", [])) == 0:
            status = "NO_DATA_FOUND"

        return {"status": status, "table": table, "count": count if count is not None else 0}

    def _validate_field_relevance(
        self, task_text: str, returned_columns: list[str], queried_table: str
    ) -> bool:
        """Validate if returned columns are semantically relevant to task (æ–¹æ¡ˆ2).

        Args:
            task_text: Original task description
            returned_columns: Field names returned from query
            queried_table: Table that was queried

        Returns:
            True if fields appear relevant, False otherwise
        """
        # Extract task keywords (nouns/technical terms)
        task_keywords = self._extract_task_keywords(task_text)
        columns_str = " ".join(returned_columns).lower()

        # Check ifä»»ä½•ä»»åŠ¡å…³é”®è¯å‡ºç°åœ¨å­—æ®µåä¸­
        # ä¾‹å¦‚: task="MPLSé…ç½®" keywords=["mpls"], columns=["hostname", "model"] â†’ False
        #       task="BGPçŠ¶æ€" keywords=["bgp"], columns=["peer", "asn", "state"] â†’ True
        matches = sum(1 for kw in task_keywords if kw in columns_str)

        # Threshold: at least 1 keyword match, or it's a generic device/interface query
        if matches > 0:
            return True

        # Special case: device/interfaces are generic inventory, acceptable for most tasks
        if queried_table in {"device", "interfaces"}:
            return True

        # No semantic match
        return False

    def _extract_task_keywords(self, task_text: str) -> list[str]:
        """Extract technical keywords from task description."""
        lower = task_text.lower()
        # Common network protocol/feature keywords
        keywords = [
            "mpls",
            "ldp",
            "rsvp",
            "bgp",
            "ospf",
            "eigrp",
            "isis",
            "vlan",
            "vxlan",
            "evpn",
            "interface",
            "route",
            "prefix",
            "neighbor",
            "peer",
            "session",
            "tunnel",
            "policy",
            "qos",
            "acl",
            "nat",
            "firewall",
            "vpn",
        ]
        return [kw for kw in keywords if kw in lower]

    async def should_continue(
        self, state: DeepDiveState
    ) -> Literal["execute_todo", "recursive_check"]:
        """Decide whether to continue executing todos or move to recursive check.

        Args:
            state: Current workflow state

        Returns:
            Next node to execute
        """
        todos = state["todos"]
        pending_count = sum(1 for t in todos if t["status"] == "pending")

        if pending_count > 0:
            return "execute_todo"
        return "recursive_check"

    async def recursive_check_node(self, state: DeepDiveState) -> dict:
        """Check if recursive deep dive is needed.

        Phase 3.4 Enhancement: Handles multiple failures in parallel, not just the first one.
        Creates focused sub-tasks for each failed todo (up to max_failures_per_recursion).

        Args:
            state: Current workflow state

        Returns:
            Updated state with potential new sub-todos for all failures
        """
        recursion_depth = state.get("recursion_depth", 0)
        max_depth = state.get("max_depth", 3)
        max_failures_per_recursion = (
            3  # Limit parallel failure investigation to avoid prompt explosion
        )

        # Depth guard
        if recursion_depth >= max_depth:
            return {
                "messages": [
                    AIMessage(
                        content=f"Max recursion depth ({max_depth}) reached. Moving to summary."
                    )
                ],
                "trigger_recursion": False,
            }

        todos = state.get("todos", [])
        failed_todos = [t for t in todos if t.get("status") == "failed"]

        if not failed_todos:
            return {
                "messages": [AIMessage(content="No deeper analysis needed.")],
                "trigger_recursion": False,
            }

        # PHASE 3.4: Handle multiple failures (not just first one)
        # Limit to top N failures to avoid overwhelming prompt/planning
        failures_to_analyze = failed_todos[:max_failures_per_recursion]

        # Build recursive prompt for ALL selected failures
        failure_summaries = []
        for failed in failures_to_analyze:
            parent_task_id = failed["id"]
            parent_task_text = failed["task"]
            parent_result = (failed.get("result") or "")[
                :400
            ]  # Truncate per failure to fit multiple
            parent_reason = failed.get("failure_reason", "Unknown")

            failure_summaries.append(
                f"  â€¢ å¤±è´¥ä»»åŠ¡ {parent_task_id}: {parent_task_text}\n"
                f"    å¤±è´¥åŸå› : {parent_reason}\n"
                f"    è¾“å‡ºæ‘˜è¦: {parent_result}\n"
            )

        recursive_prompt = (
            f"é€’å½’æ·±å…¥åˆ†æ: æ£€æµ‹åˆ° {len(failures_to_analyze)} ä¸ªå¤±è´¥ä»»åŠ¡ï¼Œéœ€è¦ç”Ÿæˆæ›´ç»†ç²’åº¦çš„å­ä»»åŠ¡ã€‚\n\n"
            "å¤±è´¥ä»»åŠ¡åˆ—è¡¨:\n" + "\n".join(failure_summaries) + "\n\n"
            "è¯·éµå¾ªè¦æ±‚: \n"
            f"1) ä¸ºæ¯ä¸ªå¤±è´¥ä»»åŠ¡ç”Ÿæˆ 1-2 ä¸ªæ›´å…·ä½“çš„å­ä»»åŠ¡ï¼ˆæ€»å…± {len(failures_to_analyze) * 2} ä¸ªå·¦å³ï¼‰ã€‚\n"
            "2) å­ä»»åŠ¡éœ€æ›´å…·ä½“ï¼Œä¾‹å¦‚èšç„¦æŸåè®®å®ä¾‹ã€é‚»å±…ã€æ¥å£æˆ–å­—æ®µã€‚\n"
            "3) é¿å…ä¸çˆ¶ä»»åŠ¡å®Œå…¨é‡å¤ã€‚\n"
            '4) ä½¿ç”¨ JSON è¾“å‡º: {\n  "todos": [ {"id": <int>, "task": <str>, "deps": [] } ]\n}ã€‚\n'
            "5) ID ä»ç°æœ‰æœ€å¤§ ID + 1 å¼€å§‹é€’å¢ã€‚\n"
            "6) åœ¨ task æ–‡æœ¬ä¸­åŒ…å«çˆ¶ä»»åŠ¡å¼•ç”¨: '(parent:<id>)'ï¼Œä¾‹å¦‚ 'æ£€æŸ¥ R1 BGP é…ç½® (parent:3)'ã€‚\n"
            "7) å¦‚æœæŸå¤±è´¥ä»»åŠ¡æ— æ³•è¿›ä¸€æ­¥ç»†åŒ–ï¼Œç”Ÿæˆä¸€ä¸ªéªŒè¯æ€§ä»»åŠ¡ï¼Œä¾‹å¦‚ 'éªŒè¯é‡‡é›†æ˜¯å¦ç¼ºå¤± (parent:<id>)'ã€‚\n"
        )

        return {
            "messages": [HumanMessage(content=recursive_prompt)],
            "recursion_depth": recursion_depth + 1,
            "trigger_recursion": True,
        }

    async def should_recurse(
        self, state: DeepDiveState
    ) -> Literal["final_summary", "task_planning"]:
        """Decide whether to recurse or finalize.

        Args:
            state: Current workflow state

        Returns:
            Next node to execute
        """
        if state.get("trigger_recursion"):
            return "task_planning"
        return "final_summary"

    async def final_summary_node(self, state: DeepDiveState) -> dict:
        """Generate final summary report from all completed todos.

        Args:
            state: Current workflow state

        Returns:
            Updated state with final summary message
        """
        todos = state["todos"]
        completed_results = state.get("completed_results", {})

        # Load summary prompt
        prompt = prompt_manager.load_prompt(
            category="workflows/deep_dive",
            name="final_summary",
            todos=str(todos),
            results=str(completed_results),
        )

        messages = [SystemMessage(content=prompt)]
        response = await self.llm.ainvoke(messages)

        return {
            "messages": [AIMessage(content=response.content)],
        }

    def build_graph(self, checkpointer: AsyncPostgresSaver) -> StateGraph:
        """Build Deep Dive Workflow graph with schema investigation and HITL approval.

        Flow:
        1. task_planning â†’ Generate todos
        2. schema_investigation â†’ Validate feasibility, generate execution plan
        3. [INTERRUPT] â†’ Wait for user approval/modification
        4. execute_todo â†’ Execute approved tasks
        5. recursive_check â†’ Determine if deeper analysis needed
        6. final_summary â†’ Generate report

        Args:
            checkpointer: PostgreSQL checkpointer for state persistence

        Returns:
            Compiled StateGraph with HITL interrupts
        """
        workflow = StateGraph(DeepDiveState)

        # Add nodes
        workflow.add_node("task_planning", self.task_planning_node)
        workflow.add_node("schema_investigation", self.schema_investigation_node)
        workflow.add_node("execute_todo", self.execute_todo_node)
        workflow.add_node("recursive_check", self.recursive_check_node)
        workflow.add_node("final_summary", self.final_summary_node)

        # Define edges
        workflow.set_entry_point("task_planning")
        workflow.add_edge("task_planning", "schema_investigation")

        # HITL approval after schema investigation
        # LangGraph will interrupt here if execution_plan.user_approval_required = True
        workflow.add_edge("schema_investigation", "execute_todo")

        workflow.add_conditional_edges(
            "execute_todo",
            self.should_continue,
            {
                "execute_todo": "execute_todo",  # Loop back for next todo
                "recursive_check": "recursive_check",
            },
        )
        workflow.add_conditional_edges(
            "recursive_check",
            self.should_recurse,
            {
                "task_planning": "task_planning",  # Recurse
                "final_summary": "final_summary",
            },
        )
        workflow.add_edge("final_summary", END)

        # Compile with checkpointer and interrupt points
        # When execution_plan.user_approval_required = True, graph will pause
        return workflow.compile(
            checkpointer=checkpointer,
            interrupt_before=["execute_todo"],  # Always pause before execution for review
        )
