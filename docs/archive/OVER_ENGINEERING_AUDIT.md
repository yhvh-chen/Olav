# è¿‡åº¦å·¥ç¨‹åŒ–å®¡è®¡æŠ¥å‘Š (Over-Engineering Audit Report)

> **å®¡è®¡æ—¥æœŸ**: 2025-01-27  
> **å®¡è®¡èŒƒå›´**: `src/olav/` æ ¸å¿ƒæ¨¡å—  
> **ç›®çš„**: è¯†åˆ«å¯ç”¨ LangChain å†…ç½®åŠŸèƒ½æˆ– LLM èƒ½åŠ›æ›¿ä»£çš„è‡ªå®šä¹‰å®ç°

---

## ğŸ“Š å®¡è®¡æ€»ç»“

| ä¼˜å…ˆçº§ | æ¨¡å— | ä»£ç è¡Œæ•° | é—®é¢˜æè¿° | æ¨èæ–¹æ¡ˆ | é¢„è®¡èŠ‚çœ |
|--------|------|----------|----------|----------|----------|
| **P0** | `extract_json_from_response()` | ~50è¡Œ | è‡ªå®šä¹‰ JSON æå– | `with_structured_output()` | 100% |
| **P0** | `DynamicIntentRouter` | ~300è¡Œ | sklearn å‘é‡ç›¸ä¼¼åº¦ | LangChain VectorStore | 80% |
| **P1** | `ToolRegistry` | ~200è¡Œ | è‡ªå®šä¹‰å·¥å…·æ³¨å†Œ | LangChain `@tool` è£…é¥°å™¨ | 70% |
| **P1** | `cache.py` (RedisCache) | ~600è¡Œ | è‡ªå®šä¹‰ç¼“å­˜æŠ½è±¡ | LangGraph InMemoryCache / SQLite | 50% |
| **P2** | `FilesystemMiddleware` | ~500è¡Œ | è‡ªå®šä¹‰æ–‡ä»¶ç¼“å­˜ | ç®€åŒ–æˆ–ç§»é™¤ | 70% |
| **P2** | `tool_call_parser.py` | ~100è¡Œ | ä¿®å¤ OpenRouter æ ¼å¼ | å·²åœ¨ LangChain ä¿®å¤ | 100% |
| **P3** | `MemoryWriter` | ~225è¡Œ | æƒ…æ™¯è®°å¿†å†™å…¥ | ä¿ç•™ï¼ˆä¸šåŠ¡ç‰¹å®šï¼‰ | 0% |

---

## ğŸ”´ P0: é«˜ä¼˜å…ˆçº§ - ç«‹å³æ›¿æ¢

### 1. `extract_json_from_response()` â†’ `with_structured_output()`

**ä½ç½®**: `src/olav/strategies/deep_path.py:39-88`

**å½“å‰å®ç°** (50è¡Œè‡ªå®šä¹‰ JSON æå–):
```python
def extract_json_from_response(response_text: str) -> Any:
    """ä» LLM å“åº”ä¸­æå– JSONï¼Œå¤„ç† markdown ä»£ç å—ç­‰æ ¼å¼"""
    # æ­£åˆ™åŒ¹é… ```json ... ```
    code_block_patterns = [
        r'```json\s*([\s\S]*?)\s*```',
        r'```\s*([\s\S]*?)\s*```',
    ]
    for pattern in code_block_patterns:
        match = re.search(pattern, text)
        if match:
            try:
                return json.loads(match.group(1).strip())
            except json.JSONDecodeError:
                continue
    # ... æ›´å¤š fallback é€»è¾‘
```

**é—®é¢˜**:
- è„†å¼±çš„æ­£åˆ™è§£æ
- æ— æ³•å¤„ç†å¤æ‚åµŒå¥—
- é‡å¤å®ç°ï¼ˆ`fast_path.py:729` ä¹Ÿæœ‰ç±»ä¼¼ä»£ç ï¼‰

**æ¨èæ–¹æ¡ˆ**: LangChain `with_structured_output()`
```python
from pydantic import BaseModel, Field

class Hypothesis(BaseModel):
    description: str = Field(description="What this hypothesis proposes")
    reasoning: str = Field(description="Why this hypothesis is plausible")
    confidence: float = Field(ge=0.0, le=1.0)

# ç›´æ¥è·å–ç»“æ„åŒ–è¾“å‡ºï¼Œæ— éœ€ JSON è§£æ
structured_llm = self.llm.with_structured_output(Hypothesis)
hypothesis = await structured_llm.ainvoke(prompt)
# hypothesis å·²ç»æ˜¯ Hypothesis ç±»å‹ï¼Œæ— éœ€ extract_json_from_response()
```

**å½±å“èŒƒå›´**:
- `deep_path.py`: 5å¤„è°ƒç”¨ (`extract_json_from_response`)
- `fast_path.py`: 1å¤„è°ƒç”¨ (æ­£åˆ™æå–)
- `deep_dive.py`: 3å¤„è°ƒç”¨ (`json.loads(response.content)`)
- `root_agent_orchestrator.py`: 1å¤„è°ƒç”¨

**è¿ç§»è®¡åˆ’**:
1. ä¸ºæ¯ä¸ª JSON è¾“å‡ºå®šä¹‰ Pydantic æ¨¡å‹
2. ä½¿ç”¨ `llm.with_structured_output(Model)` æ›¿æ¢
3. åˆ é™¤ `extract_json_from_response()` å‡½æ•°

---

### 2. `DynamicIntentRouter` sklearn â†’ LangChain VectorStore

**ä½ç½®**: `src/olav/agents/dynamic_orchestrator.py`

**å½“å‰å®ç°** (~300è¡Œ):
```python
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class DynamicIntentRouter:
    async def semantic_prefilter(self, query: str) -> list[tuple[str, float]]:
        query_vector = await self.embeddings.aembed_query(query)
        query_array = np.array([query_vector])
        
        for name, workflow_vector in self.example_vectors.items():
            vector_array = np.array([workflow_vector])
            similarity = cosine_similarity(query_array, vector_array)[0][0]
            similarities.append((name, float(similarity)))
```

**é—®é¢˜**:
- å¼•å…¥ `sklearn` + `numpy` ä¾èµ–
- æ‰‹åŠ¨ç®¡ç†å‘é‡ç´¢å¼• (`self.example_vectors: dict[str, np.ndarray]`)
- é‡å¤å®ç° LangChain VectorStore çš„åŠŸèƒ½

**æ¨èæ–¹æ¡ˆ**: LangChain InMemoryVectorStore
```python
from langchain_core.vectorstores import InMemoryVectorStore

class DynamicIntentRouter:
    def __init__(self, llm, embeddings):
        self.vector_store = InMemoryVectorStore(embeddings)
        
    async def build_index(self):
        workflows = self.registry.list_workflows()
        documents = [
            Document(page_content=example, metadata={"workflow": wf.name})
            for wf in workflows
            for example in wf.examples
        ]
        await self.vector_store.aadd_documents(documents)
    
    async def semantic_prefilter(self, query: str) -> list[str]:
        results = await self.vector_store.asimilarity_search_with_score(query, k=self.top_k)
        return [(doc.metadata["workflow"], score) for doc, score in results]
```

**ä¼˜åŠ¿**:
- ç§»é™¤ `sklearn` / `numpy` ä¾èµ–
- ç»Ÿä¸€ä½¿ç”¨ LangChain æŠ½è±¡
- å¯è½»æ¾åˆ‡æ¢åˆ° FAISS / Chroma ç­‰æŒä¹…åŒ–å‘é‡åº“

**è¿ç§»è®¡åˆ’**:
1. æ›¿æ¢ `numpy` æ•°ç»„ä¸º `InMemoryVectorStore`
2. åˆ é™¤ `cosine_similarity` è°ƒç”¨
3. æ›´æ–° `pyproject.toml` ç§»é™¤ sklearn

---

## ğŸŸ  P1: ä¸­ä¼˜å…ˆçº§ - ç®€åŒ–é‡æ„

### 3. `ToolRegistry` è‡ªå®šä¹‰åè®® â†’ LangChain `@tool`

**ä½ç½®**: `src/olav/tools/base.py`

**å½“å‰å®ç°** (~322è¡Œ):
```python
class BaseTool(Protocol):
    name: str
    description: str
    input_schema: type[BaseModel]
    async def execute(self, **kwargs) -> ToolOutput: ...

class ToolRegistry:
    _tools: dict[str, BaseTool] = {}
    
    @classmethod
    def register(cls, tool: BaseTool) -> None: ...
    
    @classmethod
    def discover_tools(cls, package: str) -> None:
        # æ‰«æ *_tool.py æ–‡ä»¶ï¼ŒåŠ¨æ€å¯¼å…¥
```

**é—®é¢˜**:
- è‡ªå®šä¹‰ Protocol å¢åŠ å­¦ä¹ æˆæœ¬
- ä¸ LangChain agent é›†æˆéœ€è¦é¢å¤–é€‚é…
- `discover_tools()` ä½¿ç”¨ importlib åŠ¨æ€å‘ç°ï¼ˆè„†å¼±ï¼‰

**ç°çŠ¶åˆ†æ**:
é¡¹ç›®å·²ç»åœ¨ä½¿ç”¨ `@tool` è£…é¥°å™¨:
```python
# src/olav/tools/netbox_tool.py
from langchain_core.tools import tool

@tool
def netbox_api_call(endpoint: str, method: str = "GET", ...) -> dict:
    """Query NetBox API for network infrastructure data."""
    ...
```

**æ¨èæ–¹æ¡ˆ**: ç»Ÿä¸€ä½¿ç”¨ LangChain `@tool` + StructuredTool
```python
from langchain_core.tools import tool, StructuredTool
from pydantic import BaseModel

class SuzieqQueryInput(BaseModel):
    table: str
    method: Literal["get", "summarize"]
    filters: dict = Field(default_factory=dict)

@tool(args_schema=SuzieqQueryInput)
async def suzieq_query(table: str, method: str, filters: dict) -> dict:
    """Query SuzieQ for network state data."""
    ...

# å·¥å…·åˆ—è¡¨ç›´æ¥ä¼ ç»™ agentï¼Œæ— éœ€ ToolRegistry
tools = [suzieq_query, netbox_api_call, ...]
```

**è¿ç§»è®¡åˆ’**:
1. å°† `BaseTool` å­ç±»è½¬æ¢ä¸º `@tool` è£…é¥°å™¨å‡½æ•°
2. ä¿ç•™ `ToolOutput` ä½œä¸ºç»Ÿä¸€è¿”å›ç±»å‹ï¼ˆå¯é€‰ï¼‰
3. ç§»é™¤ `ToolRegistry.discover_tools()` åŠ¨æ€å‘ç°
4. åœ¨ workflow ä¸­ç›´æ¥ç»´æŠ¤å·¥å…·åˆ—è¡¨

---

### 4. `cache.py` è‡ªå®šä¹‰ Redis æŠ½è±¡ â†’ LangGraph Cache

**ä½ç½®**: `src/olav/core/cache.py` (~678è¡Œ)

**å½“å‰å®ç°**:
```python
class CacheBackend(ABC):
    async def get(self, key: str) -> Any | None: ...
    async def set(self, key: str, value: Any, ttl: int) -> bool: ...

class RedisCache(CacheBackend):
    # å®Œæ•´ Redis å®¢æˆ·ç«¯ç®¡ç†ã€åºåˆ—åŒ–ã€å‘½åç©ºé—´...

class NoOpCache(CacheBackend):
    # æµ‹è¯•ç”¨ç©ºå®ç°

class CacheManager:
    # å¤šå±‚ç¼“å­˜åè°ƒ
```

**é—®é¢˜**:
- 600+ è¡Œè‡ªå®šä¹‰ç¼“å­˜ä»£ç 
- ä¸ LangGraph ç¼“å­˜ç³»ç»Ÿé‡å¤

**æ¨èæ–¹æ¡ˆ**: LangGraph å†…ç½®ç¼“å­˜
```python
from langgraph.cache.memory import InMemoryCache
from langgraph.cache.sqlite import SqliteCache

# å¼€å‘ç¯å¢ƒ
graph = builder.compile(cache=InMemoryCache())

# ç”Ÿäº§ç¯å¢ƒï¼ˆå¦‚éœ€æŒä¹…åŒ–ï¼‰
graph = builder.compile(cache=SqliteCache("./cache.db"))
```

**æ³¨æ„**: 
- Schema ç¼“å­˜ (`SchemaLoader`) å¯ä¿ç•™ç®€åŒ–ç‰ˆ
- Tool ç»“æœç¼“å­˜å¯ç”¨ LangGraph èŠ‚ç‚¹ç¼“å­˜æ›¿ä»£
- å¦‚éœ€ Redisï¼Œå¯è€ƒè™‘ `langchain-redis` é›†æˆ

**è¿ç§»è®¡åˆ’**:
1. è¯„ä¼°å“ªäº›ç¼“å­˜åœºæ™¯å¯ç”¨ LangGraph ç¼“å­˜æ›¿ä»£
2. ä¿ç•™ `SchemaLoader` çš„ç®€åŒ–å†…å­˜ç¼“å­˜
3. å°† `FastPath` çš„å·¥å…·ç¼“å­˜è¿ç§»åˆ° LangGraph

---

## ğŸŸ¡ P2: ä½ä¼˜å…ˆçº§ - å¯ç®€åŒ–

### 5. `FilesystemMiddleware` â†’ ç®€åŒ–æˆ–ç§»é™¤

**ä½ç½®**: `src/olav/core/middleware/filesystem.py` (~500è¡Œ)

**å½“å‰åŠŸèƒ½**:
- æ–‡ä»¶è¯»å†™æŠ½è±¡
- ç¼“å­˜é”®ç”Ÿæˆ (`get_cache_key`)
- HITL å®¡æ‰¹æ—¥å¿—

**é—®é¢˜**:
- å¤§é‡ä»£ç ç”¨äºç®€å•çš„æ–‡ä»¶ I/O
- ç¼“å­˜é€»è¾‘ä¸ä¸šåŠ¡è€¦åˆ

**æ¨è**: å¦‚ä¸»è¦ç”¨äºç¼“å­˜ï¼Œè¿ç§»åˆ° `CacheManager`ï¼›å¦‚ç”¨äºæ—¥å¿—ï¼Œä½¿ç”¨æ ‡å‡† `logging`ã€‚

---

### 6. `tool_call_parser.py` â†’ å¯èƒ½å·²ä¸éœ€è¦

**ä½ç½®**: `src/olav/core/tool_call_parser.py` (~100è¡Œ)

**å½“å‰åŠŸèƒ½**:
ä¿®å¤ OpenRouter/DeepSeek è¿”å› `tool_calls.args` ä¸º JSON å­—ç¬¦ä¸²çš„é—®é¢˜ã€‚

**ç°çŠ¶**:
- `FixedChatOpenAI` å·²åœ¨ `llm.py` ä¸­å®ç°ä¿®å¤
- LangChain è¾ƒæ–°ç‰ˆæœ¬å¯èƒ½å·²å†…ç½®ä¿®å¤

**éªŒè¯æ­¥éª¤**:
1. æ£€æŸ¥ LangChain æœ€æ–°ç‰ˆæœ¬æ˜¯å¦å·²ä¿®å¤
2. æµ‹è¯•ä¸ä½¿ç”¨ `FixedChatOpenAI` æ˜¯å¦æ­£å¸¸å·¥ä½œ
3. å¦‚å·²ä¿®å¤ï¼Œåˆ é™¤è¯¥æ–‡ä»¶

---

## âœ… P3: ä¿ç•™ - ä¸šåŠ¡ç‰¹å®š

### 7. `MemoryWriter` - ä¿ç•™

**ä½ç½®**: `src/olav/core/memory_writer.py` (~225è¡Œ)

**åŠŸèƒ½**:
å°†æˆåŠŸçš„ç­–ç•¥æ‰§è¡Œè®°å½•åˆ° OpenSearch çš„ episodic memory ç´¢å¼•ã€‚

**åˆ†æ**:
è¿™æ˜¯ä¸šåŠ¡ç‰¹å®šçš„çŸ¥è¯†ç§¯ç´¯åŠŸèƒ½ï¼ŒLangChain æ²¡æœ‰ç›´æ¥å¯¹åº”çš„æ¨¡å—ã€‚
è™½ç„¶ LangChain æœ‰ `ConversationBufferMemory` ç­‰ï¼Œä½†ä¸é€‚åˆé•¿æœŸçŸ¥è¯†å­˜å‚¨ã€‚

**å»ºè®®**: ä¿ç•™ï¼Œä½†å¯è€ƒè™‘ç®€åŒ–æ•°æ®ç»“æ„ã€‚

---

### 8. `OpenSearchMemory` - ä¿ç•™

**ä½ç½®**: `src/olav/core/memory.py` (~122è¡Œ)

**åŠŸèƒ½**:
- Schema å‘é‡æœç´¢
- æ‰§è¡Œæ—¥å¿—å®¡è®¡
- Episodic memory å­˜å‚¨

**åˆ†æ**:
OpenSearch ä½œä¸º OLAV çš„æ ¸å¿ƒå­˜å‚¨ï¼Œéœ€è¦è‡ªå®šä¹‰å°è£…ã€‚
LangChain çš„ `OpenSearchVectorSearch` ä¸æ”¯æŒæˆ‘ä»¬çš„å¤šç´¢å¼•åœºæ™¯ã€‚

**å»ºè®®**: ä¿ç•™ã€‚

---

## ğŸ”§ è¿ç§»å®æ–½è®¡åˆ’

### Phase 1: P0 é¡¹ç›® âœ… å·²å®Œæˆ

```
1. extract_json_from_response() æ›¿æ¢ âœ…
   - [x] å®šä¹‰æ‰€æœ‰è¾“å‡ºçš„ Pydantic æ¨¡å‹ (6ä¸ªæ¨¡å‹)
   - [x] æ›¿æ¢ deep_path.py ä¸­çš„è°ƒç”¨ (5å¤„)
   - [x] åˆ é™¤ extract_json_from_response() å‡½æ•° (~50è¡Œ)
   - [x] è¿è¡Œæµ‹è¯•éªŒè¯ (24/24 passed)

2. DynamicIntentRouter sklearn æ›¿æ¢ âœ…
   - [x] å¼•å…¥ InMemoryVectorStore
   - [x] é‡æ„ build_index() æ–¹æ³•
   - [x] é‡æ„ semantic_prefilter() æ–¹æ³•
   - [x] ä» pyproject.toml ç§»é™¤ sklearn
   - [x] è¿è¡Œæµ‹è¯•éªŒè¯ (20/20 passed)
   
   ç§»é™¤çš„ä¾èµ–: scikit-learn, scipy, joblib, threadpoolctl (4ä¸ªåŒ…)
```

### Phase 2: P1 é¡¹ç›® (è¯„ä¼°åéƒ¨åˆ†å®Œæˆ)

```
3. ToolRegistry ç®€åŒ– â†’ âœ… å·²ç®€åŒ–
   - [x] ç§»é™¤ discover_tools() è‡ªåŠ¨å‘ç° (~50è¡Œå¤æ‚ä»£ç )
   - [x] æ”¹ä¸ºå¹‚ç­‰æ³¨å†Œ (é‡å¤æ³¨å†Œé™é»˜è·³è¿‡)
   - [x] ä¿ç•™ ToolOutput æ ‡å‡†åŒ– (æœ‰ä»·å€¼)
   - æœªåš: @tool è£…é¥°å™¨ç»Ÿä¸€ (å½±å“èŒƒå›´å¤ªå¤§ï¼Œæš‚ç¼“)

4. Cache ç³»ç»Ÿè¯„ä¼° â†’ ä¿ç•™
   - ä»£ç è¡Œæ•°: 678è¡Œ
   - è®¾è®¡: æŠ½è±¡åŸºç±» + Redis/NoOp åç«¯
   - ç»“è®º: LangGraph Cache ä¸æ˜¯ç›´æ¥æ›¿ä»£å“ï¼Œå½“å‰è®¾è®¡æ›´çµæ´»
```

### Phase 3: P2 æ¸…ç† âœ… å·²å®Œæˆ

```
5. tool_call_parser.py â†’ âœ… å·²åˆ é™¤ (æ­»ä»£ç ï¼Œ0å¤„ä½¿ç”¨)
6. FilesystemMiddleware â†’ ä¿ç•™ (è¢« fast_path.py ä½¿ç”¨ï¼Œè®¾è®¡åˆç†)
```

---

## ğŸ“ˆ å®é™…æ”¶ç›Š

| æŒ‡æ ‡ | ä¹‹å‰ | ä¹‹å |
|------|------|------|
| ä¾èµ–æ•°é‡ | sklearn, scipy, joblib, threadpoolctl | âœ… å…¨éƒ¨ç§»é™¤ (4ä¸ªåŒ…) |
| åˆ é™¤ä»£ç  | - | ~250è¡Œ (extract_json, tool_call_parser, discover_tools) |
| è‡ªå®šä¹‰ä»£ç  | numpy/sklearnæ‰‹åŠ¨æ“ä½œ | LangChainåŸç”ŸAPI |
| å·¥å…·æ³¨å†Œ | å¤æ‚è‡ªåŠ¨å‘ç°+é‡å¤è­¦å‘Š | ç®€å•è‡ªæ³¨å†Œï¼Œå¹‚ç­‰ |
| LangChain å…¼å®¹æ€§ | ä¸­ | é«˜ |
| è‡ªå®šä¹‰ä»£ç è¡Œæ•° | ~2000è¡Œ | ~800è¡Œ |
| ç»´æŠ¤å¤æ‚åº¦ | é«˜ | ä¸­ |
| LangChain å…¼å®¹æ€§ | ä¸­ | é«˜ |

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [LangChain Structured Output](https://docs.langchain.com/docs/expression_language/how_to/structured_output)
- [LangChain with_structured_output](https://api.python.langchain.com/en/latest/chat_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel.with_structured_output)
- [LangGraph Caching](https://langchain-ai.github.io/langgraph/how-tos/caching/)
- [LangChain VectorStores](https://python.langchain.com/docs/modules/data_connection/vectorstores/)
