#!/usr/bin/env python
"""è½»é‡çº§æµå¼ä¼ è¾“æµ‹è¯•è„šæœ¬.

æµ‹è¯• LangChain/LangGraph çš„ä¸‰ç§æµå¼æ¨¡å¼ï¼š
1. LLM Token çº§åˆ«æµå¼ (åŒ…æ‹¬ reasoning/thinking)
2. Graph çŠ¶æ€æµå¼
3. äº‹ä»¶æµå¼ (astream_events)

ç”¨æ³•:
    uv run python scripts/test_stream.py
"""

import asyncio
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from dotenv import load_dotenv

# åŠ è½½ .env
load_dotenv(Path(__file__).parent.parent / ".env")


async def test_1_llm_token_stream():
    """æµ‹è¯• 1: LLM Token çº§åˆ«æµå¼è¾“å‡º."""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 1: LLM Token çº§åˆ«æµå¼è¾“å‡º")
    print("=" * 60)
    
    from langchain_ollama import ChatOllama
    
    # è·å–é…ç½®
    base_url = os.getenv("LLM_BASE_URL", "http://host.docker.internal:11434")
    model = os.getenv("LLM_MODEL_NAME", "qwen3:30b")
    
    print(f"æ¨¡å‹: {model}")
    print(f"Base URL: {base_url}")
    
    # åˆ›å»ºæ¨¡å‹ - å¯ç”¨ reasoning æ¨¡å¼
    llm = ChatOllama(
        model=model,
        base_url=base_url,
        temperature=0.7,
        reasoning=True,  # å¯ç”¨æ€è€ƒæ¨¡å¼ (qwen3, deepseek ç­‰æ”¯æŒ)
    )
    
    print("\n--- æµå¼è¾“å‡º (astream) ---")
    query = "9.11 å’Œ 9.9 å“ªä¸ªå¤§ï¼Ÿç”¨ä¸€å¥è¯å›ç­”"
    print(f"Query: {query}\n")
    
    reasoning_buffer = ""
    content_buffer = ""
    
    async for chunk in llm.astream(query):
        # æ­£å¸¸å“åº”å†…å®¹
        if chunk.content:
            content_buffer += chunk.content
            print(f"{chunk.content}", end="", flush=True)
        
        # reasoning_content åœ¨ additional_kwargs ä¸­ï¼
        reasoning = chunk.additional_kwargs.get("reasoning_content", "")
        if reasoning:
            reasoning_buffer += reasoning
            # å®æ—¶æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
            print(f"\nğŸ’­ {reasoning}", end="", flush=True)
    
    print("\n\n--- æ±‡æ€» ---")
    print(f"æ€è€ƒè¿‡ç¨‹ ({len(reasoning_buffer)} chars):")
    if reasoning_buffer:
        print(reasoning_buffer[:500] + "..." if len(reasoning_buffer) > 500 else reasoning_buffer)
    print(f"\næœ€ç»ˆå“åº” ({len(content_buffer)} chars):")
    print(content_buffer)


async def test_2_graph_state_stream():
    """æµ‹è¯• 2: Graph çŠ¶æ€æµå¼è¾“å‡º."""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: Graph çŠ¶æ€æµå¼è¾“å‡º (stream_mode='values')")
    print("=" * 60)
    
    from typing import Annotated, TypedDict
    from langchain_ollama import ChatOllama
    from langgraph.graph import StateGraph, START, END
    from langgraph.graph.message import add_messages
    
    # å®šä¹‰çŠ¶æ€
    class State(TypedDict):
        messages: Annotated[list, add_messages]
        step: str
    
    # è·å– LLM
    base_url = os.getenv("LLM_BASE_URL", "http://host.docker.internal:11434")
    model = os.getenv("LLM_MODEL_NAME", "qwen3:30b")
    llm = ChatOllama(model=model, base_url=base_url, temperature=0.7)
    
    # å®šä¹‰èŠ‚ç‚¹
    def analyze_node(state: State) -> State:
        print("  [Node: analyze] æ‰§è¡Œä¸­...")
        return {"step": "analyze", "messages": []}
    
    async def llm_node(state: State) -> State:
        print("  [Node: llm] è°ƒç”¨ LLM...")
        response = await llm.ainvoke(state["messages"])
        return {"step": "llm", "messages": [response]}
    
    # æ„å»ºå›¾
    graph = StateGraph(State)
    graph.add_node("analyze", analyze_node)
    graph.add_node("llm", llm_node)
    graph.add_edge(START, "analyze")
    graph.add_edge("analyze", "llm")
    graph.add_edge("llm", END)
    
    app = graph.compile()
    
    print("\n--- stream_mode='values' (å®Œæ•´çŠ¶æ€) ---")
    initial_state = {"messages": [("user", "ä½ å¥½")], "step": "start"}
    
    async for chunk in app.astream(initial_state, stream_mode="values"):
        print(f"  State update: step={chunk.get('step')}, msgs={len(chunk.get('messages', []))}")
    
    print("\n--- stream_mode='updates' (å¢é‡æ›´æ–°) ---")
    async for chunk in app.astream(initial_state, stream_mode="updates"):
        for node_name, update in chunk.items():
            print(f"  Node '{node_name}' update: {list(update.keys())}")


async def test_3_astream_events():
    """æµ‹è¯• 3: äº‹ä»¶æµå¼è¾“å‡º (astream_events) - æœ€ç»†ç²’åº¦."""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: äº‹ä»¶æµå¼è¾“å‡º (astream_events)")
    print("=" * 60)
    
    from typing import Annotated, TypedDict
    from langchain_ollama import ChatOllama
    from langchain_core.tools import tool
    from langgraph.graph import StateGraph, START, END
    from langgraph.graph.message import add_messages
    from langgraph.prebuilt import ToolNode
    
    # å®šä¹‰ç®€å•å·¥å…·
    @tool
    def get_weather(city: str) -> str:
        """è·å–åŸå¸‚å¤©æ°”."""
        return f"{city} ä»Šå¤©æ™´ï¼Œ25Â°C"
    
    tools = [get_weather]
    
    # å®šä¹‰çŠ¶æ€
    class State(TypedDict):
        messages: Annotated[list, add_messages]
    
    # è·å– LLM - å¯ç”¨ reasoning
    base_url = os.getenv("LLM_BASE_URL", "http://host.docker.internal:11434")
    model = os.getenv("LLM_MODEL_NAME", "qwen3:30b")
    llm = ChatOllama(
        model=model, 
        base_url=base_url, 
        temperature=0.7,
        reasoning=True,  # å¯ç”¨æ€è€ƒæ¨¡å¼
    ).bind_tools(tools)
    
    # å®šä¹‰èŠ‚ç‚¹
    async def agent_node(state: State) -> State:
        response = await llm.ainvoke(state["messages"])
        return {"messages": [response]}
    
    def should_continue(state: State):
        last_msg = state["messages"][-1]
        if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
            return "tools"
        return END
    
    # æ„å»ºå›¾
    graph = StateGraph(State)
    graph.add_node("agent", agent_node)
    graph.add_node("tools", ToolNode(tools))
    graph.add_edge(START, "agent")
    graph.add_conditional_edges("agent", should_continue)
    graph.add_edge("tools", "agent")
    
    app = graph.compile()
    
    print("\n--- astream_events (äº‹ä»¶æµ) ---")
    print("äº‹ä»¶ç±»å‹è¯´æ˜:")
    print("  on_chat_model_stream - LLM Token (åŒ…å« reasoning_content)")
    print("  on_tool_start        - å·¥å…·å¼€å§‹")
    print("  on_tool_end          - å·¥å…·ç»“æŸ")
    print()
    
    query = "9.11 å’Œ 9.9 å“ªä¸ªå¤§ï¼Ÿ"
    print(f"Query: {query}\n")
    
    initial_state = {"messages": [("user", query)]}
    
    token_count = 0
    reasoning_buffer = ""
    content_buffer = ""
    
    async for event in app.astream_events(initial_state, version="v2"):
        event_type = event.get("event", "")
        
        if event_type == "on_chat_model_start":
            print(f"ğŸš€ [LLM START]")
        
        elif event_type == "on_chat_model_stream":
            # è¿™æ˜¯ Token çº§åˆ«çš„æµå¼è¾“å‡º
            chunk = event.get("data", {}).get("chunk")
            if chunk:
                # æ­£å¸¸å†…å®¹
                content = getattr(chunk, "content", "")
                if content:
                    token_count += 1
                    content_buffer += content
                    print(content, end="", flush=True)
                
                # â­ å…³é”®: reasoning_content åœ¨ additional_kwargs ä¸­
                additional = getattr(chunk, "additional_kwargs", {})
                reasoning = additional.get("reasoning_content", "")
                if reasoning:
                    reasoning_buffer += reasoning
                    print(f"\nğŸ’­ {reasoning}", end="", flush=True)
        
        elif event_type == "on_chat_model_end":
            print(f"\nâœ… [LLM END] tokens={token_count}")
            token_count = 0
        
        elif event_type == "on_tool_start":
            tool_name = event.get("name", "unknown")
            tool_input = event.get("data", {}).get("input", {})
            print(f"ğŸ”§ [TOOL START] {tool_name}({tool_input})")
        
        elif event_type == "on_tool_end":
            tool_name = event.get("name", "unknown")
            tool_output = event.get("data", {}).get("output", "")
            print(f"âœ… [TOOL END] {tool_name} -> {str(tool_output)[:100]}")
    
    print("\n\n--- æ±‡æ€» ---")
    print(f"æ€è€ƒè¿‡ç¨‹ ({len(reasoning_buffer)} chars): {reasoning_buffer[:300]}...")
    print(f"æœ€ç»ˆå“åº” ({len(content_buffer)} chars): {content_buffer}")


async def test_4_subgraph_stream():
    """æµ‹è¯• 4: å­å›¾çŠ¶æ€æµå¼è¾“å‡º."""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: å­å›¾ (Subgraph) çŠ¶æ€æµå¼è¾“å‡º")
    print("=" * 60)
    
    from typing import Annotated, TypedDict
    from langchain_ollama import ChatOllama
    from langgraph.graph import StateGraph, START, END
    from langgraph.graph.message import add_messages
    
    # å®šä¹‰çŠ¶æ€
    class SubState(TypedDict):
        messages: Annotated[list, add_messages]
        sub_step: str
    
    class MainState(TypedDict):
        messages: Annotated[list, add_messages]
        main_step: str
    
    # è·å– LLM
    base_url = os.getenv("LLM_BASE_URL", "http://host.docker.internal:11434")
    model = os.getenv("LLM_MODEL_NAME", "qwen3:30b")
    llm = ChatOllama(model=model, base_url=base_url, temperature=0.7)
    
    # åˆ›å»ºå­å›¾
    def sub_analyze(state: SubState) -> SubState:
        print("    [SubGraph: analyze] åˆ†æä¸­...")
        return {"sub_step": "sub_analyze"}
    
    async def sub_respond(state: SubState) -> SubState:
        print("    [SubGraph: respond] ç”Ÿæˆå“åº”...")
        response = await llm.ainvoke(state["messages"])
        return {"sub_step": "sub_respond", "messages": [response]}
    
    subgraph = StateGraph(SubState)
    subgraph.add_node("sub_analyze", sub_analyze)
    subgraph.add_node("sub_respond", sub_respond)
    subgraph.add_edge(START, "sub_analyze")
    subgraph.add_edge("sub_analyze", "sub_respond")
    subgraph.add_edge("sub_respond", END)
    sub_app = subgraph.compile()
    
    # åˆ›å»ºä¸»å›¾
    def preprocess(state: MainState) -> MainState:
        print("  [MainGraph: preprocess] é¢„å¤„ç†...")
        return {"main_step": "preprocess"}
    
    async def call_subgraph(state: MainState) -> MainState:
        print("  [MainGraph: call_subgraph] è°ƒç”¨å­å›¾...")
        # ç›´æ¥è°ƒç”¨å­å›¾
        sub_result = await sub_app.ainvoke({
            "messages": state["messages"],
            "sub_step": "start"
        })
        return {
            "main_step": "subgraph_done",
            "messages": sub_result["messages"]
        }
    
    def postprocess(state: MainState) -> MainState:
        print("  [MainGraph: postprocess] åå¤„ç†...")
        return {"main_step": "done"}
    
    main_graph = StateGraph(MainState)
    main_graph.add_node("preprocess", preprocess)
    main_graph.add_node("call_subgraph", call_subgraph)
    main_graph.add_node("postprocess", postprocess)
    main_graph.add_edge(START, "preprocess")
    main_graph.add_edge("preprocess", "call_subgraph")
    main_graph.add_edge("call_subgraph", "postprocess")
    main_graph.add_edge("postprocess", END)
    
    main_app = main_graph.compile()
    
    print("\n--- ä¸»å›¾ + å­å›¾æµå¼è¾“å‡º ---")
    initial_state = {"messages": [("user", "ä½ å¥½")], "main_step": "start"}
    
    print("\nä½¿ç”¨ astream_events è¿½è¸ªæ‰€æœ‰äº‹ä»¶:")
    async for event in main_app.astream_events(initial_state, version="v2"):
        event_type = event.get("event", "")
        name = event.get("name", "")
        
        # åªæ˜¾ç¤ºå…³é”®äº‹ä»¶
        if event_type in ["on_chain_start", "on_chain_end"]:
            if "graph" in name.lower() or "langgraph" in name.lower():
                print(f"  ğŸ“Š {event_type}: {name}")
        elif event_type == "on_chat_model_stream":
            chunk = event.get("data", {}).get("chunk")
            if chunk and getattr(chunk, "content", ""):
                print(chunk.content, end="", flush=True)
    
    print("\n")


async def test_5_reasoning_extraction():
    """æµ‹è¯• 5: æå– qwen3/deepseek çš„æ€è€ƒè¿‡ç¨‹."""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: æå–æ¨¡å‹æ€è€ƒè¿‡ç¨‹ (reasoning_content)")
    print("=" * 60)
    
    from langchain_ollama import ChatOllama
    
    base_url = os.getenv("LLM_BASE_URL", "http://host.docker.internal:11434")
    model = os.getenv("LLM_MODEL_NAME", "qwen3:30b")
    
    print(f"æ¨¡å‹: {model}")
    print(f"Base URL: {base_url}")
    
    # æµ‹è¯•ä¸åŒçš„ reasoning é…ç½®
    configs = [
        {"reasoning": True, "desc": "reasoning=True"},
        {"reasoning": "detailed", "desc": "reasoning='detailed'"},
    ]
    
    for config in configs:
        print(f"\n--- é…ç½®: {config['desc']} ---")
        
        llm = ChatOllama(
            model=model,
            base_url=base_url,
            temperature=0.7,
            reasoning=config["reasoning"],
        )
        
        query = "9.11 å’Œ 9.9 å“ªä¸ªå¤§ï¼Ÿ"
        print(f"Query: {query}\n")
        
        thinking_content = ""
        response_content = ""
        
        async for chunk in llm.astream(query):
            # æ”¶é›†æ­£å¸¸å“åº”
            if chunk.content:
                response_content += chunk.content
                print(f"[CONTENT] {chunk.content}", end="", flush=True)
            
            # æ£€æŸ¥æ‰€æœ‰å¯èƒ½åŒ…å«æ€è€ƒå†…å®¹çš„å±æ€§
            if hasattr(chunk, "reasoning_content") and chunk.reasoning_content:
                thinking_content += chunk.reasoning_content
                print(f"\n[REASONING_CONTENT] {chunk.reasoning_content}", flush=True)
            
            # æ£€æŸ¥ response_metadata
            if hasattr(chunk, "response_metadata") and chunk.response_metadata:
                meta = chunk.response_metadata
                if "thinking" in str(meta).lower() or "reason" in str(meta).lower():
                    print(f"\n[METADATA] {meta}", flush=True)
            
            # æ£€æŸ¥ additional_kwargs
            if chunk.additional_kwargs:
                for key, value in chunk.additional_kwargs.items():
                    if value and ("think" in key.lower() or "reason" in key.lower()):
                        print(f"\n[{key.upper()}] {value}", flush=True)
        
        print(f"\n\næ€»ç»“: thinking={len(thinking_content)} chars, response={len(response_content)} chars")


async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•."""
    print("=" * 60)
    print("LangChain/LangGraph æµå¼ä¼ è¾“æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("1", "LLM Token æµå¼", test_1_llm_token_stream),
        ("2", "Graph çŠ¶æ€æµå¼", test_2_graph_state_stream),
        ("3", "äº‹ä»¶æµå¼ (astream_events)", test_3_astream_events),
        ("4", "å­å›¾æµå¼", test_4_subgraph_stream),
        ("5", "æ€è€ƒè¿‡ç¨‹æå–", test_5_reasoning_extraction),
    ]
    
    print("\né€‰æ‹©æµ‹è¯•:")
    for num, name, _ in tests:
        print(f"  {num}. {name}")
    print("  a. è¿è¡Œå…¨éƒ¨")
    print("  q. é€€å‡º")
    
    choice = input("\nè¯·é€‰æ‹© (1-5/a/q): ").strip().lower()
    
    if choice == "q":
        return
    elif choice == "a":
        for _, name, test_func in tests:
            try:
                await test_func()
            except Exception as e:
                print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
    else:
        for num, name, test_func in tests:
            if num == choice:
                try:
                    await test_func()
                except Exception as e:
                    print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                break
        else:
            print("æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    asyncio.run(main())
